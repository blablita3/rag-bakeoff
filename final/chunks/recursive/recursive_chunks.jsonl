{"metadata": {"id": "recursive_chunk_000", "chunk_length": 997, "method": "recursive"}, "page_content": "REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages"}
{"metadata": {"id": "recursive_chunk_001", "chunk_length": 964, "method": "recursive"}, "page_content": "require specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30 .85×the\ntime-to-first-token acceleration (3 .75×improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size"}
{"metadata": {"id": "recursive_chunk_002", "chunk_length": 971, "method": "recursive"}, "page_content": "time-to-first-token acceleration (3 .75×improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16 ×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging"}
{"metadata": {"id": "recursive_chunk_003", "chunk_length": 939, "method": "recursive"}, "page_content": "Code:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer"}
{"metadata": {"id": "recursive_chunk_004", "chunk_length": 907, "method": "recursive"}, "page_content": "to higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1arXiv:2509.01092v2  [cs.CL]  12 Oct 2025"}
{"metadata": {"id": "recursive_chunk_005", "chunk_length": 900, "method": "recursive"}, "page_content": "Optimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1arXiv:2509.01092v2  [cs.CL]  12 Oct 2025\nranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)"}
{"metadata": {"id": "recursive_chunk_006", "chunk_length": 922, "method": "recursive"}, "page_content": "overhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions"}
{"metadata": {"id": "recursive_chunk_007", "chunk_length": 899, "method": "recursive"}, "page_content": "and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed"}
{"metadata": {"id": "recursive_chunk_008", "chunk_length": 946, "method": "recursive"}, "page_content": "shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings."}
{"metadata": {"id": "recursive_chunk_009", "chunk_length": 933, "method": "recursive"}, "page_content": "and when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30 .75×TTFT acceleration without loss in perplexity\nwhich is3 .75×than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model as Mdecand the encoder model as Menc. Given an input with Ttokens\nx1, x2, . . . , x T, we assume that the first qtokens are main input tokens (e.g., questions) and the last stokens"}
{"metadata": {"id": "recursive_chunk_010", "chunk_length": 912, "method": "recursive"}, "page_content": "x1, x2, . . . , x T, we assume that the first qtokens are main input tokens (e.g., questions) and the last stokens\nare context tokens (e.g., retrieved passages in RAG). We have q+s=T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a question x1, . . . , x qand context xq+1, . . . , x Tand , the context is chunked into\nL:=s\nknumber of k-sized chunks {C1, . . . , C L}where Ci={xq+k∗i, . . . , x q+k∗i+k−1 }. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkc i=Menc(Ci). This chunk embedding\nis then projected with a projection layer ϕto match the size of the token embedding of the decoder model,\necnk"}
{"metadata": {"id": "recursive_chunk_011", "chunk_length": 889, "method": "recursive"}, "page_content": "is then projected with a projection layer ϕto match the size of the token embedding of the decoder model,\necnk\ni=ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answer y∼ M dec({e1, . . . , eq,ecnk\n1, . . . , ecnk\nL})wheree iis the\n2\nDonald Trump is \nthe President of \nthe United States . He assumed office \non January 20, \n2025, making him the 47th \nPresident of the \nUnited States.  Encoder  Encoder  Encoder \nContext Text Decoder-only Foundation Model \nSequence \nPrecomputable \nLight-weight \nEncoder Who is the President of USA? Decoder Tokenizer & \nEmbedding \nDecoder Input Text Token Embedding \nChunk \nEmbedding \nLight-weight RL-trained chunk expansion policy \nVector DB Query \nEncoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to"}
{"metadata": {"id": "recursive_chunk_012", "chunk_length": 999, "method": "recursive"}, "page_content": "Chunk \nEmbedding \nLight-weight RL-trained chunk expansion policy \nVector DB Query \nEncoder Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for token xi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q) and hence the overall input to the decoder will be reduced by a factor of ≃k. This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk."}
{"metadata": {"id": "recursive_chunk_013", "chunk_length": 980, "method": "recursive"}, "page_content": "the performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103104\n# Input T okens0102030Acceleration\nTTFT Acceleration\n103104\n# Input T okens1.01.52.02.53.0Acceleration\nTTIT Acceleration\n103104\n# Input T okens246Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk×acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2×for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG"}
{"metadata": {"id": "recursive_chunk_014", "chunk_length": 997, "method": "recursive"}, "page_content": "metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k= 16achieves16 .53×TTFT acceleration with cache and8 .59×without cache1, both surpassing CEPE\n(2.01×and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6 .78×throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. With k= 32, TTFT acceleration reaches32 .99×compared to LLaMA (3 .75×compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3\non empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it contains s+o=Tnumber"}
{"metadata": {"id": "recursive_chunk_015", "chunk_length": 998, "method": "recursive"}, "page_content": "tasks for continual pre-training (CPT). Specifically, for each data data point, it contains s+o=Tnumber\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the first stokens x1:sinto the encoder and use its output to assist the decoder in\npredicting the next otokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context."}
{"metadata": {"id": "recursive_chunk_016", "chunk_length": 995, "method": "recursive"}, "page_content": "encoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the first stokens x1:sto the encoder and learn to reconstruct tokens x1:sin\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compress ktokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information."}
{"metadata": {"id": "recursive_chunk_017", "chunk_length": 990, "method": "recursive"}, "page_content": "decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk length kincreases, the number of possible token combinations expands\nexponentially, specifically at a rate of Vk, where Vis the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructing s=k×Ltokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce"}
{"metadata": {"id": "recursive_chunk_018", "chunk_length": 924, "method": "recursive"}, "page_content": "fromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc 1forx1:kand and the decoder\nreconstructs the ktokens using the projected chunk embeddingecnk\n1. Subsequently, the model reconstructs\nx1:2kfromecnk\n1,ecnk\n2, and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting"}
{"metadata": {"id": "recursive_chunk_019", "chunk_length": 928, "method": "recursive"}, "page_content": "2, and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e., Lchunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s"}
{"metadata": {"id": "recursive_chunk_020", "chunk_length": 934, "method": "recursive"}, "page_content": "chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4\n4 Experimental Results\nTraining datasets.We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-\ntraining. This dataset contains data from Wikipedia, Arxiv, Books, StackExchange, GitHub, Commoncrawl,\nC4. We only use the Book and ArXiv domains from the dataset since these two domains contain long\ntexts (Yen et al., 2024). We sampled from this dataset to construct a20 Btoken training dataset which\ncontains50%data from Arxiv and50%data from Book.\nEvaluation datasets.We report the performance on the Book and ArXiv domain from Slimpajama which\nare hold out for evaluation only. To inspect the generalization of the model, we also report results on the"}
{"metadata": {"id": "recursive_chunk_021", "chunk_length": 911, "method": "recursive"}, "page_content": "are hold out for evaluation only. To inspect the generalization of the model, we also report results on the\nPG19 (Rae et al., 2019) and Proof-pile datasets (Azerbayev et al., 2023).\nBaselines.All baseline models are based on LLaMA-2-7B (Touvron et al., 2023), unless otherwise specified, to\nensure fair comparison with prior work (Yen et al., 2024; Shi et al., 2024). Each data point contains T= 4096\ntokens, split into s= 2048context and o= 2048output tokens. We evaluate perplexity on xs+1:s+o. Below, we\nbriefly describe the main baselines; further details are provided in section B.LLaMA-No Context: LLaMA-2-7B\nevaluated on xs+1:s+owith only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on\nxs+1:s+owith the full sequence x1:Tas input.CEPE: Memory-efficient long-context model (Yen et al., 2024)\na previous SOTA model which share some similarity toREFRAG CEPEDdenotes its instruction-tuned"}
{"metadata": {"id": "recursive_chunk_022", "chunk_length": 948, "method": "recursive"}, "page_content": "xs+1:s+owith the full sequence x1:Tas input.CEPE: Memory-efficient long-context model (Yen et al., 2024)\na previous SOTA model which share some similarity toREFRAG CEPEDdenotes its instruction-tuned\nvariant.LLaMA-32K: LLaMA-2-7B fine-tuned for 32K context length.REPLUG: Retrieval-augmented LLaMA-\n2-7B (Shi et al., 2024).REFRAG: Our approach (see Figure 1);REFRAG kdenotes compression rate k,\nREFRAG RLuses RL-based selective compression. LLaMA K: LLaMA-2-7B evaluated on xs+1:s+owith the\ntruncated sequencex s−K:Tas input to match the token count ofREFRAG.\nTable 1 reports performance for s= 2048and o∈ { 512,1024,2048}, where, e.g., P512 denotes o= 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K , which use full\ncontext without compression and are expected to perform best. Notably, REFRAG 8andREFRAG 16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than"}
{"metadata": {"id": "recursive_chunk_023", "chunk_length": 918, "method": "recursive"}, "page_content": "context without compression and are expected to perform best. Notably, REFRAG 8andREFRAG 16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference, LLaMA 256uses only the last 256 tokens, matching the number of chunk\nembeddings in REFRAG 8(s/k= 256), yet REFRAG 8consistently surpasses LLaMA 256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluates o= 2048with extended context lengths s∈ { 4096,8192,16384}. Although our model is\ntrained on s+o= 6144, both REFRAG 8andREFRAG 16maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9 .3%average log-perplexity improvement over CEPE across"}
{"metadata": {"id": "recursive_chunk_024", "chunk_length": 960, "method": "recursive"}, "page_content": "chunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9 .3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16 .53×faster than LLaMA in TTFT and2 .01×faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expand pfraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kpdecreases when\nfewer chunks are compressed (i.e., pincreases). We compare the perplexity of xs+1:s+ousing different selection\npolicy under different p. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured"}
{"metadata": {"id": "recursive_chunk_025", "chunk_length": 950, "method": "recursive"}, "page_content": "chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated asLLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5\nTable 1Log-Perplexity on output tokens xs+1:s+ogiven context tokens x1:sfor different models. We use s= 2048and"}
{"metadata": {"id": "recursive_chunk_026", "chunk_length": 934, "method": "recursive"}, "page_content": "LLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5\nTable 1Log-Perplexity on output tokens xs+1:s+ogiven context tokens x1:sfor different models. We use s= 2048and\no∈ { 512,1024,2048}here. Bolding are based on comparing baselines excludingLLaMA-Full ContextandLLaMA-\n32Ksince they are expected to be the best (ideally). The lower the better (↓).\nArxiv Book PG19 ProofPile\nP512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nLLaMA-Full Context1.075 1.074 1.069 1.830 1.827 1.826 1.947 1.941 1.935 0.952 0.940 0.931\nLLaMA-32K1.086 1.084 1.076 1.887 1.883 1.880 1.988 1.982 1.975 0.961 0.948 0.937\nLLaMA-No Context1.526 1.371 1.254 2.101 1.995 1.927 2.211 2.102 2.030 1.437 1.256 1.127\nLLaMA 256 1.267 1.221 1.1711.9241.897 1.874 2.031 2.003 1.978 1.156 1.094 1.038\nREPLUG1.526 1.371 1.254 2.101 1.995 1.927 2.211 2.102 2.030 1.437 1.256 1.127\nCEPE1.196 1.148 1.107 1.946 1.896 1.864 2.057 2.002 1.964 1.075 1.014 0.968"}
{"metadata": {"id": "recursive_chunk_027", "chunk_length": 984, "method": "recursive"}, "page_content": "REPLUG1.526 1.371 1.254 2.101 1.995 1.927 2.211 2.102 2.030 1.437 1.256 1.127\nCEPE1.196 1.148 1.107 1.946 1.896 1.864 2.057 2.002 1.964 1.075 1.014 0.968\nREFRAG 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.956 1.927 0.997 0.952 0.916\nREFRAG 16 1.157 1.114 1.0761.9251.882 1.853 2.016 1.971 1.938 1.034 0.976 0.931\nREFRAG 32 1.215 1.1541.1031.9461.896 1.8622.0391.987 1.9491.097 1.0200.961\nTable 2Log-Perplexity on output tokens xs+1:s+ogiven different length of context. We use s∈ { 4096,8192,16384}and\no= 2048here. Bolding are based on comparing baselines excludingLLaMA-Full ContextandLLaMA-32Ksince\nthey are expected to be the best (ideally). The lower the better (↓).\nContext Length =4096 Context Length=8192 Context Length=16384\nArxiv Book PG19 ProofPile Arxiv Book PG19 ProofPile Arxiv Book PG19 ProofPile↓\nLLaMA-Full Context6.751 6.956 6.829 6.701 9.675 9.069 8.963 9.401 9.043 8.913 8.848 8.989\nLLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770"}
{"metadata": {"id": "recursive_chunk_028", "chunk_length": 893, "method": "recursive"}, "page_content": "LLaMA-Full Context6.751 6.956 6.829 6.701 9.675 9.069 8.963 9.401 9.043 8.913 8.848 8.989\nLLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG 8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG 16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG 32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:sfrom\ns/kchunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task."}
{"metadata": {"id": "recursive_chunk_029", "chunk_length": 986, "method": "recursive"}, "page_content": "s/kchunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjusting p). Notably, a compression rate of8can be obtained either\nby configuring REFRAG 16to compress the appropriate number of chunks, or by employing REFRAG 8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates that REFRAG 16with RL-based"}
{"metadata": {"id": "recursive_chunk_030", "chunk_length": 973, "method": "recursive"}, "page_content": "does the former approach outperform the latter? Table 13 demonstrates that REFRAG 16with RL-based\nselective compression consistently outperforms REFRAG 8across different datasets and context lengths.\n4 8 16\nCompression rate1.021.031.041.051.061.07Log-perplexity\nArxiv\n4 8 16\nCompression rate1.821.831.841.851.86Log-perplexity\nBook\n4 8 16\nCompression rate1.911.921.931.941.95Log-perplexity\nPG19\n4 8 16\nCompression rate0.870.880.890.900.910.92Log-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+ounder varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6\n0100101102\nNumber of Passages50525456Performance\nPerformance vs. Retrieved Passages\n0100101102103\nInput Length (Latency)50525456Performance\nPerformance vs. Latency"}
{"metadata": {"id": "recursive_chunk_031", "chunk_length": 902, "method": "recursive"}, "page_content": "6\n0100101102\nNumber of Passages50525456Performance\nPerformance vs. Retrieved Passages\n0100101102103\nInput Length (Latency)50525456Performance\nPerformance vs. Latency\nLlama REFRAG 8× compression\n0100101102\nNumber of Passages5254Performance\nPerformance vs. Retrieved Passages\n0100101102103\nInput Length (Latency)5254Performance\nPerformance vs. Latency\nLlama REFRAG 8× compressionFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, as REFRAG 16achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance of REFRAG 8. These results further highlight the"}
{"metadata": {"id": "recursive_chunk_032", "chunk_length": 927, "method": "recursive"}, "page_content": "chunk embeddings, yet still surpasses the performance of REFRAG 8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B"}
{"metadata": {"id": "recursive_chunk_033", "chunk_length": 984, "method": "recursive"}, "page_content": "model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in"}
{"metadata": {"id": "recursive_chunk_034", "chunk_length": 952, "method": "recursive"}, "page_content": "This observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG."}
{"metadata": {"id": "recursive_chunk_035", "chunk_length": 901, "method": "recursive"}, "page_content": "downstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant\nConversations Dataset.Open-Domain QA: CommonsenseQA, MathQA, Web Questions, Wiki Question\nAnswering, Yahoo! Answers QA, FreebaseQA, MS MARCO.Reading Comprehension: Discrete Reasoning\nOver Paragraphs, PubMedQA, QuaRel, SQuADv2.Chain-of-thought Reasoning: Algebra QA with Rationales,\nExplanations for CommonsenseQ, Grade School Math 8K, MathQA, StrategyQA.\n7\nEvaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation."}
{"metadata": {"id": "recursive_chunk_036", "chunk_length": 925, "method": "recursive"}, "page_content": "Explanations for CommonsenseQ, Grade School Math 8K, MathQA, StrategyQA.\n7\nEvaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error"}
{"metadata": {"id": "recursive_chunk_037", "chunk_length": 937, "method": "recursive"}, "page_content": "weak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency). LLaMA FTis"}
{"metadata": {"id": "recursive_chunk_038", "chunk_length": 928, "method": "recursive"}, "page_content": "This is used as a metric to gauge the latency of the model (the higher, the lower latency). LLaMA FTis\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage for LLaMA FTand use 8 passages for all our models. The baseline of REFRAG 8will\nhave the same latency as the LLaMA FTmodel. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly, REFRAG 16andREFRAG 32both\noutperform the LLaMA FTmodel despite having2 ×and4×fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests"}
{"metadata": {"id": "recursive_chunk_039", "chunk_length": 942, "method": "recursive"}, "page_content": "tasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal"}
{"metadata": {"id": "recursive_chunk_040", "chunk_length": 999, "method": "recursive"}, "page_content": "Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5 .26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1 .22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0 .71%and accelerates TTFT by5 .26×compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1."}
{"metadata": {"id": "recursive_chunk_041", "chunk_length": 966, "method": "recursive"}, "page_content": "2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperforms LLaMA FTon two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMA FT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe same LLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use"}
{"metadata": {"id": "recursive_chunk_042", "chunk_length": 963, "method": "recursive"}, "page_content": "3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8\nTable 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMA FT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG 8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG 16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG 32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMA FT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03"}
{"metadata": {"id": "recursive_chunk_043", "chunk_length": 946, "method": "recursive"}, "page_content": "CEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG 8+80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG 16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG 32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMA FT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG 8+ 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG 16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG 32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMA FT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -"}
{"metadata": {"id": "recursive_chunk_044", "chunk_length": 995, "method": "recursive"}, "page_content": "Long context\nLLaMA FT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG 8+80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG 16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG 32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMA FT2 20.7318.7226.98"}
{"metadata": {"id": "recursive_chunk_045", "chunk_length": 998, "method": "recursive"}, "page_content": "Table 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMA FT2 20.7318.7226.98\nREFRAG 8221.1717.73 28.04\nREFRAG 162 20.19 17.30 27.89\nREFRAG 322 19.70 17.3528.67\nLLaMA FT4 20.3316.4223.50\nREFRAG 84 22.78 15.61 26.93\nREFRAG 16421.9415.2727.03\nREFRAG 324 21.68 15.45 26.45\nLLaMA FT6 20.7611.9223.10\nREFRAG 8623.1110.88 25.37\nREFRAG 166 21.69 10.7526.17\nREFRAG 326 21.19 10.69 25.51# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMA FT2 16.52 17.31 23.02\nREFRAG 8221.15 17.9227.97\nREFRAG 162 20.79 17.3728.45\nREFRAG 322 19.67 17.16 28.31\nLLaMA FT4 16.90 14.69 20.23\nREFRAG 8422.63 15.6825.95\nREFRAG 164 21.84 15.2126.12\nREFRAG 324 21.75 15.33 25.77\nLLaMA FT6 14.44 10.72 19.52\nREFRAG 86 20.5911.0025.16\nREFRAG 166 21.05 10.50 24.96\nREFRAG 32621.6710.7925.23\nTable 5Performance on multi-turn RAG tasks with different number of passages.\nREFRAG LLaMA FT\n# Passages ORConvQA QReCC TopiOCQA↑ORConvQA QReCC TopiOCQA↑"}
{"metadata": {"id": "recursive_chunk_046", "chunk_length": 983, "method": "recursive"}, "page_content": "REFRAG 32621.6710.7925.23\nTable 5Performance on multi-turn RAG tasks with different number of passages.\nREFRAG LLaMA FT\n# Passages ORConvQA QReCC TopiOCQA↑ORConvQA QReCC TopiOCQA↑\n0 19.27 15.32 28.19 19.16 15.49 28.22\n5 20.18 17.3728.2419.6518.7127.08\n820.5217.60 28.17 16.87 18.05 25.36\n10 19.67 17.41 27.62 15.72 17.42 23.60\n6 Related Works\nRetrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi-\ntectures to improve retrieval-augmented generation. Guu et al. (2020) introduced pre-training for retrieval-\naugmented masked language models. Building on this, Borgeaud et al. (2022) proposed a new architecture\nand pre-training paradigm for generative LLMs, leveraging cross-attention and end-to-end pre-training with\n9\nretrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and"}
{"metadata": {"id": "recursive_chunk_047", "chunk_length": 942, "method": "recursive"}, "page_content": "and Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency."}
{"metadata": {"id": "recursive_chunk_048", "chunk_length": 950, "method": "recursive"}, "page_content": "memory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like"}
{"metadata": {"id": "recursive_chunk_049", "chunk_length": 992, "method": "recursive"}, "page_content": "context into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the"}
{"metadata": {"id": "recursive_chunk_050", "chunk_length": 951, "method": "recursive"}, "page_content": "decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within"}
{"metadata": {"id": "recursive_chunk_051", "chunk_length": 974, "method": "recursive"}, "page_content": "work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop"}
{"metadata": {"id": "recursive_chunk_052", "chunk_length": 920, "method": "recursive"}, "page_content": "rank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression\nand faster inference Li et al. (2023); Liskavets et al. (2024). These approaches are complementary to our work\nand can be integrated to further reduce the latency ofREFRAG.\n10\n7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG"}
{"metadata": {"id": "recursive_chunk_053", "chunk_length": 947, "method": "recursive"}, "page_content": "including RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30 .85×TTFT acceleration (3 .75×over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11\nReferences\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. TopiOCQA: Open-domain\nconversationalquestionansweringwithtopicswitching.Transactions of the Association for Computational Linguistics,"}
{"metadata": {"id": "recursive_chunk_054", "chunk_length": 984, "method": "recursive"}, "page_content": "conversationalquestionansweringwithtopicswitching.Transactions of the Association for Computational Linguistics,\n10:468–483, 04 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00471.https://doi.org/10.1162/tacl_a_00471.\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi.\nOpen-domain question answering goes conversational via question rewriting.Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proofpile: A pre-training dataset of mathematical\ntexts. https://huggingface.co/datasets/hoskinson-center/proof-pile , 2023. Dataset available on Hugging Face. The\ndataset is 13GB and contains 8.3 billion tokens of informal and formal mathematics from diverse sources including\narXiv.math, formal math libraries (Lean, Isabelle, Coq, HOL Light, Metamath, Mizar), Math Stack Exchange,"}
{"metadata": {"id": "recursive_chunk_055", "chunk_length": 899, "method": "recursive"}, "page_content": "arXiv.math, formal math libraries (Lean, Isabelle, Coq, HOL Light, Metamath, Mizar), Math Stack Exchange,\nWikipedia math articles, and more.\nIrwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with\nreinforcement learning. InWorkshop track of the International Conference on Learning Representations (ICLR),\n2017.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv:2004.05150,\n2020.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language. InThirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob"}
{"metadata": {"id": "recursive_chunk_056", "chunk_length": 947, "method": "recursive"}, "page_content": "Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob\nMenick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and\nLaurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International\nConference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,\n17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.\nIn Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on Empirical Methods"}
{"metadata": {"id": "recursive_chunk_057", "chunk_length": 950, "method": "recursive"}, "page_content": "In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 3829–3846, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.232.https://aclanthology.org/2023.emnlp-main.232.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\nPeter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,\nand Adrian Weller. Rethinking attention with performers. InInternational Conference on Learning Representations,\n2021.https://openreview.net/forum?id=Ua6zuk0WRH.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:"}
{"metadata": {"id": "recursive_chunk_058", "chunk_length": 926, "method": "recursive"}, "page_content": "2021.https://openreview.net/forum?id=Ua6zuk0WRH.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:\nExploring the surprising difficulty of natural yes/no questions. InNAACL, 2019.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian.\nA discourse-aware attention model for abstractive summarization of long documents. In Marilyn Walker, Heng Ji,\nand Amanda Stent, editors,Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New\nOrleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2097. https:\n//aclanthology.org/N18-2097/.\nHanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms"}
{"metadata": {"id": "recursive_chunk_059", "chunk_length": 943, "method": "recursive"}, "page_content": "//aclanthology.org/N18-2097/.\nHanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms\nover graphs. InAdvances in Neural Information Processing Systems (NeurIPS) 30, pages 6348–6358, 2017.\nYuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao.\nPretraining context compressor for large language models with embedding-based memory. In Wanxiang Che, Joyce\nNabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 28715–28732, Vienna, Austria, July\n12\n2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1394.\nhttps://aclanthology.org/2025.acl-long.1394/.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model"}
{"metadata": {"id": "recursive_chunk_060", "chunk_length": 994, "method": "recursive"}, "page_content": "https://aclanthology.org/2025.acl-long.1394/.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model\npre-training. InInternational conference on machine learning, pages 3929–3938. PMLR, 2020.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring\nmassive multitask language understanding. InProc. ICLR, 2021.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question\nanswering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors,Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, Online, April\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. https://aclanthology.org/\n2021.eacl-main.74/.\nGautier Izacard, Mostafa Dehghani, Sina Hosseini, Holger Schwenk, Fabio Petroni, and Sebastian Riedel. Few-shot"}
{"metadata": {"id": "recursive_chunk_061", "chunk_length": 914, "method": "recursive"}, "page_content": "2021.eacl-main.74/.\nGautier Izacard, Mostafa Dehghani, Sina Hosseini, Holger Schwenk, Fabio Petroni, and Sebastian Riedel. Few-shot\nlearning with retrieval augmented language models.arXiv preprint arXiv:2208.03299, 2022. https://arxiv.org/abs/\n2208.03299.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Edouard Grave, and Sebastian Riedel. Atlas: Few-shot learning with retrieval augmented language models.\nJ. Mach. Learn. Res., 24:37:1–37:37, 2023a.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models.\nJournal of Machine Learning Research, 24(251):1–43, 2023b.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for"}
{"metadata": {"id": "recursive_chunk_062", "chunk_length": 964, "method": "recursive"}, "page_content": "Journal of Machine Learning Research, 24(251):1–43, 2023b.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for\naccelerated inference of large language models. InProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 13358–13376, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.825.https://aclanthology.org/2023.emnlp-main.825/.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua:\nAccelerating and enhancing LLMs in long context scenarios via prompt compression. InProceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics.\nYuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector"}
{"metadata": {"id": "recursive_chunk_063", "chunk_length": 907, "method": "recursive"}, "page_content": "August 2024. Association for Computational Linguistics.\nYuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector\nand back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 19323–19339, Vienna, Austria, July 2025. Association\nfor Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. https://aclanthology.\norg/2025.acl-long.948/.\nBozhou Li, Hao Liang, Zimo Meng, and Wentao Zhang. Are bigger encoders always better in vision large models?\narXiv preprint arXiv:2408.00620, August 2024. Preprint.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large"}
{"metadata": {"id": "recursive_chunk_064", "chunk_length": 954, "method": "recursive"}, "page_content": "arXiv preprint arXiv:2408.00620, August 2024. Preprint.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large\nlanguage models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 6342–6353, Singapore, December 2023. Association for Computational Linguistics. https://aclanthology.org/\n2023.emnlp-main.391.pdf.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen tau Yih, and Xilun Chen.\nHow to train your dragon: Diverse augmentation towards generalizable dense retrieval. InThe 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2023.https://openreview.net/forum?id=d00kbjbYv2.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn,\nGergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-augmented dual instruction"}
{"metadata": {"id": "recursive_chunk_065", "chunk_length": 996, "method": "recursive"}, "page_content": "Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-augmented dual instruction\ntuning. InThe Twelfth International Conference on Learning Representations, 2024. https://openreview.net/\nforum?id=22OTbutug9.\nBarys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke. Prompt compression\nwith context-aware sentence encoding for fast and improved llm inference.arXiv preprint arXiv:2409.01227, 2024.\nhttps://arxiv.org/abs/2409.01227. Accepted at AAAI 2025.\n13\nJingyu Liu, Beidi Chen, and Ce Zhang. Speculative prefill: Turbocharging TTFT with lightweight and training-\nfree token importance estimation. InForty-second International Conference on Machine Learning, 2025. https:\n//openreview.net/forum?id=bzbuZ0ItBq.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.arXiv preprint arXiv:1907.11692,"}
{"metadata": {"id": "recursive_chunk_066", "chunk_length": 966, "method": "recursive"}, "page_content": "and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.arXiv preprint arXiv:1907.11692,\n2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on Learning\nRepresentations (ICLR), 2019.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\nJernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowledge intensive language tasks.arXiv\npreprint arXiv:2009.02252, 2020.\nChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. Open-Retrieval Conversational\nQuestion Answering. InSIGIR, 2020.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling.arXiv preprint, 2019.https://arxiv.org/abs/1911.05507.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive"}
{"metadata": {"id": "recursive_chunk_067", "chunk_length": 898, "method": "recursive"}, "page_content": "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive\ntransformers for long-range sequence modelling. InInternational Conference on Learning Representations, 2020.\nhttps://openreview.net/forum?id=SylKikSYDH.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael\nSmith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain chatbot. In Paola Merlo, Jorg\nTiedemann, and Reut Tsarfaty, editors,Proceedings of the 16th Conference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages 300–325, Online, April 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.eacl-main.24.https://aclanthology.org/2021.eacl-main.24/.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning"}
{"metadata": {"id": "recursive_chunk_068", "chunk_length": 966, "method": "recursive"}, "page_content": "Linguistics. doi: 10.18653/v1/2021.eacl-main.24.https://aclanthology.org/2021.eacl-main.24/.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning\nabout social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association\nfor Computational Linguistics.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms.arXiv preprint arXiv:1707.06347, 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.arXiv\npreprint arXiv:2402.03300, 2024."}
{"metadata": {"id": "recursive_chunk_069", "chunk_length": 970, "method": "recursive"}, "page_content": "Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.arXiv\npreprint arXiv:2402.03300, 2024.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and\nSteven Bethard, editors,Proceedings of the 2024 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8371–8384, Mexico\nCity, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. https:\n//aclanthology.org/2024.naacl-long.463/.\nXiaoxiang Shi, Colin Cai, and Junjia Du. Proactive intra-gpu disaggregation of prefill and decode in llm serving, 2025.\nhttps://arxiv.org/abs/2507.06608.\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPa-"}
{"metadata": {"id": "recursive_chunk_070", "chunk_length": 986, "method": "recursive"}, "page_content": "https://arxiv.org/abs/2507.06608.\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPa-\njama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama , June 2023. https://huggingface.co/\ndatasets/cerebras/SlimPajama-627B.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with\nattention sinks. InThe Twelfth International Conference on Learning Representations, 2024. https://openreview.\nnet/forum?id=NG7sS51zVF.\n14\nHoward Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In"}
{"metadata": {"id": "recursive_chunk_071", "chunk_length": 909, "method": "recursive"}, "page_content": "net/forum?id=NG7sS51zVF.\n14\nHoward Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021. https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen -Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large -scale generative pre -training for conversational response generation. In Asli Celikyilmaz\nand Tsung -Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15\nAppendix\nA Additional Discussion"}
{"metadata": {"id": "recursive_chunk_072", "chunk_length": 944, "method": "recursive"}, "page_content": "doi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15\nAppendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters: sas the context\nlength, oas the output length, bas the batch size, das the dimensionality of the hidden states, las the\nnumber of layers in the decoder, and nas the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU is mand we use the compression rate of kin our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the"}
{"metadata": {"id": "recursive_chunk_073", "chunk_length": 991, "method": "recursive"}, "page_content": "which is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength swe are able to achieve k×acceleration in TTFT and up to k×acceleration in throughput. With\nlonger context length s, we are able to achieve up to k2×acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53×acceleration in TTFT with cache and8 .59×without cache. Both higher than CEPE (i.e.,\n2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer"}
{"metadata": {"id": "recursive_chunk_074", "chunk_length": 999, "method": "recursive"}, "page_content": "2.01×and1 .04×acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32 .99×acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3 ×acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6 .78×and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memoryks+ko\ns+ko1∼k×k×\nTTFTk2(6ds+s2)\n6dsk+s2 k×k2×\nTTIT2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok1×k×\nThroughputk∗TTFT+k∗TTIT\nTTFT+kTTIT∼k2∗TTFT+k2∗TTIT"}
{"metadata": {"id": "recursive_chunk_075", "chunk_length": 901, "method": "recursive"}, "page_content": "Acceleration/SaveShortsLongs\nKV cache memoryks+ko\ns+ko1∼k×k×\nTTFTk2(6ds+s2)\n6dsk+s2 k×k2×\nTTIT2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok1×k×\nThroughputk∗TTFT+k∗TTIT\nTTFT+kTTIT∼k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT1∼k×k∼k2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context of stokens x1, . . . , x s, chunked into Lfixed-length chunks"}
{"metadata": {"id": "recursive_chunk_076", "chunk_length": 933, "method": "recursive"}, "page_content": "of token and chunk embeddings. Given a context of stokens x1, . . . , x s, chunked into Lfixed-length chunks\nC1, . . . , C L, we achieve a compression fraction of1 −pby randomly selecting T′:=pLchunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pick T′chunk indices l={lj}T′\nj=1, where lt∈[L]. The input arrangement is E(x,{l j}T′\nj=1) =\n{E1, . . . , E L}, with Ei=ecnk\niifi /∈ {l j}T′\nj=1(compressed), and Ei={ek∗i, . . . , ek∗i+k−1 }ifi∈ {l j}T′\nj=1\n16\n(uncompressed). This arrangement is input to the decoder Mdecto predict xs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to choose T′chunks from L"}
{"metadata": {"id": "recursive_chunk_077", "chunk_length": 987, "method": "recursive"}, "page_content": "at the beginning. Within our selective compression framework, the objective is to choose T′chunks from L\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]r(x, l)\ns.t.|l|=T′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy network πθthat takes chunk embeddings {ci}L\ni=1and sequentially selects T′chunk indices"}
{"metadata": {"id": "recursive_chunk_078", "chunk_length": 994, "method": "recursive"}, "page_content": "train an effective policy (see section 2).\nWe learn a policy network πθthat takes chunk embeddings {ci}L\ni=1and sequentially selects T′chunk indices\nl1, . . . , l T′, wherel t∈[L]. At staget, the policy samples from:\nπθ(lt=i|x,{l j}t−1\nj=1):=πθ(lt=i|{c j}L\nj=1,{lj}t−1\nj=1) =exp(s i−ni)PL\nj=1exp(s j−nj).\nwherenj=∞iffj∈ {l i}t−1\ni=1and0otherwise4;s= gθ({ci}i∈[L],i/∈{l j}t−1\nj=1)is the output of a two-layer\ntransformer network over chunk embeddings, producing logits ifor each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1as transformer input and do not recompute logitss iafter each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for each xwe randomly select\nGnumber of lengthT′action sequences{l(i)}G\ni=1. We have the following objective:\nJθ=1\nGPG\ni=1E x∼P(X),"}
{"metadata": {"id": "recursive_chunk_079", "chunk_length": 934, "method": "recursive"}, "page_content": "Gnumber of lengthT′action sequences{l(i)}G\ni=1. We have the following objective:\nJθ=1\nGPG\ni=1E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)1\nT′PT′\nt=1min\u0014\nπθ(l(i)\nt|x,{l(i)\nj}t−1\nj=1)\nπθold(l(i)\nt|x,{l(i)\nj}t−1\nj=1)A(i)\nt,clip\u0012\nπθ(l(i)\nt|x,{l(i)\nj}t−1\nj=1)\nπθold(l(i)\nt|x,{l(i)\nj}t−1\nj=1),1−ϵ,1 +ϵ\u0013\nA(i)\nt\u0015\n(1)\nwhere ϵis the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training, θis the current\npolicy and θoldis the policy fro the previous iteration, Atis the advantage function. We define our advantage\nfunction using the negative log-perplexity on theotokens x s+1:s+o:\nri=r\u0010\nx,{l(i)\nj}T′\nj=1\u0011\n=−M dec\u0010\nxs+1:s+o |E(x,{l(i)\nj}T′\nj=1)\u0011\n.\nWe compute the advantage function following GRPO as:\nA(i)\nt=ri−mean\u0000\n{ri}G\ni=1\u0001\nstd\u0000\n{ri}G\ni=1\u0001 .\nB Additional Details on Experimental Settings\nB.1 Additional Details on Baselines\nAll baseline models are based on the LLaMA-2-7B model (Touvron et al., 2023), unless otherwise specified, to"}
{"metadata": {"id": "recursive_chunk_080", "chunk_length": 960, "method": "recursive"}, "page_content": "i=1\u0001 .\nB Additional Details on Experimental Settings\nB.1 Additional Details on Baselines\nAll baseline models are based on the LLaMA-2-7B model (Touvron et al., 2023), unless otherwise specified, to\nensure a fair comparison since the previous methods are trained based on this model.5We do provide results\n4We adopt the masking mechanism from Pointer Networks (Bello et al., 2017) to constrain the action space.\n5Unless specified, we use the pre-trained checkpoint. The reason of choosing this model is that existing baselines (Yen et al.,\n2024; Shi et al., 2024) adapts LLaMA-2-7B. If we use other base model, we will have to retrain their model for fair comparison.\nWe show the effectiveness of our training recipe in table 14.\n17\nContext Text Sequence \nPrecomputable \nWho is the President of USA? Decoder Tokenizer & \nEmbedding \nDecoder Input Text Token Embedding \nChunk \nEmbedding \nRL-trained chunk expansion policy \nReward = - Log(Perplexity) \nDonald Trump"}
{"metadata": {"id": "recursive_chunk_081", "chunk_length": 998, "method": "recursive"}, "page_content": "Who is the President of USA? Decoder Tokenizer & \nEmbedding \nDecoder Input Text Token Embedding \nChunk \nEmbedding \nRL-trained chunk expansion policy \nReward = - Log(Perplexity) \nDonald Trump \nAnswer Figure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT= 4096tokens, where the first s= 2048tokens are referred to as the context tokens, and the remaining\no= 2048tokens are the output tokens, such that s+o=T. We evaluate the perplexity on xs+1:s+oin this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly on xs+1:s+owith only xs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity on xs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e., x1:T. Therefore, the"}
{"metadata": {"id": "recursive_chunk_082", "chunk_length": 992, "method": "recursive"}, "page_content": "we also input the whole sequence to the model, including the context tokens, i.e., x1:T. Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMA K:Similar to theLLaMA-Full Context, we pass last Ktokens xsK:sin addition to xs+1:s+oto\ncompute perplexity in xs+1:s+o. The performance of LLaMA Kfalls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feed x1:sinto their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K."}
{"metadata": {"id": "recursive_chunk_083", "chunk_length": 942, "method": "recursive"}, "page_content": "tokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUG Chatto refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nandREPLUG FTto refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:stokens and evaluating the perplexity on the output tokens xs+1:s+o. We use REFRAG kto denote\nour model with compression rate of k. We use REFRAG RLto refer to the model with selective compression\nusing our RL policy."}
{"metadata": {"id": "recursive_chunk_084", "chunk_length": 995, "method": "recursive"}, "page_content": "our model with compression rate of k. We use REFRAG RLto refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2 e−4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5 e−5since we train all the\n18\nparameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2 e−5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6)."}
{"metadata": {"id": "recursive_chunk_085", "chunk_length": 991, "method": "recursive"}, "page_content": "output size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np= 0.1(i.e., compression90%of the chunks) and randomly select pLchunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match"}
{"metadata": {"id": "recursive_chunk_086", "chunk_length": 906, "method": "recursive"}, "page_content": "the decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The"}
{"metadata": {"id": "recursive_chunk_087", "chunk_length": 996, "method": "recursive"}, "page_content": "proportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters: sas the context length, oas the output length, bas the batch size, das\n19\nStage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage050100PercentageContext\n1×k\n2×k\n4×k\n8×k\n16×k\n32×k\n64×k\n128×k\n256×kFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000"}
{"metadata": {"id": "recursive_chunk_088", "chunk_length": 931, "method": "recursive"}, "page_content": "1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.\nthe dimensionality of the hidden states, las the number of layers in the decoder, and nas the number of"}
{"metadata": {"id": "recursive_chunk_089", "chunk_length": 980, "method": "recursive"}, "page_content": "sequences.\nthe dimensionality of the hidden states, las the number of layers in the decoder, and nas the number of\nmodel parameters. The flop rate of the GPU is f, and the high bandwidth memory of the GPU is m. The\nmodel is loaded with bfloat16 precision. We focus our analysis on LLaMA-2-7B model. The results should be\ngeneralizable to other models.\nTTFT: Computationally Bounded AnalysisExisting work (Liu et al., 2025) has shown that the TTFT\nlatency is primarily limited by computation. The primary computations in each layer of LLaMA-2 involve\nattention calculations and feedforward layers. We follow the analysis in (Liu et al., 2025) to calculate the\nTTFT. Note that each operation involves both a multiplication and an addition, hence we multiply the flop\ncount by 2.\n•Attention Calculation:\n–QKV Projection:Transforms input from[b, s, d]to[d,3d], requiring6bsd2flops.\n–Attention Score Calculation: QKToperation from[ b, h, s, d/h ]×[b, h, d/h, s ], requiring2 bds2flops."}
{"metadata": {"id": "recursive_chunk_090", "chunk_length": 961, "method": "recursive"}, "page_content": "–QKV Projection:Transforms input from[b, s, d]to[d,3d], requiring6bsd2flops.\n–Attention Score Calculation: QKToperation from[ b, h, s, d/h ]×[b, h, d/h, s ], requiring2 bds2flops.\n–Attention Output Calculation:Weighted average of the value hidden state,[ b, h, s, s ]×[b, h, s, d/h ],\nrequiring2bds2flops.\n–Output Projection:[b, s, d]×[d, d], requiring2bsd2flops.\nThe total flops for attention is8bsd2+ 4bds2.\n•Feedforward Layer:In LLaMA-2-7B, the MLP layer first projects to2 .6875dwith a gated function\nand then back to d. Each projection requires5 .375bsd2flops. With three such operations, the total is\n20\n16.125bsd2.\n•Total Computation per Layer:Summing the above, each layer requires approximately24 bsd2+ 4bds2flops.\nFor a sequence length s, number of layers l, and batch size b, the total computation for pre-fill is(24 d2+4ds)lbs.\nGiven the flop rate f, the latency for pre-fill is dominated by computation, yielding a final latency of\n(24d2+4ds)lbs\nf."}
{"metadata": {"id": "recursive_chunk_091", "chunk_length": 994, "method": "recursive"}, "page_content": "Given the flop rate f, the latency for pre-fill is dominated by computation, yielding a final latency of\n(24d2+4ds)lbs\nf.\nGeneration analysis: Memory bounded AnalysisFor generation latency, existing work have shown that the\ngeneration process is memory bounded (Shi et al., 2025) which requires transferring KV cache and model\nparameter to high-bandwidth memory, we analyse the data transfer latency as follows:\n•Memory Latency:\n–KV Cache Data:Requires4 dlb(s+o)bytes (bfloat16 uses 2 bytes per number, and there are\nseparate key/value copies).\n–Model Parameters:Require2nbytes.\nThe data transfer latency to high-bandwidth memory is2n+4dlb(s+o)\nm.\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput=bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\u0000s\nk+o\u0001\nTTFT(24d2+4ds)lbs\nf(24d2+4ds\nk)lbs\nk\nf\nTTIT2n+4dlb(s+o)\nm2n+4dlb (s\nk+o)\nm\nThroughputbo\nTTFT before +TTIT beforebo\nTTFT after+TTIT after"}
{"metadata": {"id": "recursive_chunk_092", "chunk_length": 984, "method": "recursive"}, "page_content": "Before After\nKV cache memory4dlb(s+o) 4dlb\u0000s\nk+o\u0001\nTTFT(24d2+4ds)lbs\nf(24d2+4ds\nk)lbs\nk\nf\nTTIT2n+4dlb(s+o)\nm2n+4dlb (s\nk+o)\nm\nThroughputbo\nTTFT before +TTIT beforebo\nTTFT after+TTIT after\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results"}
{"metadata": {"id": "recursive_chunk_093", "chunk_length": 999, "method": "recursive"}, "page_content": "resource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21\nP0P1P2P3P4P0P1P2P3P4layer 0\nP0P1P2P3P4P0P1P2P3P4layer 1\nP0P1P2P3P4P0P1P2P3P4layer 2\nP0P1P2P3P4P0P1P2P3P4layer 3\nP0P1P2P3P4P0P1P2P3P4layer 4\nP0P1P2P3P4P0P1P2P3P4layer 5\nP0P1P2P3P4P0P1P2P3P4layer 6"}
{"metadata": {"id": "recursive_chunk_094", "chunk_length": 920, "method": "recursive"}, "page_content": "21\nP0P1P2P3P4P0P1P2P3P4layer 0\nP0P1P2P3P4P0P1P2P3P4layer 1\nP0P1P2P3P4P0P1P2P3P4layer 2\nP0P1P2P3P4P0P1P2P3P4layer 3\nP0P1P2P3P4P0P1P2P3P4layer 4\nP0P1P2P3P4P0P1P2P3P4layer 5\nP0P1P2P3P4P0P1P2P3P4layer 6\nP0P1P2P3P4P0P1P2P3P4layer 7\nP0P1P2P3P4P0P1P2P3P4layer 8\nP0P1P2P3P4P0P1P2P3P4layer 9\nP0P1P2P3P4P0P1P2P3P4layer 10\nP0P1P2P3P4P0P1P2P3P4layer 11\nP0P1P2P3P4P0P1P2P3P4layer 12\nP0P1P2P3P4P0P1P2P3P4layer 13\nP0P1P2P3P4P0P1P2P3P4layer 14\nP0P1P2P3P4P0P1P2P3P4layer 15\nP0P1P2P3P4P0P1P2P3P4layer 16\nP0P1P2P3P4P0P1P2P3P4layer 17\nP0P1P2P3P4P0P1P2P3P4layer 18\nP0P1P2P3P4P0P1P2P3P4layer 19\nP0P1P2P3P4P0P1P2P3P4layer 20\nP0P1P2P3P4P0P1P2P3P4layer 21\nP0P1P2P3P4P0P1P2P3P4layer 22\nP0P1P2P3P4P0P1P2P3P4layer 23\nP0P1P2P3P4P0P1P2P3P4layer 24\nP0P1P2P3P4P0P1P2P3P4layer 25\nP0P1P2P3P4P0P1P2P3P4layer 26\nP0P1P2P3P4P0P1P2P3P4layer 27\nP0P1P2P3P4P0P1P2P3P4layer 28\nP0P1P2P3P4P0P1P2P3P4layer 29\nP0P1P2P3P4P0P1P2P3P4layer 30\nP0P1P2P3P4P0P1P2P3P4layer 31"}
{"metadata": {"id": "recursive_chunk_095", "chunk_length": 958, "method": "recursive"}, "page_content": "P0P1P2P3P4P0P1P2P3P4layer 26\nP0P1P2P3P4P0P1P2P3P4layer 27\nP0P1P2P3P4P0P1P2P3P4layer 28\nP0P1P2P3P4P0P1P2P3P4layer 29\nP0P1P2P3P4P0P1P2P3P4layer 30\nP0P1P2P3P4P0P1P2P3P4layer 31\nall layers avgFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and"}
{"metadata": {"id": "recursive_chunk_096", "chunk_length": 962, "method": "recursive"}, "page_content": "success of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22\n103104\n# Input T okens0204060Acceleration\nTTFT Acceleration\n103104\n# Input T okens1.01.52.02.53.0Acceleration\nTTIT Acceleration\n103104\n# Input T okens246Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPEFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103104\n# Input T okens051015Acceleration\nTTFT Acceleration\n103104\n# Input T okens1.01.52.02.53.0Acceleration\nTTIT Acceleration\n103104\n# Input T okens246Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate."}
{"metadata": {"id": "recursive_chunk_097", "chunk_length": 926, "method": "recursive"}, "page_content": "Ablation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000200003000040000500006000070000\nTraining Steps1.46×1001.47×1001.48×1001.49×1001.5×1001.51×1001.52×100Lossx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression"}
{"metadata": {"id": "recursive_chunk_098", "chunk_length": 967, "method": "recursive"}, "page_content": "10000200003000040000500006000070000\nTraining Steps1.46×1001.47×1001.48×1001.49×1001.5×1001.51×1001.52×100Lossx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23\n100002000030000400005000060000\nTraining Steps1.441.461.481.50LossLlama-2-7B\nLlama-2-13B\n100002000030000400005000060000\nTraining Steps1.461.471.481.491.501.51LossRoberta-Base\nRoberta-LargeFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n100002000030000400005000060000\nTraining Steps1.441.451.461.471.481.491.50LossRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting."}
{"metadata": {"id": "recursive_chunk_099", "chunk_length": 917, "method": "recursive"}, "page_content": "outperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The"}
{"metadata": {"id": "recursive_chunk_100", "chunk_length": 973, "method": "recursive"}, "page_content": "REFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUG FTmeans that we adopt the REPLUG framework using LLaMA FT, and REPLUG Chat\nmeans that we adopt the LLaMA-2-7B-Chat model for REPLUG . We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG 16performs better than REFRAG 8at a decoder token count of 128, since the former model is"}
{"metadata": {"id": "recursive_chunk_101", "chunk_length": 953, "method": "recursive"}, "page_content": "REFRAG 16performs better than REFRAG 8at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24\nTable 10The 5 retrieved passages for the query “how bruce lee died”.\nContent\nP0 \"Water is necessary to survive, but as we all know, sometimes too much of a good thing (even\nwater) can be harmful. In 2022, a group of kidney specialists from Madrid, Spain, revisited the\ndeath of Kung Fu legend Bruce Lee and concluded that water intoxication was the most likely cause\nof his untimely death. Bruce Lee, the martial arts legend and iconic figure in the history of cinema,\ndied on July 20, 1973, at the young age of 32. The official cause of death at the time was reported\nas a probable drug reaction and classified as \"death by misadventure.\" Hours before his death, Lee\ncomplained of a headache while visiting a fellow actress Betty Ting Pei at her apartment. She gave"}
{"metadata": {"id": "recursive_chunk_102", "chunk_length": 925, "method": "recursive"}, "page_content": "as a probable drug reaction and classified as \"death by misadventure.\" Hours before his death, Lee\ncomplained of a headache while visiting a fellow actress Betty Ting Pei at her apartment. She gave\nhim one of her own prescription painkillers (one that contained aspirin and meprobamate), and he\nlaid down to take a nap. He never woke up and was unable to be resuscitated even after being\ntransferred to a Hong Kong hospital. In the years since Lee’s death, many theories have been put\nforward as to the true cause of his passing. These theories include murder by gangsters or a jilted\nlover, a family curse, epilepsy, heatstroke, and possibly\nP1 Bruce Lee May Have Died From Drinking Too Much Water, Claims Study The ’Enter The Dragon’\nactor, who helped bring martial arts into popular culture, died in July 1973 at the age of 32.\nAmerican martial arts legend and actor Bruce Lee might have died from drinking too much water,"}
{"metadata": {"id": "recursive_chunk_103", "chunk_length": 967, "method": "recursive"}, "page_content": "actor, who helped bring martial arts into popular culture, died in July 1973 at the age of 32.\nAmerican martial arts legend and actor Bruce Lee might have died from drinking too much water,\nscientists have claimed in a new study. The ’Enter The Dragon’ actor, who helped bring martial\narts into popular culture, died in July 1973 at the age of 32 from cerebral oedema, a swelling of\nthe brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops"}
{"metadata": {"id": "recursive_chunk_104", "chunk_length": 903, "method": "recursive"}, "page_content": "about how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was\nresponsible for Lee’s death and that perhaps she’d even poisoned him. Unfortunately, that wasn’t"}
{"metadata": {"id": "recursive_chunk_105", "chunk_length": 912, "method": "recursive"}, "page_content": "tabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was\nresponsible for Lee’s death and that perhaps she’d even poisoned him. Unfortunately, that wasn’t\nthe only rumor involving murder. One of the most popular theories says other martial artists were\nangry at Lee for teaching their secrets to Westerners, so they decided to bump him off. Some say\nninjas were responsible, and others claim Lee was killed with the \"Dim Mak,\" a mythical martial\narts move that supposedly kills a victim with one fateful blow. Others believe he was killed after\nrefusing to pay protection money to the Triads, while others claim the Mafia did the deed because\nLee wouldn’t let them control his career. The more mystical conspiracy theorists even say there’s a\nfamily curse that took the life\nP3 Bruce Lee complained of a headache, was given an Equagesic — a painkiller that contains both"}
{"metadata": {"id": "recursive_chunk_106", "chunk_length": 993, "method": "recursive"}, "page_content": "family curse that took the life\nP3 Bruce Lee complained of a headache, was given an Equagesic — a painkiller that contains both\naspirin and the tranquilizer meprobamate — and went down for a nap. He never woke up. His\ndeath was said to be an allergic reaction to the tranquilizer resulting in a cerebral edema (he had\nsuffered a previous edema months before), though others claim his death was due to a negative\nreaction to cannabis, which Lee consumed regularly to reduce stress. Because he was so young,\nnews of his death invited wild media speculation, from murder to a family curse. 5. Brandon Lee\nSadly, Bruce Lee’s son Brandon also died young, at age 28, and also under strange circumstances.\nWhile filming the horror film The Crow, Lee was accidentally killed by a prop gun that, due to a\nmalfunction in a previous scene, was accidentally loaded with a dummy bullet and a live primer.\nWhen the gun was fired, the bullet was ejected with virtually the same force as if loaded with a live"}
{"metadata": {"id": "recursive_chunk_107", "chunk_length": 956, "method": "recursive"}, "page_content": "malfunction in a previous scene, was accidentally loaded with a dummy bullet and a live primer.\nWhen the gun was fired, the bullet was ejected with virtually the same force as if loaded with a live\nround. Lee was hit in the abdomen and died in surgery later that day, on March 31, 1993. Like his\nfather, Brandon’s abrupt death fed rumors. Conspiracy theorists believe Illuminati\nP4 Bruce Lee moved to a house in Hong Kong’s Kowloon Tong district, it was said that the building\nsuffered from bad feng shui. According to Lee biographer Bruce Thomas, the house’s two previous\nowners had financial issues, and the building “faced the wrong way,” and had disturbed natural\nwinds. To fix this problem, a feng shui adviser ordered a mirror to be put on the roof. This was\nsupposed to deflect the bad energy, but the mirror was knocked off during a typhoon. Ominously,\nLee died just two days after the charm was blown away. While some of Lee’s neighbors apparently"}
{"metadata": {"id": "recursive_chunk_108", "chunk_length": 974, "method": "recursive"}, "page_content": "supposed to deflect the bad energy, but the mirror was knocked off during a typhoon. Ominously,\nLee died just two days after the charm was blown away. While some of Lee’s neighbors apparently\nlinked the two events at the time, the problem with this theory is that feng shui is nothing but a\nsuperstition. There’s no scientific evidence for any of its tenets, including qi. At most, feng shui\ncould be regarded as a kind of art. Lee’s death after the loss of his mirror is a simple coincidence.\nMoreover, Lee died in Betty Ting’s apartment, not in his own house. 2. Murder The abruptness\nof Bruce Lee’s death, combined with his extraordinary fitness, made some fans wonder whether\nsomething more sinister was at work. People who believe that Lee was murdered\n25\nTable 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021"}
{"metadata": {"id": "recursive_chunk_109", "chunk_length": 968, "method": "recursive"}, "page_content": "as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e., REFRAG 8) and\nselective compression (i.e.,REFRAG 16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG 8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916"}
{"metadata": {"id": "recursive_chunk_110", "chunk_length": 966, "method": "recursive"}, "page_content": "Compression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG 8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG 16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096\nREFRAG 8 8 1.098 1.065 1.042 1.895 1.860 1.837 1.989 1.950 1.922 0.965 0.923 0.894\nREFRAG 16+RL 8.01571.065 1.048 1.033 1.851 1.837 1.828 1.952 1.934 1.918 0.932 0.905 0.883\nTable 14Log-Perplexity of continual pre-training for different encoder-decoder combinations. Lower log-perplexity\nindicates better performance.\nEncoder–Decoder Context LengthLLaMA-3.1-8B LLaMA-3.2-3B\nP512 P1024 P2048 P512 P1024 P2048↓\nFull Context 2048 1.000 0.989 0.972 1.092 1.080 1.062\nNo Context 0 1.445 1.286 1.162 1.559 1.392 1.262\nRoberta-Base 2048 1.109 1.067 1.026 1.175 1.133 1.093\nRoberta-Large 2048 1.107 1.065 1.025 1.170 1.130 1.091\nRoberta-Base 4096 1.067 1.032 0.999 1.142 1.105 1.070"}
{"metadata": {"id": "recursive_chunk_111", "chunk_length": 938, "method": "recursive"}, "page_content": "Roberta-Base 2048 1.109 1.067 1.026 1.175 1.133 1.093\nRoberta-Large 2048 1.107 1.065 1.025 1.170 1.130 1.091\nRoberta-Base 4096 1.067 1.032 0.999 1.142 1.105 1.070\nRoberta-Large 4096 1.065 1.031 0.998 1.130 1.096 1.064\nTable 15Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the strong retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.07 18.73 65.80 34.67 60.20 89.18 87.42 68.89 43.72 67.25 70.18\n1 50.49 21.39 69.46 37.33 68.06 86.60 89.40 80.00 43.26 68.17 70.08\n3 50.49 22.01 66.02 38.67 71.01 89.18 95.36 71.11 45.50 68.73 71.44\n5 50.62 23.00 66.07 41.33 72.48 91.75 96.03 75.56 45.48 68.17 71.38\n8 50.29 22.96 66.59 38.67 73.46 92.27 94.70 75.56 45.23 68.94 71.38\n20 51.01 24.30 67.77 40.00 75.18 91.75 98.01 75.56 45.09 68.53 71.00\n50 51.08 24.76 69.39 40.00 75.92 91.75 97.35 75.56 44.78 67.81 69.97"}
{"metadata": {"id": "recursive_chunk_112", "chunk_length": 997, "method": "recursive"}, "page_content": "20 51.01 24.30 67.77 40.00 75.18 91.75 98.01 75.56 45.09 68.53 71.00\n50 51.08 24.76 69.39 40.00 75.92 91.75 97.35 75.56 44.78 67.81 69.97\n80 50.42 24.15 68.83 37.33 74.20 92.27 97.35 71.11 44.61 68.22 69.37\n100 50.23 23.99 69.80 36.00 74.45 92.27 97.35 71.11 44.57 68.07 69.75\n26\nTable 16Comparison of model performance of different models with different number of retrieved chunks for RAG. The\nnumber of contexts in all the evaluation here is 5.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑\nLLaMA FT 21.8861.85 7.96 34.6772.978.72 71.1129.54\nCEPE0.05 60.68 0.01 0.00 0.25 0.00 0.00 56.70\nREPLUG14.9671.5611.01 25.33 53.32 4.70 66.67 3.15\nLLaMA-32K2.26 0.23 2.17 14.67 9.83 0.67 4.44 0.06\nREFRAG 820.86 63.4412.3738.67 65.60 11.4173.333.06\nREFRAG 1620.60 60.45 11.8640.0066.09 11.41 73.33 5.57\nREFRAG 3221.39 61.97 12.03 40.00 67.3212.7568.89 1.80\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nLLaMA FT 49.9784.02 97.48 86.09 42.78 67.09 68.39 54.78"}
{"metadata": {"id": "recursive_chunk_113", "chunk_length": 969, "method": "recursive"}, "page_content": "REFRAG 3221.39 61.97 12.03 40.00 67.3212.7568.89 1.80\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nLLaMA FT 49.9784.02 97.48 86.09 42.78 67.09 68.39 54.78\nCEPE26.06 20.62 24.16 19.87 24.99 33.57 49.13 46.96\nREPLUG47.35 77.84 99.50 79.4749.2664.9971.9856.04\nLLaMA-32K24.17 18.04 22.32 15.89 24.09 16.84 28.02 48.78\nREFRAG 849.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG 1649.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG 3249.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMA FT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG 8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG 16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×"}
{"metadata": {"id": "recursive_chunk_114", "chunk_length": 964, "method": "recursive"}, "page_content": "REFRAG 8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG 16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG 32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMA FT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG 8+80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG 16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG 32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMA FT+ 1 context 48.86 82.99 99.50 84.77 42.08 67.91 67.46 55.491×\nREFRAG 8+ 8 passages50.10 91.24 99.66 96.03 45.15 68.17 70.40 57.461×\nREFRAG 16+ 8 passages49.77 90.21 99.66 96.6939.3268.73 70.46 56.432×"}
{"metadata": {"id": "recursive_chunk_115", "chunk_length": 983, "method": "recursive"}, "page_content": "REFRAG 8+ 8 passages50.10 91.24 99.66 96.03 45.15 68.17 70.40 57.461×\nREFRAG 16+ 8 passages49.77 90.21 99.66 96.6939.3268.73 70.46 56.432×\nREFRAG 32+ 8 passages50.10 91.7599.5096.03 42.36 68.83 68.28 55.804×\nLong context\nLLaMA FT+ 10 passages 45.20 83.51 63.42 85.43 41.43 67.60 67.36 54.301×\nCEPED+80 passages 26.52 24.74 23.83 22.52 24.97 32.86 48.80 44.20\nREPLUG+80 passages - - - 76.16 - 65.46 - 55.33\nLLaMA-32K+80 passages 22.01 18.04 19.97 16.56 23.69 23.80 33.19 48.62\nREFRAG 8+80 passages50.03 90.72 99.66 97.35 44.44 67.66 69.48 56.911×\nREFRAG 16+80 passages49.77 90.21 99.66 95.3638.2968.12 70.57 56.912×\nREFRAG 32+80 passages50.03 91.24 99.50 98.01 43.02 68.58 68.55 57.224×\n- means the corresponding model has out-of-memory error.\nTable 18Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑"}
{"metadata": {"id": "recursive_chunk_116", "chunk_length": 980, "method": "recursive"}, "page_content": "under the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.14 19.09 61.40 30.67 59.71 85.05 86.75 55.56 36.57 64.59 68.82\n1 49.97 20.08 64.15 38.67 64.62 87.63 92.72 71.11 39.08 68.58 70.57\n3 49.64 20.63 60.80 40.00 68.55 89.69 95.36 75.56 39.41 69.40 71.11\n5 49.84 20.60 60.45 40.00 66.09 90.21 96.69 73.33 39.52 68.63 70.95\n8 49.77 20.73 60.86 40.00 66.83 90.21 96.69 77.78 39.32 68.73 70.46\n20 50.03 21.29 62.32 36.00 68.06 89.69 95.36 75.56 38.58 69.29 70.62\n50 49.84 22.12 63.54 37.33 71.99 89.69 96.69 75.56 38.11 68.53 70.84\n80 49.77 22.63 65.07 38.67 71.74 90.21 95.36 68.89 38.29 68.12 70.57\n100 50.62 22.80 65.17 37.33 73.46 89.69 96.03 68.89 38.51 68.37 70.18\n27\nGround True Abstract Generated Abstract\nbackground : timely access to cardiovascular health services is\nnecessary to prevent heart damages . the present study examined\ninequality in geographical distribution of cardiovascular health"}
{"metadata": {"id": "recursive_chunk_117", "chunk_length": 984, "method": "recursive"}, "page_content": "background : timely access to cardiovascular health services is\nnecessary to prevent heart damages . the present study examined\ninequality in geographical distribution of cardiovascular health\nservicesiniran. methods: presentstudyisacross-sectionalstudy\nconducted using demographic data from all iranian provinces ( 31\nprovinces ) from 2012 census by the statistics center of iran ( sci ) .\ntheginicoefficientsofccubedsandcardiologistswereusedtoassess\nequality in access to cardiovascular health services in iran . ms\nexcel software was used to calculate gini coefficients . results : the\nproportionsofccubedandcardiologistper100,000populationwere\n4.88 and 1.27 , respectively ; also the gini coefficients were 0.129\nand 0.045 , respectively . conclusion : descriptive statistics showed\na skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular"}
{"metadata": {"id": "recursive_chunk_118", "chunk_length": 984, "method": "recursive"}, "page_content": "a skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular\nspecialists does not mean they are sufficiently available in iran .background : this study aimed to investigate the inequality of\ndistribution of cardiac care units ( ccu ) and cardiologists in iran\n. methods : this study used demographic data from national\nstatistics collected by the central statistics of iran ( sci ) in 2012 .\nthe number of ccu beds and cardiologists per 100,000 individuals\nand the number of cardiologists per 10 ccu beds were explored .\nthe gini coefficient was applied to measure inequality . results :\nthe mean number of ccu beds per 100,000 individuals in iran was\n4.88 , which is out of a total of 31 provinces below the national\nmean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was"}
{"metadata": {"id": "recursive_chunk_119", "chunk_length": 958, "method": "recursive"}, "page_content": "4.88 , which is out of a total of 31 provinces below the national\nmean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was\n2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the\ngini coefficient of ccu beds was 0.129 . conclusion : our descriptive\nstatistics showed that there is a skewness in the distribution of\npubic cardiovascular health services in iran . moreover , the equal\ndistribution of cardiovascular health facilities such as ccu beds is\nnot necessarily provided in iran .\nlumbar spinal stenosis is a commonly treated with epidural in-\njections of local anesthetics and corticosteroids , however , these\ntherapies may relieve leg pain for weeks to months but do not\ninfluence functional status . furthermore , the majority of pa-\ntients report no substantial symptom change over the repeated\ntreatment . utilizing balloon catheters , we successfully treated"}
{"metadata": {"id": "recursive_chunk_120", "chunk_length": 948, "method": "recursive"}, "page_content": "influence functional status . furthermore , the majority of pa-\ntients report no substantial symptom change over the repeated\ntreatment . utilizing balloon catheters , we successfully treated\nwith three patients who complained persistent symptoms despite\nrepeated conventional steroid injections . our results suggest that\ntransforaminal decompression using a balloon catheter may have\npotential in the nonsurgical treatment of spinal stenosis by modi-\nfying the underlying pathophysiology .epidural injection is a common treatment for spinal stenosis .\nhowever , there is little information on the optimal management of\nspinal stenosis . we describe the use of epidural balloon catheters to\ndecompress the intervertebral foramen in three patients with spinal\nstenosis . patients were followed - up for 24 weeks . one patient\nreported moderate pain relief , three patients reported symptom\nimprovement and one patient reported no change in symptoms"}
{"metadata": {"id": "recursive_chunk_121", "chunk_length": 923, "method": "recursive"}, "page_content": "stenosis . patients were followed - up for 24 weeks . one patient\nreported moderate pain relief , three patients reported symptom\nimprovement and one patient reported no change in symptoms\n. this report suggests that transforaminal balloon decompression\nusing a balloon may have potential in the nonsurgical treatment\nof spinal stenosis by modifying the underlying pathophysiology of\nsegmental spinal stenosis .\nwe describe a 26-year - old woman who presented with a nodular\nrash on the elbows following an insect bite . two days later , she\ndeveloped erythema nodosum . both these lesions were treated\nsymptomatically . one week later , she had purpura , abdominal\npain , hematuria , and arthralgias , following which steroids were\nadministered . her investigations revealed only microscopic hema-\nturia that disappeared with therapy . this pattern of sequential\nappearance of rash and a nodular morphology are both unique"}
{"metadata": {"id": "recursive_chunk_122", "chunk_length": 998, "method": "recursive"}, "page_content": "administered . her investigations revealed only microscopic hema-\nturia that disappeared with therapy . this pattern of sequential\nappearance of rash and a nodular morphology are both unique\nfeatures not previously reported .we report a case of herpes simplex purpura ( hsp ) that presented\nwith a sequential pattern of rashes following an insect bite . the\npatient was a 26-year - old woman who presented to our outpatient\ndepartment(opd)withanodularrashonherelbowsanderythema\nnodosum on her lower limbs following an insect bite . she had\npurpura on her lower limbs 2 weeks later . she had similar lesions\non both upper and lower limbs 1 week after a second insect bite .\nthis pattern of rashes has not been previously reported in hsp .\nTable 19Comparison of ground true abstract and abstract generated byREFRAGfor PubMed.\n28\nGround True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often"}
{"metadata": {"id": "recursive_chunk_123", "chunk_length": 957, "method": "recursive"}, "page_content": "28\nGround True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types"}
{"metadata": {"id": "recursive_chunk_124", "chunk_length": 994, "method": "recursive"}, "page_content": "that removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .memory - based learning is a prominent and successful approach to\nmachine learning . however , its use in nlp tasks has been limited\nby its over - generalisation properties . this paper investigates two\noptimisations that can be applied to memory - based learning in\norder to improve its generalisation performance : ( 1 ) replacing\ninstance types ( memory tokens ) by instance types with frequency\ninformation , and ( 2 ) removing redundant information ( i.e. ,\ninstance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation"}
{"metadata": {"id": "recursive_chunk_125", "chunk_length": 998, "method": "recursive"}, "page_content": "instance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -"}
{"metadata": {"id": "recursive_chunk_126", "chunk_length": 995, "method": "recursive"}, "page_content": "bility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .ewe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints"}
{"metadata": {"id": "recursive_chunk_127", "chunk_length": 986, "method": "recursive"}, "page_content": "particle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints\n. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-\nnary classification problem , we constrain each class as a cluster ,\nwhich is enclosed by an ellipsoid . the estimation of the optimal\nhyperplane between the two clusters is posed as a quadratically\nconstrained quadratic problem . the optimization problem is solved\nin distributed format using modified particle swarms . our method\nhas the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the"}
{"metadata": {"id": "recursive_chunk_128", "chunk_length": 991, "method": "recursive"}, "page_content": "has the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the\niris , pima , wine , and thyroid datasets show that the proposed\nmethod works better than a neural network and the performance\nis close to that of svm . * keywords * quadratic programming\n; particle swarms ; hyperplane ; quadratic constraints ; binary\nclassification .support vector machines are used for classification of data in\nmachine learning . support vector machines use quadratic pro-\ngramming formulation for minimizing the objective function . the\nquadratic programming problem is solved by particle swarm opti-\nmization . the proposed method is compared with khachiya s and\nkarman s support vector machine algorithms for linear and neural\nnetworks and quadratic programming . the results show that the\nproposed method is better than the other two methods .\nTable 20Comparison of ground true abstract and abstract generated byREFRAGfor ArXiv.\n29"}
{"metadata": {"id": "recursive_chunk_129", "chunk_length": 975, "method": "recursive"}, "page_content": "proposed method is better than the other two methods .\nTable 20Comparison of ground true abstract and abstract generated byREFRAGfor ArXiv.\n29\nTable 21Performance on summarization tasks under the same latency.\nArxiv Pubmed\nRouge-1 Rouge-2 Rouge-L Rouge-1 Rouge-2 Rouge-L↑\n# Decoder tokens = 128\nLLaMA FT 29.69 6.89 18.28 29.79 8.37 18.41\nCEPED12.67 1.66 8.39 12.01 1.41 7.74\nREPLUG FT 5.30 0.78 3.77 5.11 0.81 3.55\nREPLUG Chat 15.11 1.58 9.80 14.94 1.51 9.40\nLLaMA-32K2.83 0.48 2.11 7.94 1.63 5.31\nREFRAG 8 36.50 12.48 22.21 38.27 13.91 23.20\nREFRAG 16 38.48 12.50 22.66 38.93 12.83 23.07\n# Decoder tokens =512\nLLaMA FT 36.03 11.16 21.49 38.15 14.36 23.27\nCEPED19.28 3.16 12.22 17.60 2.43 10.89\nREPLUG FT 28.33 6.42 17.04 28.29 7.59 16.97\nREPLUG Chat 31.41 7.00 18.32 30.67 7.13 17.56\nLLaMA-32K3.03 0.65 2.28 8.49 2.54 5.47\nREFRAG 8 41.95 15.56 24.84 43.55 17.53 26.38\n# Decoder tokens =1024\nLLaMA FT 41.24 15.07 24.45 42.45 17.58 26.11\nCEPED25.20 5.07 15.45 23.00 3.94 13.71"}
{"metadata": {"id": "recursive_chunk_130", "chunk_length": 368, "method": "recursive"}, "page_content": "LLaMA-32K3.03 0.65 2.28 8.49 2.54 5.47\nREFRAG 8 41.95 15.56 24.84 43.55 17.53 26.38\n# Decoder tokens =1024\nLLaMA FT 41.24 15.07 24.45 42.45 17.58 26.11\nCEPED25.20 5.07 15.45 23.00 3.94 13.71\nREPLUG FT 19.32 3.18 12.73 17.07 2.93 11.20\nREPLUG Chat 27.38 5.46 16.84 27.89 5.16 15.93\nLLaMA-32K4.34 0.95 3.35 10.19 3.11 6.47\nREFRAG 8 43.88 17.03 26.01 44.43 18.06 26.85\n30"}
