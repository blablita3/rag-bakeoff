retrieval_method,chunking_method,run,question_id,question,answers,correct_answer,paper_reference,retrieved_chunks,llm_answer,llm_source,is_x,answer_correct,source_correct,latency
baseline,recursive,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.722
baseline,recursive,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.257
baseline,recursive,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.563
baseline,recursive,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.614
baseline,recursive,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.484
baseline,recursive,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.479
baseline,recursive,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.531
baseline,recursive,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.598
baseline,recursive,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.476
baseline,recursive,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.632
baseline,recursive,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.272
baseline,recursive,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.443
baseline,recursive,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.541
baseline,recursive,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.479
baseline,recursive,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.653
baseline,recursive,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.476
baseline,recursive,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.565
baseline,recursive,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.5
baseline,recursive,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.459
baseline,recursive,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.499
baseline,recursive,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.661
baseline,recursive,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.517
baseline,recursive,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.313
baseline,recursive,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.518
baseline,recursive,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.448
baseline,recursive,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.605
baseline,recursive,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.547
baseline,recursive,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.564
baseline,recursive,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.469
baseline,recursive,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.269
baseline,recursive,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.489
baseline,recursive,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.617
baseline,recursive,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.32
baseline,recursive,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.536
baseline,recursive,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.448
baseline,recursive,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.426
baseline,recursive,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.394
baseline,recursive,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.494
baseline,recursive,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.474
baseline,recursive,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.531
baseline,recursive,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.283
baseline,recursive,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.466
baseline,recursive,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.438
baseline,recursive,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.444
baseline,recursive,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.584
baseline,recursive,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.487
baseline,recursive,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.431
baseline,recursive,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.285
baseline,recursive,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.583
baseline,recursive,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.26
baseline,recursive,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.637
baseline,recursive,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.52
baseline,recursive,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.471
baseline,recursive,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.5
baseline,recursive,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.523
baseline,recursive,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.554
baseline,recursive,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.477
baseline,recursive,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.417
baseline,recursive,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.289
baseline,recursive,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.419
baseline,recursive,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.468
baseline,recursive,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.447
baseline,recursive,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.58
baseline,recursive,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.471
baseline,recursive,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.559
baseline,recursive,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.308
baseline,recursive,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.43
baseline,recursive,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.306
baseline,recursive,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.482
baseline,recursive,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.942
baseline,recursive,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.502
baseline,recursive,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.308
baseline,recursive,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.556
baseline,recursive,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.594
baseline,recursive,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.3
baseline,recursive,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.546
baseline,recursive,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.507
baseline,recursive,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.902
baseline,recursive,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.421
baseline,recursive,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.491
baseline,recursive,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.698
baseline,recursive,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.263
baseline,recursive,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.49
baseline,recursive,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.503
baseline,recursive,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.528
baseline,recursive,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.479
baseline,recursive,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.331
baseline,recursive,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.346
baseline,recursive,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.506
baseline,recursive,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.584
baseline,recursive,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.314
baseline,recursive,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.452
baseline,recursive,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.561
baseline,recursive,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.493
baseline,recursive,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.498
baseline,recursive,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.248
baseline,recursive,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.545
baseline,recursive,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.497
baseline,recursive,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.38
baseline,recursive,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,2.42
baseline,recursive,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.414
baseline,recursive,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.44
baseline,recursive,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.43
baseline,recursive,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.794
baseline,recursive,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.516
baseline,recursive,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.76
baseline,recursive,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.549
baseline,recursive,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.842
baseline,recursive,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.424
baseline,recursive,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.441
baseline,recursive,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.428
baseline,recursive,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.544
baseline,recursive,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.487
baseline,recursive,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.541
baseline,recursive,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.489
baseline,recursive,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.471
baseline,recursive,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.433
baseline,recursive,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.453
baseline,recursive,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,1.204
baseline,recursive,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.588
baseline,recursive,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.409
baseline,recursive,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.271
baseline,recursive,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,1.113
baseline,recursive,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.445
baseline,recursive,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.422
baseline,recursive,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.809
baseline,recursive,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.264
baseline,recursive,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.562
baseline,recursive,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.263
baseline,recursive,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.431
baseline,recursive,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.286
baseline,recursive,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.284
baseline,recursive,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.414
baseline,recursive,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.487
baseline,recursive,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.603
baseline,recursive,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.279
baseline,recursive,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.3
baseline,recursive,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.433
baseline,recursive,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.429
baseline,recursive,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.484
baseline,recursive,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.666
baseline,recursive,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.494
baseline,recursive,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.493
baseline,recursive,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.473
baseline,recursive,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.442
baseline,recursive,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.289
baseline,recursive,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.441
baseline,recursive,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.515
baseline,recursive,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.46
baseline,recursive,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.314
baseline,recursive,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.273
baseline,recursive,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.437
baseline,recursive,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.472
baseline,recursive,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.327
baseline,recursive,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.493
baseline,recursive,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.367
baseline,recursive,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.54
baseline,recursive,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.475
baseline,recursive,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.289
baseline,recursive,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.283
baseline,recursive,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.343
baseline,recursive,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.249
baseline,recursive,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.261
baseline,recursive,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,2.469
baseline,recursive,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.745
baseline,recursive,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.44
baseline,recursive,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.524
baseline,recursive,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.426
baseline,recursive,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.49
baseline,recursive,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.427
baseline,recursive,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.256
baseline,recursive,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.464
baseline,recursive,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.616
baseline,recursive,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.309
baseline,recursive,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.293
baseline,recursive,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.594
baseline,recursive,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.522
baseline,recursive,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.433
baseline,recursive,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.38
baseline,recursive,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.444
baseline,recursive,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.457
baseline,recursive,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.268
baseline,recursive,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.527
baseline,recursive,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.277
baseline,recursive,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.322
baseline,recursive,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.484
baseline,recursive,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.366
baseline,recursive,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.566
baseline,recursive,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.509
baseline,recursive,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.433
baseline,recursive,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.391
baseline,recursive,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.447
baseline,recursive,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.537
baseline,recursive,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.587
baseline,recursive,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.393
baseline,recursive,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.295
baseline,recursive,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.416
baseline,recursive,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.311
baseline,recursive,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.52
baseline,recursive,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.492
baseline,recursive,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.411
baseline,recursive,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,2.633
baseline,recursive,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.482
baseline,recursive,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.476
baseline,recursive,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.434
baseline,recursive,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.308
baseline,recursive,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.424
baseline,recursive,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.545
baseline,recursive,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.611
baseline,recursive,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.461
baseline,recursive,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.778
baseline,recursive,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.828
baseline,recursive,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.295
baseline,recursive,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.481
baseline,recursive,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.532
baseline,recursive,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.512
baseline,recursive,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.528
baseline,recursive,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.491
baseline,recursive,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.554
baseline,recursive,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.431
baseline,recursive,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.601
baseline,recursive,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.359
baseline,recursive,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.493
baseline,recursive,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.425
baseline,recursive,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.571
baseline,recursive,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.579
baseline,recursive,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.701
baseline,recursive,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.485
baseline,recursive,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.256
baseline,recursive,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.534
baseline,recursive,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.27
baseline,recursive,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.406
baseline,recursive,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.469
baseline,recursive,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.324
baseline,recursive,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.253
baseline,recursive,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.465
baseline,recursive,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.44
baseline,recursive,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.437
baseline,recursive,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.485
baseline,recursive,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.292
baseline,recursive,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.542
baseline,recursive,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.293
baseline,recursive,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.444
baseline,recursive,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.306
baseline,recursive,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.459
baseline,recursive,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.473
baseline,recursive,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.37
baseline,recursive,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.447
baseline,recursive,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.48
baseline,recursive,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.275
baseline,recursive,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.502
baseline,recursive,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.554
baseline,recursive,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.499
baseline,recursive,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.314
baseline,recursive,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.469
baseline,recursive,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.429
baseline,recursive,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.384
baseline,recursive,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.737
baseline,recursive,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.524
baseline,recursive,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.631
baseline,recursive,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.527
baseline,recursive,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.398
baseline,recursive,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.417
baseline,recursive,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.485
baseline,recursive,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.341
baseline,recursive,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.351
baseline,recursive,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.427
baseline,recursive,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.497
baseline,recursive,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.458
baseline,recursive,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.464
baseline,recursive,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.245
baseline,recursive,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.393
baseline,recursive,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.423
baseline,recursive,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.278
baseline,recursive,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.537
baseline,recursive,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.466
baseline,recursive,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.472
baseline,recursive,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.585
baseline,recursive,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.271
baseline,recursive,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.298
baseline,recursive,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.424
baseline,recursive,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.448
baseline,recursive,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,1.186
baseline,recursive,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.426
baseline,recursive,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.571
baseline,recursive,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.455
baseline,recursive,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.434
baseline,recursive,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.663
baseline,recursive,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.537
baseline,recursive,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.268
baseline,recursive,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.485
baseline,recursive,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.479
baseline,recursive,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.281
baseline,recursive,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.864
baseline,recursive,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,2.341
baseline,recursive,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.47
baseline,recursive,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.28
baseline,recursive,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.483
baseline,recursive,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.593
baseline,recursive,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.545
baseline,recursive,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.475
baseline,recursive,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.539
baseline,recursive,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.462
baseline,recursive,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.276
baseline,recursive,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.573
baseline,recursive,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.469
baseline,recursive,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.609
baseline,recursive,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.407
baseline,recursive,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.497
baseline,recursive,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.435
baseline,recursive,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.237
baseline,recursive,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.591
baseline,recursive,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.274
baseline,recursive,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.376
baseline,recursive,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,1.005
baseline,recursive,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.48
baseline,recursive,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.39
baseline,recursive,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.441
baseline,recursive,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.533
baseline,recursive,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.306
baseline,recursive,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.461
baseline,recursive,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.315
baseline,recursive,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.516
baseline,recursive,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,1.752
baseline,recursive,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.53
baseline,recursive,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.309
baseline,recursive,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.483
baseline,recursive,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.65
baseline,recursive,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.429
baseline,recursive,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.553
baseline,recursive,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.454
baseline,recursive,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.419
baseline,recursive,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.506
baseline,recursive,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.23
baseline,recursive,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.421
baseline,recursive,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.294
baseline,recursive,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.459
baseline,recursive,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.264
baseline,recursive,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.488
baseline,recursive,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.492
baseline,recursive,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.477
baseline,recursive,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.568
baseline,recursive,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.568
baseline,recursive,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.539
baseline,recursive,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.401
baseline,recursive,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.538
baseline,recursive,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.465
baseline,recursive,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.473
baseline,recursive,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.51
baseline,recursive,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.568
bm25,recursive,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_100', 'recursive_chunk_005', 'recursive_chunk_033']",C,['recursive_chunk_100'],False,True,False,0.281
bm25,recursive,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.488
bm25,recursive,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_049', 'recursive_chunk_048', 'recursive_chunk_029']",B,['recursive_chunk_048'],False,True,True,0.538
bm25,recursive,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_038', 'recursive_chunk_041', 'recursive_chunk_009']",D,['recursive_chunk_041'],False,True,True,0.568
bm25,recursive,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_000', 'recursive_chunk_008', 'recursive_chunk_099']",C,"['recursive_chunk_000', 'recursive_chunk_008']",False,True,True,0.609
bm25,recursive,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.323
bm25,recursive,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.333
bm25,recursive,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_073', 'recursive_chunk_074', 'recursive_chunk_014']",B,"['recursive_chunk_073', 'recursive_chunk_074']",False,True,True,0.555
bm25,recursive,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_014']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.72
bm25,recursive,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.554
bm25,recursive,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_083']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.515
bm25,recursive,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_048', 'recursive_chunk_050']",C,['recursive_chunk_008'],False,True,True,0.464
bm25,recursive,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_073', 'recursive_chunk_014', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.284
bm25,recursive,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_019', 'recursive_chunk_006', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.292
bm25,recursive,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.495
bm25,recursive,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.467
bm25,recursive,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_024', 'recursive_chunk_097', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.54
bm25,recursive,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_050', 'recursive_chunk_048', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.272
bm25,recursive,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.34
bm25,recursive,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_032', 'recursive_chunk_050']",C,['recursive_chunk_050'],False,True,False,0.581
bm25,recursive,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.709
bm25,recursive,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_049', 'recursive_chunk_073', 'recursive_chunk_047']",C,['recursive_chunk_073'],False,True,True,0.598
bm25,recursive,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_049', 'recursive_chunk_051']",C,['recursive_chunk_008'],False,True,True,0.278
bm25,recursive,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_016', 'recursive_chunk_018', 'recursive_chunk_015']",C,['recursive_chunk_016'],False,True,True,0.44
bm25,recursive,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",C,"['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",False,True,True,0.554
bm25,recursive,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_073', 'recursive_chunk_013', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_014']",False,True,True,0.273
bm25,recursive,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.501
bm25,recursive,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_050', 'recursive_chunk_024']",C,['recursive_chunk_023'],False,True,True,0.522
bm25,recursive,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_072', 'recursive_chunk_029', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,True,0.434
bm25,recursive,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_041', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.411
bm25,recursive,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_050', 'recursive_chunk_007', 'recursive_chunk_048']",A,['recursive_chunk_007'],False,False,True,0.603
bm25,recursive,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_038', 'recursive_chunk_037', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.293
bm25,recursive,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_024', 'recursive_chunk_073', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.566
bm25,recursive,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.348
bm25,recursive,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.608
bm25,recursive,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_029', 'recursive_chunk_076', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_076']",False,True,True,0.451
bm25,recursive,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_023', 'recursive_chunk_040']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.487
bm25,recursive,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_014', 'recursive_chunk_033']",X,[],True,False,,0.475
bm25,recursive,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.716
bm25,recursive,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_036']",C,['recursive_chunk_040'],False,True,True,0.504
bm25,recursive,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_007', 'recursive_chunk_047']",C,['recursive_chunk_007'],False,True,True,0.499
bm25,recursive,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.522
bm25,recursive,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_029', 'recursive_chunk_048']",C,"['recursive_chunk_012', 'recursive_chunk_029']",False,True,True,0.483
bm25,recursive,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_096', 'recursive_chunk_079']",B,['recursive_chunk_048'],False,False,True,0.463
bm25,recursive,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_029', 'recursive_chunk_032', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_029']",False,True,True,0.6
bm25,recursive,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_001'],False,True,True,0.502
bm25,recursive,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_003', 'recursive_chunk_002']",B,['recursive_chunk_004'],False,True,True,0.508
bm25,recursive,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_088', 'recursive_chunk_029', 'recursive_chunk_126']",B,"['recursive_chunk_088', 'recursive_chunk_029']",False,True,True,0.472
bm25,recursive,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.307
bm25,recursive,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_125', 'recursive_chunk_048']",X,[],True,False,,0.332
bm25,recursive,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.575
bm25,recursive,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_018']",C,['recursive_chunk_008'],False,True,True,0.284
bm25,recursive,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_017', 'recursive_chunk_012', 'recursive_chunk_011']",C,"['recursive_chunk_011', 'recursive_chunk_017']",False,True,True,0.439
bm25,recursive,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.712
bm25,recursive,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.607
bm25,recursive,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.776
bm25,recursive,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_024', 'recursive_chunk_025']",A,['recursive_chunk_025'],False,False,False,0.502
bm25,recursive,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_024', 'recursive_chunk_041', 'recursive_chunk_023']",A,['recursive_chunk_023'],False,False,True,0.512
bm25,recursive,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.469
bm25,recursive,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_015', 'recursive_chunk_014']",C,['recursive_chunk_048'],False,True,True,2.885
bm25,recursive,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.515
bm25,recursive,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.47
bm25,recursive,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_076', 'recursive_chunk_029']",C,"['recursive_chunk_024', 'recursive_chunk_076']",False,True,True,0.558
bm25,recursive,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.664
bm25,recursive,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,2.861
bm25,recursive,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.496
bm25,recursive,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.605
bm25,recursive,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_083', 'recursive_chunk_002', 'recursive_chunk_082']",C,['002'],False,True,False,0.306
bm25,recursive,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_029', 'recursive_chunk_014']",X,[],True,False,,0.448
bm25,recursive,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_029', 'recursive_chunk_097', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.455
bm25,recursive,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_100', 'recursive_chunk_005', 'recursive_chunk_033']",C,['recursive_chunk_100'],False,True,False,0.328
bm25,recursive,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.619
bm25,recursive,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_049', 'recursive_chunk_048', 'recursive_chunk_029']",B,['recursive_chunk_048'],False,True,True,0.57
bm25,recursive,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_038', 'recursive_chunk_041', 'recursive_chunk_009']",D,['recursive_chunk_041'],False,True,True,0.972
bm25,recursive,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_000', 'recursive_chunk_008', 'recursive_chunk_099']",C,"['recursive_chunk_000', 'recursive_chunk_008']",False,True,True,0.337
bm25,recursive,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.571
bm25,recursive,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.288
bm25,recursive,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_073', 'recursive_chunk_074', 'recursive_chunk_014']",B,"['recursive_chunk_073', 'recursive_chunk_074']",False,True,True,0.387
bm25,recursive,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_014']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.456
bm25,recursive,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.442
bm25,recursive,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_083']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.552
bm25,recursive,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_048', 'recursive_chunk_050']",C,['recursive_chunk_008'],False,True,True,0.471
bm25,recursive,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_073', 'recursive_chunk_014', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.553
bm25,recursive,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_019', 'recursive_chunk_006', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.574
bm25,recursive,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.388
bm25,recursive,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.476
bm25,recursive,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_024', 'recursive_chunk_097', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.332
bm25,recursive,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_050', 'recursive_chunk_048', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.422
bm25,recursive,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.518
bm25,recursive,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_032', 'recursive_chunk_050']",C,['recursive_chunk_050'],False,True,False,0.331
bm25,recursive,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.651
bm25,recursive,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_049', 'recursive_chunk_073', 'recursive_chunk_047']",C,['recursive_chunk_073'],False,True,True,0.286
bm25,recursive,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_049', 'recursive_chunk_051']",C,['recursive_chunk_008'],False,True,True,0.348
bm25,recursive,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_016', 'recursive_chunk_018', 'recursive_chunk_015']",C,['recursive_chunk_016'],False,True,True,0.493
bm25,recursive,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",C,"['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",False,True,True,0.481
bm25,recursive,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_073', 'recursive_chunk_013', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_014']",False,True,True,0.532
bm25,recursive,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.51
bm25,recursive,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_050', 'recursive_chunk_024']",C,['recursive_chunk_023'],False,True,True,0.288
bm25,recursive,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_072', 'recursive_chunk_029', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,True,0.449
bm25,recursive,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_041', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.489
bm25,recursive,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_050', 'recursive_chunk_007', 'recursive_chunk_048']",A,['recursive_chunk_007'],False,False,True,0.446
bm25,recursive,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_038', 'recursive_chunk_037', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.458
bm25,recursive,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_024', 'recursive_chunk_073', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.463
bm25,recursive,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.842
bm25,recursive,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.316
bm25,recursive,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_029', 'recursive_chunk_076', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_076']",False,True,True,0.874
bm25,recursive,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_023', 'recursive_chunk_040']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.364
bm25,recursive,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_014', 'recursive_chunk_033']",X,[],True,False,,0.591
bm25,recursive,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.303
bm25,recursive,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_036']",C,['recursive_chunk_040'],False,True,True,0.5
bm25,recursive,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_007', 'recursive_chunk_047']",C,['recursive_chunk_007'],False,True,True,0.46
bm25,recursive,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.45
bm25,recursive,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_029', 'recursive_chunk_048']",C,"['recursive_chunk_012', 'recursive_chunk_029']",False,True,True,0.618
bm25,recursive,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_096', 'recursive_chunk_079']",B,['recursive_chunk_048'],False,False,True,0.47
bm25,recursive,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_029', 'recursive_chunk_032', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_029']",False,True,True,0.62
bm25,recursive,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_001'],False,True,True,0.498
bm25,recursive,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_003', 'recursive_chunk_002']",B,['recursive_chunk_004'],False,True,True,0.546
bm25,recursive,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_088', 'recursive_chunk_029', 'recursive_chunk_126']",B,"['recursive_chunk_088', 'recursive_chunk_029']",False,True,True,0.548
bm25,recursive,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.456
bm25,recursive,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_125', 'recursive_chunk_048']",X,[],True,False,,0.521
bm25,recursive,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.449
bm25,recursive,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_018']",C,['recursive_chunk_008'],False,True,True,0.457
bm25,recursive,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_017', 'recursive_chunk_012', 'recursive_chunk_011']",C,"['recursive_chunk_011', 'recursive_chunk_017']",False,True,True,0.451
bm25,recursive,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.556
bm25,recursive,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.292
bm25,recursive,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.447
bm25,recursive,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_024', 'recursive_chunk_025']",A,['recursive_chunk_025'],False,False,False,0.304
bm25,recursive,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_024', 'recursive_chunk_041', 'recursive_chunk_023']",A,['recursive_chunk_023'],False,False,True,0.521
bm25,recursive,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.727
bm25,recursive,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_015', 'recursive_chunk_014']",C,['recursive_chunk_048'],False,True,True,0.293
bm25,recursive,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.468
bm25,recursive,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.402
bm25,recursive,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_076', 'recursive_chunk_029']",C,"['recursive_chunk_024', 'recursive_chunk_076']",False,True,True,0.437
bm25,recursive,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.554
bm25,recursive,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.657
bm25,recursive,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.443
bm25,recursive,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.3
bm25,recursive,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_083', 'recursive_chunk_002', 'recursive_chunk_082']",C,['002'],False,True,False,0.619
bm25,recursive,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_029', 'recursive_chunk_014']",X,[],True,False,,0.531
bm25,recursive,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_029', 'recursive_chunk_097', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.492
bm25,recursive,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_100', 'recursive_chunk_005', 'recursive_chunk_033']",C,['recursive_chunk_100'],False,True,False,0.458
bm25,recursive,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.994
bm25,recursive,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_049', 'recursive_chunk_048', 'recursive_chunk_029']",B,['recursive_chunk_048'],False,True,True,0.319
bm25,recursive,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_038', 'recursive_chunk_041', 'recursive_chunk_009']",D,['recursive_chunk_041'],False,True,True,0.291
bm25,recursive,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_000', 'recursive_chunk_008', 'recursive_chunk_099']",C,"['recursive_chunk_000', 'recursive_chunk_008']",False,True,True,0.515
bm25,recursive,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.578
bm25,recursive,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.603
bm25,recursive,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_073', 'recursive_chunk_074', 'recursive_chunk_014']",B,"['recursive_chunk_073', 'recursive_chunk_074']",False,True,True,0.535
bm25,recursive,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_014']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.621
bm25,recursive,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.292
bm25,recursive,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_083']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.508
bm25,recursive,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_048', 'recursive_chunk_050']",C,['recursive_chunk_008'],False,True,True,0.486
bm25,recursive,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_073', 'recursive_chunk_014', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.616
bm25,recursive,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_019', 'recursive_chunk_006', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.495
bm25,recursive,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.452
bm25,recursive,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.456
bm25,recursive,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_024', 'recursive_chunk_097', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.621
bm25,recursive,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_050', 'recursive_chunk_048', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.439
bm25,recursive,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.404
bm25,recursive,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_032', 'recursive_chunk_050']",C,['recursive_chunk_050'],False,True,False,0.402
bm25,recursive,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.437
bm25,recursive,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_049', 'recursive_chunk_073', 'recursive_chunk_047']",C,['recursive_chunk_073'],False,True,True,0.49
bm25,recursive,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_049', 'recursive_chunk_051']",C,['recursive_chunk_008'],False,True,True,0.453
bm25,recursive,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_016', 'recursive_chunk_018', 'recursive_chunk_015']",C,['recursive_chunk_016'],False,True,True,0.47
bm25,recursive,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",C,"['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",False,True,True,0.644
bm25,recursive,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_073', 'recursive_chunk_013', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_014']",False,True,True,0.509
bm25,recursive,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.524
bm25,recursive,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_050', 'recursive_chunk_024']",C,['recursive_chunk_023'],False,True,True,0.503
bm25,recursive,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_072', 'recursive_chunk_029', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,True,0.501
bm25,recursive,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_041', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.387
bm25,recursive,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_050', 'recursive_chunk_007', 'recursive_chunk_048']",A,['recursive_chunk_007'],False,False,True,0.953
bm25,recursive,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_038', 'recursive_chunk_037', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.519
bm25,recursive,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_024', 'recursive_chunk_073', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.452
bm25,recursive,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.451
bm25,recursive,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.299
bm25,recursive,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_029', 'recursive_chunk_076', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_076']",False,True,True,0.455
bm25,recursive,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_023', 'recursive_chunk_040']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.475
bm25,recursive,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_014', 'recursive_chunk_033']",X,[],True,False,,0.45
bm25,recursive,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.558
bm25,recursive,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_036']",C,['recursive_chunk_040'],False,True,True,0.571
bm25,recursive,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_007', 'recursive_chunk_047']",C,['recursive_chunk_007'],False,True,True,0.437
bm25,recursive,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.47
bm25,recursive,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_029', 'recursive_chunk_048']",C,"['recursive_chunk_012', 'recursive_chunk_029']",False,True,True,0.579
bm25,recursive,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_096', 'recursive_chunk_079']",B,['recursive_chunk_048'],False,False,True,0.568
bm25,recursive,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_029', 'recursive_chunk_032', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_029']",False,True,True,0.528
bm25,recursive,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_001'],False,True,True,0.625
bm25,recursive,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_003', 'recursive_chunk_002']",B,['recursive_chunk_004'],False,True,True,0.563
bm25,recursive,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_088', 'recursive_chunk_029', 'recursive_chunk_126']",B,"['recursive_chunk_088', 'recursive_chunk_029']",False,True,True,0.408
bm25,recursive,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.474
bm25,recursive,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_125', 'recursive_chunk_048']",X,[],True,False,,0.448
bm25,recursive,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.559
bm25,recursive,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_018']",C,['recursive_chunk_008'],False,True,True,0.516
bm25,recursive,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_017', 'recursive_chunk_012', 'recursive_chunk_011']",C,"['recursive_chunk_011', 'recursive_chunk_017']",False,True,True,0.556
bm25,recursive,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.473
bm25,recursive,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.687
bm25,recursive,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.33
bm25,recursive,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_024', 'recursive_chunk_025']",A,['recursive_chunk_025'],False,False,False,0.459
bm25,recursive,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_024', 'recursive_chunk_041', 'recursive_chunk_023']",A,['recursive_chunk_023'],False,False,True,0.593
bm25,recursive,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.456
bm25,recursive,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_015', 'recursive_chunk_014']",C,['recursive_chunk_048'],False,True,True,0.273
bm25,recursive,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.315
bm25,recursive,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.466
bm25,recursive,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_076', 'recursive_chunk_029']",C,"['recursive_chunk_024', 'recursive_chunk_076']",False,True,True,0.493
bm25,recursive,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.305
bm25,recursive,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.628
bm25,recursive,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.504
bm25,recursive,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.393
bm25,recursive,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_083', 'recursive_chunk_002', 'recursive_chunk_082']",C,['002'],False,True,False,0.293
bm25,recursive,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_029', 'recursive_chunk_014']",X,[],True,False,,0.459
bm25,recursive,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_029', 'recursive_chunk_097', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.634
bm25,recursive,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_100', 'recursive_chunk_005', 'recursive_chunk_033']",C,['recursive_chunk_100'],False,True,False,0.471
bm25,recursive,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.489
bm25,recursive,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_049', 'recursive_chunk_048', 'recursive_chunk_029']",B,['recursive_chunk_048'],False,True,True,0.428
bm25,recursive,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_038', 'recursive_chunk_041', 'recursive_chunk_009']",D,['recursive_chunk_041'],False,True,True,0.509
bm25,recursive,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_000', 'recursive_chunk_008', 'recursive_chunk_099']",C,"['recursive_chunk_000', 'recursive_chunk_008']",False,True,True,0.296
bm25,recursive,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.429
bm25,recursive,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.391
bm25,recursive,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_073', 'recursive_chunk_074', 'recursive_chunk_014']",B,"['recursive_chunk_073', 'recursive_chunk_074']",False,True,True,0.587
bm25,recursive,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_014']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.614
bm25,recursive,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.305
bm25,recursive,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_083']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.488
bm25,recursive,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_048', 'recursive_chunk_050']",C,['recursive_chunk_008'],False,True,True,0.489
bm25,recursive,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_073', 'recursive_chunk_014', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.573
bm25,recursive,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_019', 'recursive_chunk_006', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.543
bm25,recursive,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.539
bm25,recursive,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.487
bm25,recursive,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_024', 'recursive_chunk_097', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.657
bm25,recursive,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_050', 'recursive_chunk_048', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.543
bm25,recursive,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.486
bm25,recursive,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_032', 'recursive_chunk_050']",C,['recursive_chunk_050'],False,True,False,0.512
bm25,recursive,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.304
bm25,recursive,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_049', 'recursive_chunk_073', 'recursive_chunk_047']",C,['recursive_chunk_073'],False,True,True,0.447
bm25,recursive,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_049', 'recursive_chunk_051']",C,['recursive_chunk_008'],False,True,True,0.514
bm25,recursive,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_016', 'recursive_chunk_018', 'recursive_chunk_015']",C,['recursive_chunk_016'],False,True,True,0.506
bm25,recursive,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",C,"['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",False,True,True,0.546
bm25,recursive,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_073', 'recursive_chunk_013', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_014']",False,True,True,0.522
bm25,recursive,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.467
bm25,recursive,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_050', 'recursive_chunk_024']",C,['recursive_chunk_023'],False,True,True,0.43
bm25,recursive,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_072', 'recursive_chunk_029', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,True,0.454
bm25,recursive,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_041', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.518
bm25,recursive,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_050', 'recursive_chunk_007', 'recursive_chunk_048']",A,['recursive_chunk_007'],False,False,True,0.448
bm25,recursive,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_038', 'recursive_chunk_037', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.522
bm25,recursive,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_024', 'recursive_chunk_073', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.61
bm25,recursive,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.494
bm25,recursive,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.515
bm25,recursive,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_029', 'recursive_chunk_076', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_076']",False,True,True,0.509
bm25,recursive,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_023', 'recursive_chunk_040']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.469
bm25,recursive,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_014', 'recursive_chunk_033']",X,[],True,False,,0.57
bm25,recursive,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.407
bm25,recursive,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_036']",C,['recursive_chunk_040'],False,True,True,0.517
bm25,recursive,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_007', 'recursive_chunk_047']",C,['recursive_chunk_007'],False,True,True,0.608
bm25,recursive,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.506
bm25,recursive,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_029', 'recursive_chunk_048']",C,"['recursive_chunk_012', 'recursive_chunk_029']",False,True,True,0.27
bm25,recursive,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_096', 'recursive_chunk_079']",B,['recursive_chunk_048'],False,False,True,0.534
bm25,recursive,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_029', 'recursive_chunk_032', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_029']",False,True,True,0.578
bm25,recursive,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_001'],False,True,True,0.406
bm25,recursive,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_003', 'recursive_chunk_002']",B,['recursive_chunk_004'],False,True,True,0.616
bm25,recursive,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_088', 'recursive_chunk_029', 'recursive_chunk_126']",B,"['recursive_chunk_088', 'recursive_chunk_029']",False,True,True,0.515
bm25,recursive,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.286
bm25,recursive,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_125', 'recursive_chunk_048']",X,[],True,False,,0.565
bm25,recursive,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.555
bm25,recursive,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_018']",C,['recursive_chunk_008'],False,True,True,0.551
bm25,recursive,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_017', 'recursive_chunk_012', 'recursive_chunk_011']",C,"['recursive_chunk_011', 'recursive_chunk_017']",False,True,True,0.445
bm25,recursive,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.308
bm25,recursive,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.472
bm25,recursive,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.46
bm25,recursive,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_024', 'recursive_chunk_025']",A,['recursive_chunk_025'],False,False,False,0.556
bm25,recursive,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_024', 'recursive_chunk_041', 'recursive_chunk_023']",A,['recursive_chunk_023'],False,False,True,0.477
bm25,recursive,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.646
bm25,recursive,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_015', 'recursive_chunk_014']",C,['recursive_chunk_048'],False,True,True,0.554
bm25,recursive,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.577
bm25,recursive,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.287
bm25,recursive,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_076', 'recursive_chunk_029']",C,"['recursive_chunk_024', 'recursive_chunk_076']",False,True,True,0.526
bm25,recursive,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.451
bm25,recursive,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.592
bm25,recursive,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.901
bm25,recursive,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.74
bm25,recursive,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_083', 'recursive_chunk_002', 'recursive_chunk_082']",C,['002'],False,True,False,0.438
bm25,recursive,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_029', 'recursive_chunk_014']",X,[],True,False,,0.511
bm25,recursive,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_029', 'recursive_chunk_097', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.572
bm25,recursive,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_100', 'recursive_chunk_005', 'recursive_chunk_033']",C,['recursive_chunk_100'],False,True,False,0.46
bm25,recursive,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.637
bm25,recursive,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_049', 'recursive_chunk_048', 'recursive_chunk_029']",B,['recursive_chunk_048'],False,True,True,0.49
bm25,recursive,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_038', 'recursive_chunk_041', 'recursive_chunk_009']",D,['recursive_chunk_041'],False,True,True,0.538
bm25,recursive,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_000', 'recursive_chunk_008', 'recursive_chunk_099']",C,"['recursive_chunk_000', 'recursive_chunk_008']",False,True,True,0.47
bm25,recursive,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.692
bm25,recursive,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.583
bm25,recursive,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_073', 'recursive_chunk_074', 'recursive_chunk_014']",B,"['recursive_chunk_073', 'recursive_chunk_074']",False,True,True,0.411
bm25,recursive,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_014']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.306
bm25,recursive,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.475
bm25,recursive,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_083']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.626
bm25,recursive,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_048', 'recursive_chunk_050']",C,['recursive_chunk_008'],False,True,True,0.441
bm25,recursive,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_073', 'recursive_chunk_014', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.535
bm25,recursive,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_019', 'recursive_chunk_006', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.467
bm25,recursive,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.499
bm25,recursive,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.472
bm25,recursive,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_024', 'recursive_chunk_097', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.499
bm25,recursive,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_050', 'recursive_chunk_048', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.473
bm25,recursive,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.491
bm25,recursive,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_032', 'recursive_chunk_050']",C,['recursive_chunk_050'],False,True,False,0.308
bm25,recursive,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.481
bm25,recursive,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_049', 'recursive_chunk_073', 'recursive_chunk_047']",C,['recursive_chunk_073'],False,True,True,0.258
bm25,recursive,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_049', 'recursive_chunk_051']",C,['recursive_chunk_008'],False,True,True,0.267
bm25,recursive,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_016', 'recursive_chunk_018', 'recursive_chunk_015']",C,['recursive_chunk_016'],False,True,True,0.474
bm25,recursive,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",C,"['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_076']",False,True,True,0.504
bm25,recursive,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_073', 'recursive_chunk_013', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_014']",False,True,True,0.298
bm25,recursive,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.518
bm25,recursive,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_050', 'recursive_chunk_024']",C,['recursive_chunk_023'],False,True,True,0.479
bm25,recursive,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_072', 'recursive_chunk_029', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,True,0.511
bm25,recursive,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_041', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.499
bm25,recursive,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_050', 'recursive_chunk_007', 'recursive_chunk_048']",A,['recursive_chunk_007'],False,False,True,0.813
bm25,recursive,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_038', 'recursive_chunk_037', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.77
bm25,recursive,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_024', 'recursive_chunk_073', 'recursive_chunk_013']",C,"['recursive_chunk_073', 'recursive_chunk_013']",False,True,True,0.53
bm25,recursive,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.563
bm25,recursive,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.316
bm25,recursive,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_029', 'recursive_chunk_076', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_076']",False,True,True,0.53
bm25,recursive,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_023', 'recursive_chunk_040']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.42
bm25,recursive,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_014', 'recursive_chunk_033']",X,[],True,False,,0.466
bm25,recursive,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.565
bm25,recursive,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_036']",C,['recursive_chunk_040'],False,True,True,0.635
bm25,recursive,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_007', 'recursive_chunk_047']",C,['recursive_chunk_007'],False,True,True,0.3
bm25,recursive,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.523
bm25,recursive,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_029', 'recursive_chunk_048']",C,"['recursive_chunk_012', 'recursive_chunk_029']",False,True,True,0.524
bm25,recursive,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_096', 'recursive_chunk_079']",B,['recursive_chunk_048'],False,False,True,0.484
bm25,recursive,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_029', 'recursive_chunk_032', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_029']",False,True,True,0.481
bm25,recursive,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_001'],False,True,True,0.576
bm25,recursive,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_003', 'recursive_chunk_002']",B,['recursive_chunk_004'],False,True,True,0.448
bm25,recursive,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_088', 'recursive_chunk_029', 'recursive_chunk_126']",B,"['recursive_chunk_088', 'recursive_chunk_029']",False,True,True,0.582
bm25,recursive,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.393
bm25,recursive,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_125', 'recursive_chunk_048']",X,[],True,False,,0.23
bm25,recursive,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.254
bm25,recursive,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_018']",C,['recursive_chunk_008'],False,True,True,0.524
bm25,recursive,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_017', 'recursive_chunk_012', 'recursive_chunk_011']",C,"['recursive_chunk_011', 'recursive_chunk_017']",False,True,True,0.46
bm25,recursive,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.564
bm25,recursive,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.457
bm25,recursive,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.382
bm25,recursive,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_024', 'recursive_chunk_025']",A,['recursive_chunk_025'],False,False,False,0.479
bm25,recursive,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_024', 'recursive_chunk_041', 'recursive_chunk_023']",A,['recursive_chunk_023'],False,False,True,0.29
bm25,recursive,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.359
bm25,recursive,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_015', 'recursive_chunk_014']",C,['recursive_chunk_048'],False,True,True,0.438
bm25,recursive,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.404
bm25,recursive,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.43
bm25,recursive,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_076', 'recursive_chunk_029']",C,"['recursive_chunk_024', 'recursive_chunk_076']",False,True,True,0.433
bm25,recursive,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.25
bm25,recursive,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.531
bm25,recursive,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_051']",C,['recursive_chunk_048'],False,True,True,0.506
bm25,recursive,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.563
bm25,recursive,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_083', 'recursive_chunk_002', 'recursive_chunk_082']",C,['002'],False,True,False,0.293
bm25,recursive,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_029', 'recursive_chunk_014']",X,[],True,False,,0.306
bm25,recursive,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_029', 'recursive_chunk_097', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.315
dense,recursive,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_023']",C,"['recursive_chunk_032', 'recursive_chunk_023']",False,True,False,1.3
dense,recursive,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,1.235
dense,recursive,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_073']",D,['recursive_chunk_023'],False,False,False,0.41
dense,recursive,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_040', 'recursive_chunk_040']",X,[],True,False,,0.586
dense,recursive,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_023']",C,['recursive_chunk_007'],False,True,True,0.541
dense,recursive,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.624
dense,recursive,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_008', 'recursive_chunk_032']",C,['recursive_chunk_008'],False,True,True,0.415
dense,recursive,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.915
dense,recursive,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_031', 'recursive_chunk_040']",C,"['recursive_chunk_038', 'recursive_chunk_031']",False,True,True,0.622
dense,recursive,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_110']",C,['recursive_chunk_032'],False,True,True,0.389
dense,recursive,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_006']",C,"['recursive_chunk_006', 'recursive_chunk_007']",False,True,True,0.352
dense,recursive,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_017']",C,['recursive_chunk_007'],False,True,True,0.632
dense,recursive,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.602
dense,recursive,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_010']",C,"['recursive_chunk_081', 'recursive_chunk_010']",False,True,False,0.596
dense,recursive,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_016', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.626
dense,recursive,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_019']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.539
dense,recursive,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",C,"['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",False,True,True,0.597
dense,recursive,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_074', 'recursive_chunk_014']",B,['recursive_chunk_074'],False,False,False,0.638
dense,recursive,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.39
dense,recursive,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_007', 'recursive_chunk_049', 'recursive_chunk_002']",C,['recursive_chunk_002'],False,True,True,0.542
dense,recursive,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_012', 'recursive_chunk_081']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.686
dense,recursive,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.623
dense,recursive,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_097']",C,"['recursive_chunk_032', 'recursive_chunk_030']",False,True,False,0.577
dense,recursive,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_011', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.963
dense,recursive,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_019', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,1.071
dense,recursive,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.724
dense,recursive,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,3.347
dense,recursive,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_012', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.429
dense,recursive,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_044', 'recursive_chunk_097']",C,['recursive_chunk_032'],False,True,True,0.765
dense,recursive,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,False,1.214
dense,recursive,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,True,0.633
dense,recursive,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.563
dense,recursive,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_023']",False,True,False,0.691
dense,recursive,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.7
dense,recursive,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_029']",C,['recursive_chunk_018'],False,True,True,0.583
dense,recursive,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_030']",C,['recursive_chunk_032'],False,True,False,0.587
dense,recursive,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_030', 'recursive_chunk_024', 'recursive_chunk_014']",C,['recursive_chunk_024'],False,True,True,0.562
dense,recursive,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_016', 'recursive_chunk_017', 'recursive_chunk_011']",X,[],True,False,,0.411
dense,recursive,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_037', 'recursive_chunk_007']",C,['recursive_chunk_037'],False,True,True,0.532
dense,recursive,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.81
dense,recursive,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_031', 'recursive_chunk_032', 'recursive_chunk_033']",C,"['recursive_chunk_031', 'recursive_chunk_032']",False,True,False,0.635
dense,recursive,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,1.005
dense,recursive,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_032', 'recursive_chunk_023', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,False,0.688
dense,recursive,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_014', 'recursive_chunk_023', 'recursive_chunk_040']",D,"['recursive_chunk_014', 'recursive_chunk_023']",False,False,True,1.444
dense,recursive,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_030']",False,True,True,0.632
dense,recursive,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_000', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.613
dense,recursive,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_073', 'recursive_chunk_004', 'recursive_chunk_071']",B,['recursive_chunk_004'],False,True,True,0.602
dense,recursive,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.567
dense,recursive,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_031', 'recursive_chunk_031']",C,['recursive_chunk_031'],False,True,True,0.576
dense,recursive,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,False,0.408
dense,recursive,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",C,"['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",False,True,True,0.685
dense,recursive,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_012', 'recursive_chunk_007', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.373
dense,recursive,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",X,[],True,False,,0.592
dense,recursive,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.461
dense,recursive,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_033']",C,['recursive_chunk_016'],False,True,True,0.58
dense,recursive,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.797
dense,recursive,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_076', 'recursive_chunk_077', 'recursive_chunk_008']",B,"['recursive_chunk_077', 'recursive_chunk_008']",False,False,False,0.673
dense,recursive,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_030', 'recursive_chunk_023', 'recursive_chunk_032']",C,"['recursive_chunk_030', 'recursive_chunk_023']",False,True,True,0.7
dense,recursive,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.751
dense,recursive,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_014', 'recursive_chunk_048', 'recursive_chunk_074']",C,['recursive_chunk_048'],False,True,True,0.53
dense,recursive,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",C,['recursive_chunk_007'],False,True,True,0.699
dense,recursive,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.599
dense,recursive,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_110', 'recursive_chunk_029']",C,['recursive_chunk_024'],False,True,True,0.455
dense,recursive,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",X,[],True,False,,0.516
dense,recursive,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.756
dense,recursive,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_053', 'recursive_chunk_000']",C,['recursive_chunk_048'],False,True,True,0.504
dense,recursive,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.71
dense,recursive,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.792
dense,recursive,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_002', 'recursive_chunk_034', 'recursive_chunk_071']",X,[],True,False,,0.515
dense,recursive,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_013']",C,['recursive_chunk_023'],False,True,True,0.704
dense,recursive,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_023']",C,"['recursive_chunk_032', 'recursive_chunk_023']",False,True,False,2.898
dense,recursive,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.658
dense,recursive,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_073']",D,['recursive_chunk_023'],False,False,False,0.697
dense,recursive,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_040', 'recursive_chunk_040']",X,[],True,False,,0.677
dense,recursive,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_023']",C,['recursive_chunk_007'],False,True,True,0.574
dense,recursive,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.514
dense,recursive,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_008', 'recursive_chunk_032']",C,['recursive_chunk_008'],False,True,True,0.402
dense,recursive,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.556
dense,recursive,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_031', 'recursive_chunk_040']",C,"['recursive_chunk_038', 'recursive_chunk_031']",False,True,True,0.595
dense,recursive,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_110']",C,['recursive_chunk_032'],False,True,True,0.574
dense,recursive,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_006']",C,"['recursive_chunk_006', 'recursive_chunk_007']",False,True,True,0.577
dense,recursive,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_017']",C,['recursive_chunk_007'],False,True,True,0.301
dense,recursive,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.998
dense,recursive,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_010']",C,"['recursive_chunk_081', 'recursive_chunk_010']",False,True,False,0.653
dense,recursive,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_016', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.623
dense,recursive,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_019']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.517
dense,recursive,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",C,"['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",False,True,True,0.552
dense,recursive,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_074', 'recursive_chunk_014']",B,['recursive_chunk_074'],False,False,False,0.624
dense,recursive,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.591
dense,recursive,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_007', 'recursive_chunk_049', 'recursive_chunk_002']",C,['recursive_chunk_002'],False,True,True,0.535
dense,recursive,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_012', 'recursive_chunk_081']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.421
dense,recursive,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.662
dense,recursive,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_097']",C,"['recursive_chunk_032', 'recursive_chunk_030']",False,True,False,0.63
dense,recursive,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_011', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.643
dense,recursive,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_019', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.717
dense,recursive,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.639
dense,recursive,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.602
dense,recursive,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_012', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.673
dense,recursive,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_044', 'recursive_chunk_097']",C,['recursive_chunk_032'],False,True,True,0.62
dense,recursive,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,False,0.409
dense,recursive,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,True,0.695
dense,recursive,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.5
dense,recursive,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_023']",False,True,False,0.422
dense,recursive,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.567
dense,recursive,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_029']",C,['recursive_chunk_018'],False,True,True,0.526
dense,recursive,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_030']",C,['recursive_chunk_032'],False,True,False,0.562
dense,recursive,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_030', 'recursive_chunk_024', 'recursive_chunk_014']",C,['recursive_chunk_024'],False,True,True,0.727
dense,recursive,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_016', 'recursive_chunk_017', 'recursive_chunk_011']",X,[],True,False,,0.595
dense,recursive,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_037', 'recursive_chunk_007']",C,['recursive_chunk_037'],False,True,True,0.637
dense,recursive,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.613
dense,recursive,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_031', 'recursive_chunk_032', 'recursive_chunk_033']",C,"['recursive_chunk_031', 'recursive_chunk_032']",False,True,False,0.639
dense,recursive,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.649
dense,recursive,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_032', 'recursive_chunk_023', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,False,0.483
dense,recursive,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_014', 'recursive_chunk_023', 'recursive_chunk_040']",D,"['recursive_chunk_014', 'recursive_chunk_023']",False,False,True,0.639
dense,recursive,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_030']",False,True,True,0.49
dense,recursive,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_000', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.835
dense,recursive,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_073', 'recursive_chunk_004', 'recursive_chunk_071']",B,['recursive_chunk_004'],False,True,True,0.529
dense,recursive,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.534
dense,recursive,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_031', 'recursive_chunk_031']",C,['recursive_chunk_031'],False,True,True,0.642
dense,recursive,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,False,0.516
dense,recursive,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",C,"['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",False,True,True,0.61
dense,recursive,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_012', 'recursive_chunk_007', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.599
dense,recursive,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",X,[],True,False,,0.599
dense,recursive,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.615
dense,recursive,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_033']",C,['recursive_chunk_016'],False,True,True,0.555
dense,recursive,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.548
dense,recursive,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_076', 'recursive_chunk_077', 'recursive_chunk_008']",B,"['recursive_chunk_077', 'recursive_chunk_008']",False,False,False,0.739
dense,recursive,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_030', 'recursive_chunk_023', 'recursive_chunk_032']",C,"['recursive_chunk_030', 'recursive_chunk_023']",False,True,True,0.44
dense,recursive,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.571
dense,recursive,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_014', 'recursive_chunk_048', 'recursive_chunk_074']",C,['recursive_chunk_048'],False,True,True,0.634
dense,recursive,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",C,['recursive_chunk_007'],False,True,True,0.879
dense,recursive,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.576
dense,recursive,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_110', 'recursive_chunk_029']",C,['recursive_chunk_024'],False,True,True,0.565
dense,recursive,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",X,[],True,False,,0.375
dense,recursive,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.546
dense,recursive,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_053', 'recursive_chunk_000']",C,['recursive_chunk_048'],False,True,True,0.505
dense,recursive,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.542
dense,recursive,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.594
dense,recursive,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_002', 'recursive_chunk_034', 'recursive_chunk_071']",X,[],True,False,,0.496
dense,recursive,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_013']",C,['recursive_chunk_023'],False,True,True,0.541
dense,recursive,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_023']",C,"['recursive_chunk_032', 'recursive_chunk_023']",False,True,False,0.7
dense,recursive,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.42
dense,recursive,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_073']",D,['recursive_chunk_023'],False,False,False,0.824
dense,recursive,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_040', 'recursive_chunk_040']",X,[],True,False,,0.768
dense,recursive,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_023']",C,['recursive_chunk_007'],False,True,True,0.639
dense,recursive,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.331
dense,recursive,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_008', 'recursive_chunk_032']",C,['recursive_chunk_008'],False,True,True,0.484
dense,recursive,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.744
dense,recursive,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_031', 'recursive_chunk_040']",C,"['recursive_chunk_038', 'recursive_chunk_031']",False,True,True,0.631
dense,recursive,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_110']",C,['recursive_chunk_032'],False,True,True,0.576
dense,recursive,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_006']",C,"['recursive_chunk_006', 'recursive_chunk_007']",False,True,True,0.574
dense,recursive,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_017']",C,['recursive_chunk_007'],False,True,True,0.527
dense,recursive,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,1.059
dense,recursive,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_010']",C,"['recursive_chunk_081', 'recursive_chunk_010']",False,True,False,0.594
dense,recursive,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_016', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.483
dense,recursive,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_019']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.533
dense,recursive,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",C,"['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",False,True,True,0.592
dense,recursive,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_074', 'recursive_chunk_014']",B,['recursive_chunk_074'],False,False,False,0.626
dense,recursive,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.52
dense,recursive,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_007', 'recursive_chunk_049', 'recursive_chunk_002']",C,['recursive_chunk_002'],False,True,True,0.388
dense,recursive,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_012', 'recursive_chunk_081']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.641
dense,recursive,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.393
dense,recursive,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_097']",C,"['recursive_chunk_032', 'recursive_chunk_030']",False,True,False,0.382
dense,recursive,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_011', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.594
dense,recursive,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_019', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.522
dense,recursive,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.587
dense,recursive,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.712
dense,recursive,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_012', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.852
dense,recursive,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_044', 'recursive_chunk_097']",C,['recursive_chunk_032'],False,True,True,0.348
dense,recursive,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,False,0.655
dense,recursive,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,True,0.732
dense,recursive,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.581
dense,recursive,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_023']",False,True,False,0.577
dense,recursive,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.568
dense,recursive,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_029']",C,['recursive_chunk_018'],False,True,True,0.575
dense,recursive,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_030']",C,['recursive_chunk_032'],False,True,False,0.606
dense,recursive,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_030', 'recursive_chunk_024', 'recursive_chunk_014']",C,['recursive_chunk_024'],False,True,True,0.419
dense,recursive,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_016', 'recursive_chunk_017', 'recursive_chunk_011']",X,[],True,False,,0.691
dense,recursive,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_037', 'recursive_chunk_007']",C,['recursive_chunk_037'],False,True,True,0.729
dense,recursive,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.347
dense,recursive,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_031', 'recursive_chunk_032', 'recursive_chunk_033']",C,"['recursive_chunk_031', 'recursive_chunk_032']",False,True,False,0.625
dense,recursive,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.382
dense,recursive,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_032', 'recursive_chunk_023', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,False,0.366
dense,recursive,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_014', 'recursive_chunk_023', 'recursive_chunk_040']",D,"['recursive_chunk_014', 'recursive_chunk_023']",False,False,True,0.576
dense,recursive,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_030']",False,True,True,0.402
dense,recursive,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_000', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.558
dense,recursive,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_073', 'recursive_chunk_004', 'recursive_chunk_071']",B,['recursive_chunk_004'],False,True,True,0.53
dense,recursive,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.566
dense,recursive,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_031', 'recursive_chunk_031']",C,['recursive_chunk_031'],False,True,True,0.569
dense,recursive,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,False,0.382
dense,recursive,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",C,"['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",False,True,True,0.617
dense,recursive,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_012', 'recursive_chunk_007', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.405
dense,recursive,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",X,[],True,False,,0.512
dense,recursive,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.621
dense,recursive,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_033']",C,['recursive_chunk_016'],False,True,True,0.47
dense,recursive,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.353
dense,recursive,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_076', 'recursive_chunk_077', 'recursive_chunk_008']",B,"['recursive_chunk_077', 'recursive_chunk_008']",False,False,False,0.536
dense,recursive,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_030', 'recursive_chunk_023', 'recursive_chunk_032']",C,"['recursive_chunk_030', 'recursive_chunk_023']",False,True,True,0.66
dense,recursive,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.555
dense,recursive,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_014', 'recursive_chunk_048', 'recursive_chunk_074']",C,['recursive_chunk_048'],False,True,True,0.593
dense,recursive,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",C,['recursive_chunk_007'],False,True,True,0.541
dense,recursive,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.58
dense,recursive,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_110', 'recursive_chunk_029']",C,['recursive_chunk_024'],False,True,True,0.414
dense,recursive,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",X,[],True,False,,0.54
dense,recursive,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.329
dense,recursive,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_053', 'recursive_chunk_000']",C,['recursive_chunk_048'],False,True,True,0.479
dense,recursive,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.613
dense,recursive,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.535
dense,recursive,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_002', 'recursive_chunk_034', 'recursive_chunk_071']",X,[],True,False,,0.498
dense,recursive,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_013']",C,['recursive_chunk_023'],False,True,True,0.534
dense,recursive,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_023']",C,"['recursive_chunk_032', 'recursive_chunk_023']",False,True,False,0.372
dense,recursive,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.373
dense,recursive,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_073']",D,['recursive_chunk_023'],False,False,False,0.62
dense,recursive,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_040', 'recursive_chunk_040']",X,[],True,False,,0.377
dense,recursive,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_023']",C,['recursive_chunk_007'],False,True,True,0.542
dense,recursive,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.673
dense,recursive,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_008', 'recursive_chunk_032']",C,['recursive_chunk_008'],False,True,True,0.542
dense,recursive,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.386
dense,recursive,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_031', 'recursive_chunk_040']",C,"['recursive_chunk_038', 'recursive_chunk_031']",False,True,True,0.532
dense,recursive,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_110']",C,['recursive_chunk_032'],False,True,True,0.482
dense,recursive,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_006']",C,"['recursive_chunk_006', 'recursive_chunk_007']",False,True,True,0.605
dense,recursive,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_017']",C,['recursive_chunk_007'],False,True,True,0.364
dense,recursive,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.524
dense,recursive,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_010']",C,"['recursive_chunk_081', 'recursive_chunk_010']",False,True,False,0.627
dense,recursive,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_016', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.625
dense,recursive,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_019']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.587
dense,recursive,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",C,"['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",False,True,True,0.586
dense,recursive,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_074', 'recursive_chunk_014']",B,['recursive_chunk_074'],False,False,False,0.483
dense,recursive,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.635
dense,recursive,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_007', 'recursive_chunk_049', 'recursive_chunk_002']",C,['recursive_chunk_002'],False,True,True,0.492
dense,recursive,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_012', 'recursive_chunk_081']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.648
dense,recursive,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.52
dense,recursive,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_097']",C,"['recursive_chunk_032', 'recursive_chunk_030']",False,True,False,0.587
dense,recursive,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_011', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.527
dense,recursive,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_019', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.592
dense,recursive,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.592
dense,recursive,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.64
dense,recursive,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_012', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.544
dense,recursive,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_044', 'recursive_chunk_097']",C,['recursive_chunk_032'],False,True,True,0.924
dense,recursive,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,False,0.632
dense,recursive,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,True,0.514
dense,recursive,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.557
dense,recursive,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_023']",False,True,False,0.848
dense,recursive,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.517
dense,recursive,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_029']",C,['recursive_chunk_018'],False,True,True,0.533
dense,recursive,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_030']",C,['recursive_chunk_032'],False,True,False,0.45
dense,recursive,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_030', 'recursive_chunk_024', 'recursive_chunk_014']",C,['recursive_chunk_024'],False,True,True,0.855
dense,recursive,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_016', 'recursive_chunk_017', 'recursive_chunk_011']",X,[],True,False,,0.649
dense,recursive,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_037', 'recursive_chunk_007']",C,['recursive_chunk_037'],False,True,True,0.541
dense,recursive,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.618
dense,recursive,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_031', 'recursive_chunk_032', 'recursive_chunk_033']",C,"['recursive_chunk_031', 'recursive_chunk_032']",False,True,False,0.522
dense,recursive,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.334
dense,recursive,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_032', 'recursive_chunk_023', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,False,0.925
dense,recursive,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_014', 'recursive_chunk_023', 'recursive_chunk_040']",D,"['recursive_chunk_014', 'recursive_chunk_023']",False,False,True,0.658
dense,recursive,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_030']",False,True,True,0.68
dense,recursive,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_000', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.365
dense,recursive,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_073', 'recursive_chunk_004', 'recursive_chunk_071']",B,['recursive_chunk_004'],False,True,True,0.531
dense,recursive,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.493
dense,recursive,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_031', 'recursive_chunk_031']",C,['recursive_chunk_031'],False,True,True,0.59
dense,recursive,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,False,0.705
dense,recursive,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",C,"['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",False,True,True,0.593
dense,recursive,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_012', 'recursive_chunk_007', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.808
dense,recursive,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",X,[],True,False,,0.507
dense,recursive,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.539
dense,recursive,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_033']",C,['recursive_chunk_016'],False,True,True,0.686
dense,recursive,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.607
dense,recursive,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_076', 'recursive_chunk_077', 'recursive_chunk_008']",B,"['recursive_chunk_077', 'recursive_chunk_008']",False,False,False,0.531
dense,recursive,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_030', 'recursive_chunk_023', 'recursive_chunk_032']",C,"['recursive_chunk_030', 'recursive_chunk_023']",False,True,True,0.637
dense,recursive,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.587
dense,recursive,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_014', 'recursive_chunk_048', 'recursive_chunk_074']",C,['recursive_chunk_048'],False,True,True,0.616
dense,recursive,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",C,['recursive_chunk_007'],False,True,True,0.519
dense,recursive,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.345
dense,recursive,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_110', 'recursive_chunk_029']",C,['recursive_chunk_024'],False,True,True,0.385
dense,recursive,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",X,[],True,False,,0.507
dense,recursive,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.538
dense,recursive,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_053', 'recursive_chunk_000']",C,['recursive_chunk_048'],False,True,True,0.638
dense,recursive,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.972
dense,recursive,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.524
dense,recursive,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_002', 'recursive_chunk_034', 'recursive_chunk_071']",X,[],True,False,,0.493
dense,recursive,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_013']",C,['recursive_chunk_023'],False,True,True,0.689
dense,recursive,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_023']",C,"['recursive_chunk_032', 'recursive_chunk_023']",False,True,False,0.559
dense,recursive,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.508
dense,recursive,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_073']",D,['recursive_chunk_023'],False,False,False,0.596
dense,recursive,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_040', 'recursive_chunk_040']",X,[],True,False,,0.698
dense,recursive,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_023']",C,['recursive_chunk_007'],False,True,True,0.482
dense,recursive,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,True,0.58
dense,recursive,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_008', 'recursive_chunk_032']",C,['recursive_chunk_008'],False,True,True,0.651
dense,recursive,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.671
dense,recursive,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_031', 'recursive_chunk_040']",C,"['recursive_chunk_038', 'recursive_chunk_031']",False,True,True,0.577
dense,recursive,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_110']",C,['recursive_chunk_032'],False,True,True,0.557
dense,recursive,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_006']",C,"['recursive_chunk_006', 'recursive_chunk_007']",False,True,True,0.522
dense,recursive,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_017']",C,['recursive_chunk_007'],False,True,True,0.696
dense,recursive,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.553
dense,recursive,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_081', 'recursive_chunk_010']",C,"['recursive_chunk_081', 'recursive_chunk_010']",False,True,False,0.582
dense,recursive,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_016', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.565
dense,recursive,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_019']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.517
dense,recursive,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",C,"['recursive_chunk_030', 'recursive_chunk_097', 'recursive_chunk_029']",False,True,True,0.384
dense,recursive,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_074', 'recursive_chunk_014']",B,['recursive_chunk_074'],False,False,False,0.316
dense,recursive,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.482
dense,recursive,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_007', 'recursive_chunk_049', 'recursive_chunk_002']",C,['recursive_chunk_002'],False,True,True,0.539
dense,recursive,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_012', 'recursive_chunk_081']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.47
dense,recursive,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.706
dense,recursive,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_097']",C,"['recursive_chunk_032', 'recursive_chunk_030']",False,True,False,0.4
dense,recursive,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_011', 'recursive_chunk_016']",C,['recursive_chunk_016'],False,True,True,0.67
dense,recursive,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_019', 'recursive_chunk_007']",C,['recursive_chunk_019'],False,True,True,0.595
dense,recursive,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.714
dense,recursive,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.519
dense,recursive,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_012', 'recursive_chunk_023', 'recursive_chunk_007']",C,['recursive_chunk_023'],False,True,True,0.629
dense,recursive,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_044', 'recursive_chunk_097']",C,['recursive_chunk_032'],False,True,True,0.558
dense,recursive,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,False,0.6
dense,recursive,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_007', 'recursive_chunk_049']",C,"['recursive_chunk_007', 'recursive_chunk_049']",False,True,True,0.553
dense,recursive,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.578
dense,recursive,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_023']",False,True,False,0.585
dense,recursive,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.501
dense,recursive,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_018', 'recursive_chunk_029']",C,['recursive_chunk_018'],False,True,True,0.666
dense,recursive,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_032', 'recursive_chunk_030']",C,['recursive_chunk_032'],False,True,False,0.427
dense,recursive,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_030', 'recursive_chunk_024', 'recursive_chunk_014']",C,['recursive_chunk_024'],False,True,True,0.522
dense,recursive,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_016', 'recursive_chunk_017', 'recursive_chunk_011']",X,[],True,False,,0.634
dense,recursive,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_037', 'recursive_chunk_007']",C,['recursive_chunk_037'],False,True,True,0.553
dense,recursive,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.381
dense,recursive,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_031', 'recursive_chunk_032', 'recursive_chunk_033']",C,"['recursive_chunk_031', 'recursive_chunk_032']",False,True,False,0.815
dense,recursive,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.39
dense,recursive,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_032', 'recursive_chunk_023', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,False,0.348
dense,recursive,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_014', 'recursive_chunk_023', 'recursive_chunk_040']",D,"['recursive_chunk_014', 'recursive_chunk_023']",False,False,True,0.51
dense,recursive,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_030', 'recursive_chunk_031']",C,"['recursive_chunk_031', 'recursive_chunk_030']",False,True,True,0.638
dense,recursive,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_000', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.536
dense,recursive,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_073', 'recursive_chunk_004', 'recursive_chunk_071']",B,['recursive_chunk_004'],False,True,True,0.701
dense,recursive,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",B,['recursive_chunk_029'],False,True,False,0.547
dense,recursive,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_031', 'recursive_chunk_031']",C,['recursive_chunk_031'],False,True,True,0.547
dense,recursive,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_032', 'recursive_chunk_033', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,False,0.687
dense,recursive,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",C,"['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_093']",False,True,True,0.56
dense,recursive,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_012', 'recursive_chunk_007', 'recursive_chunk_008']",C,['recursive_chunk_008'],False,True,True,0.49
dense,recursive,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",X,[],True,False,,0.677
dense,recursive,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.64
dense,recursive,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_033']",C,['recursive_chunk_016'],False,True,True,0.329
dense,recursive,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.525
dense,recursive,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_076', 'recursive_chunk_077', 'recursive_chunk_008']",B,"['recursive_chunk_077', 'recursive_chunk_008']",False,False,False,0.635
dense,recursive,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_030', 'recursive_chunk_023', 'recursive_chunk_032']",C,"['recursive_chunk_030', 'recursive_chunk_023']",False,True,True,0.636
dense,recursive,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_039', 'recursive_chunk_040', 'recursive_chunk_042']",C,['recursive_chunk_040'],False,True,True,0.396
dense,recursive,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_014', 'recursive_chunk_048', 'recursive_chunk_074']",C,['recursive_chunk_048'],False,True,True,0.591
dense,recursive,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_007', 'recursive_chunk_007']",C,['recursive_chunk_007'],False,True,True,0.546
dense,recursive,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_013', 'recursive_chunk_013']",C,['recursive_chunk_013'],False,True,True,0.536
dense,recursive,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_110', 'recursive_chunk_029']",C,['recursive_chunk_024'],False,True,True,1.008
dense,recursive,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",X,[],True,False,,0.526
dense,recursive,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_029', 'recursive_chunk_029']",C,['recursive_chunk_029'],False,True,False,0.369
dense,recursive,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_053', 'recursive_chunk_000']",C,['recursive_chunk_048'],False,True,True,0.583
dense,recursive,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_040'],False,True,True,0.539
dense,recursive,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.59
dense,recursive,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_002', 'recursive_chunk_034', 'recursive_chunk_071']",X,[],True,False,,0.527
dense,recursive,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_023', 'recursive_chunk_013']",C,['recursive_chunk_023'],False,True,True,0.859
hybrid,recursive,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_100', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,False,0.369
hybrid,recursive,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.588
hybrid,recursive,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_049', 'recursive_chunk_048']",B,"['recursive_chunk_048', 'recursive_chunk_023']",False,True,True,0.516
hybrid,recursive,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_038', 'recursive_chunk_041']",D,['recursive_chunk_041'],False,True,True,0.896
hybrid,recursive,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_000', 'recursive_chunk_008']",C,"['recursive_chunk_007', 'recursive_chunk_000']",False,True,True,0.374
hybrid,recursive,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.68
hybrid,recursive,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.651
hybrid,recursive,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_074']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.523
hybrid,recursive,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_031']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.708
hybrid,recursive,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.625
hybrid,recursive,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_006', 'recursive_chunk_001']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.401
hybrid,recursive,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_007', 'recursive_chunk_048']",C,['recursive_chunk_008'],False,True,True,0.552
hybrid,recursive,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.597
hybrid,recursive,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_019', 'recursive_chunk_006']",C,['recursive_chunk_019'],False,True,True,0.361
hybrid,recursive,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.833
hybrid,recursive,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.573
hybrid,recursive,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_097', 'recursive_chunk_029', 'recursive_chunk_024']",C,"['recursive_chunk_029', 'recursive_chunk_097']",False,True,True,0.595
hybrid,recursive,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_050', 'recursive_chunk_048']",C,['recursive_chunk_048'],False,True,True,0.513
hybrid,recursive,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.628
hybrid,recursive,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.557
hybrid,recursive,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.578
hybrid,recursive,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_049']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.567
hybrid,recursive,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_032', 'recursive_chunk_049']",C,['recursive_chunk_008'],False,True,True,0.525
hybrid,recursive,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.546
hybrid,recursive,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_075']",False,True,True,0.594
hybrid,recursive,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",False,True,True,2.19
hybrid,recursive,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.539
hybrid,recursive,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_023'],False,True,True,0.355
hybrid,recursive,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_072', 'recursive_chunk_029']",C,['recursive_chunk_032'],False,True,True,0.528
hybrid,recursive,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_041', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.499
hybrid,recursive,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_050', 'recursive_chunk_033']",C,"['recursive_chunk_007', 'recursive_chunk_050']",False,True,True,0.364
hybrid,recursive,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_038', 'recursive_chunk_037']",C,['recursive_chunk_013'],False,True,True,0.515
hybrid,recursive,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_024', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.763
hybrid,recursive,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.539
hybrid,recursive,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.586
hybrid,recursive,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_076']",C,"['recursive_chunk_076', 'recursive_chunk_032']",False,True,True,0.633
hybrid,recursive,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_030', 'recursive_chunk_023']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.648
hybrid,recursive,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_014']",X,[],True,False,,0.586
hybrid,recursive,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.383
hybrid,recursive,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.725
hybrid,recursive,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_031', 'recursive_chunk_007']",C,"['recursive_chunk_007', 'recursive_chunk_001']",False,True,True,0.555
hybrid,recursive,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.62
hybrid,recursive,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",C,"['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",False,True,True,0.546
hybrid,recursive,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_096']",B,['recursive_chunk_048'],False,False,True,0.489
hybrid,recursive,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_031', 'recursive_chunk_029']",C,"['recursive_chunk_029', 'recursive_chunk_031']",False,True,True,0.637
hybrid,recursive,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_000', 'recursive_chunk_012']",C,['recursive_chunk_001'],False,True,True,0.537
hybrid,recursive,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_073', 'recursive_chunk_003']",B,['recursive_chunk_004'],False,True,True,0.574
hybrid,recursive,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_088', 'recursive_chunk_126']",B,"['recursive_chunk_029', 'recursive_chunk_088']",False,True,True,0.394
hybrid,recursive,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.57
hybrid,recursive,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_032', 'recursive_chunk_125']",C,['recursive_chunk_032'],False,True,False,0.516
hybrid,recursive,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.638
hybrid,recursive,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_008']",False,True,True,0.382
hybrid,recursive,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",X,[],True,False,,0.394
hybrid,recursive,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.554
hybrid,recursive,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_011']",C,['recursive_chunk_016'],False,True,True,0.389
hybrid,recursive,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.541
hybrid,recursive,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_076', 'recursive_chunk_024']",B,['recursive_chunk_024'],False,False,False,0.543
hybrid,recursive,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_023', 'recursive_chunk_024', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,True,0.616
hybrid,recursive,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.578
hybrid,recursive,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_015']",C,['recursive_chunk_048'],False,True,True,0.518
hybrid,recursive,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.484
hybrid,recursive,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.687
hybrid,recursive,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_029', 'recursive_chunk_076']",C,['recursive_chunk_076'],False,True,False,0.588
hybrid,recursive,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.483
hybrid,recursive,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.521
hybrid,recursive,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_053']",C,['recursive_chunk_048'],False,True,True,0.37
hybrid,recursive,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_031', 'recursive_chunk_039']",C,['recursive_chunk_040'],False,True,True,0.482
hybrid,recursive,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_083', 'recursive_chunk_002']",C,['recursive_chunk_023'],False,True,True,0.391
hybrid,recursive,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_002', 'recursive_chunk_029']",X,[],True,False,,0.511
hybrid,recursive,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_029', 'recursive_chunk_097']",C,['recursive_chunk_023'],False,True,True,0.529
hybrid,recursive,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_100', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,False,0.354
hybrid,recursive,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.636
hybrid,recursive,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_049', 'recursive_chunk_048']",B,"['recursive_chunk_048', 'recursive_chunk_023']",False,True,True,0.713
hybrid,recursive,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_038', 'recursive_chunk_041']",D,['recursive_chunk_041'],False,True,True,0.519
hybrid,recursive,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_000', 'recursive_chunk_008']",C,"['recursive_chunk_007', 'recursive_chunk_000']",False,True,True,0.545
hybrid,recursive,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.572
hybrid,recursive,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.584
hybrid,recursive,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_074']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.662
hybrid,recursive,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_031']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.515
hybrid,recursive,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.53
hybrid,recursive,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_006', 'recursive_chunk_001']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.643
hybrid,recursive,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_007', 'recursive_chunk_048']",C,['recursive_chunk_008'],False,True,True,0.626
hybrid,recursive,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.642
hybrid,recursive,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_019', 'recursive_chunk_006']",C,['recursive_chunk_019'],False,True,True,0.577
hybrid,recursive,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.514
hybrid,recursive,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.533
hybrid,recursive,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_097', 'recursive_chunk_029', 'recursive_chunk_024']",C,"['recursive_chunk_029', 'recursive_chunk_097']",False,True,True,0.708
hybrid,recursive,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_050', 'recursive_chunk_048']",C,['recursive_chunk_048'],False,True,True,0.387
hybrid,recursive,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.382
hybrid,recursive,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.714
hybrid,recursive,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.59
hybrid,recursive,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_049']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.54
hybrid,recursive,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_032', 'recursive_chunk_049']",C,['recursive_chunk_008'],False,True,True,0.567
hybrid,recursive,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.392
hybrid,recursive,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_075']",False,True,True,0.586
hybrid,recursive,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",False,True,True,0.691
hybrid,recursive,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.468
hybrid,recursive,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_023'],False,True,True,0.525
hybrid,recursive,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_072', 'recursive_chunk_029']",C,['recursive_chunk_032'],False,True,True,0.583
hybrid,recursive,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_041', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.478
hybrid,recursive,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_050', 'recursive_chunk_033']",C,"['recursive_chunk_007', 'recursive_chunk_050']",False,True,True,0.706
hybrid,recursive,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_038', 'recursive_chunk_037']",C,['recursive_chunk_013'],False,True,True,0.533
hybrid,recursive,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_024', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.563
hybrid,recursive,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.739
hybrid,recursive,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,1.788
hybrid,recursive,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_076']",C,"['recursive_chunk_076', 'recursive_chunk_032']",False,True,True,0.76
hybrid,recursive,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_030', 'recursive_chunk_023']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.397
hybrid,recursive,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_014']",X,[],True,False,,1.048
hybrid,recursive,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.651
hybrid,recursive,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.672
hybrid,recursive,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_031', 'recursive_chunk_007']",C,"['recursive_chunk_007', 'recursive_chunk_001']",False,True,True,0.324
hybrid,recursive,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.458
hybrid,recursive,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",C,"['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",False,True,True,0.844
hybrid,recursive,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_096']",B,['recursive_chunk_048'],False,False,True,0.651
hybrid,recursive,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_031', 'recursive_chunk_029']",C,"['recursive_chunk_029', 'recursive_chunk_031']",False,True,True,0.865
hybrid,recursive,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_000', 'recursive_chunk_012']",C,['recursive_chunk_001'],False,True,True,0.462
hybrid,recursive,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_073', 'recursive_chunk_003']",B,['recursive_chunk_004'],False,True,True,0.695
hybrid,recursive,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_088', 'recursive_chunk_126']",B,"['recursive_chunk_029', 'recursive_chunk_088']",False,True,True,0.569
hybrid,recursive,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.651
hybrid,recursive,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_032', 'recursive_chunk_125']",C,['recursive_chunk_032'],False,True,False,0.591
hybrid,recursive,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.56
hybrid,recursive,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_008']",False,True,True,0.501
hybrid,recursive,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",X,[],True,False,,0.575
hybrid,recursive,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.517
hybrid,recursive,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_011']",C,['recursive_chunk_016'],False,True,True,0.558
hybrid,recursive,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.572
hybrid,recursive,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_076', 'recursive_chunk_024']",B,['recursive_chunk_024'],False,False,False,0.484
hybrid,recursive,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_023', 'recursive_chunk_024', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,True,0.542
hybrid,recursive,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.641
hybrid,recursive,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_015']",C,['recursive_chunk_048'],False,True,True,0.581
hybrid,recursive,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.529
hybrid,recursive,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.529
hybrid,recursive,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_029', 'recursive_chunk_076']",C,['recursive_chunk_076'],False,True,False,0.748
hybrid,recursive,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.531
hybrid,recursive,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.613
hybrid,recursive,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_053']",C,['recursive_chunk_048'],False,True,True,0.515
hybrid,recursive,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_031', 'recursive_chunk_039']",C,['recursive_chunk_040'],False,True,True,0.393
hybrid,recursive,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_083', 'recursive_chunk_002']",C,['recursive_chunk_023'],False,True,True,1.003
hybrid,recursive,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_002', 'recursive_chunk_029']",X,[],True,False,,0.726
hybrid,recursive,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_029', 'recursive_chunk_097']",C,['recursive_chunk_023'],False,True,True,0.552
hybrid,recursive,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_100', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,False,0.57
hybrid,recursive,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.682
hybrid,recursive,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_049', 'recursive_chunk_048']",B,"['recursive_chunk_048', 'recursive_chunk_023']",False,True,True,0.371
hybrid,recursive,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_038', 'recursive_chunk_041']",D,['recursive_chunk_041'],False,True,True,0.651
hybrid,recursive,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_000', 'recursive_chunk_008']",C,"['recursive_chunk_007', 'recursive_chunk_000']",False,True,True,0.579
hybrid,recursive,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.539
hybrid,recursive,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.385
hybrid,recursive,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_074']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.524
hybrid,recursive,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_031']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.419
hybrid,recursive,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.548
hybrid,recursive,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_006', 'recursive_chunk_001']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.339
hybrid,recursive,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_007', 'recursive_chunk_048']",C,['recursive_chunk_008'],False,True,True,0.453
hybrid,recursive,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.388
hybrid,recursive,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_019', 'recursive_chunk_006']",C,['recursive_chunk_019'],False,True,True,0.39
hybrid,recursive,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,0.732
hybrid,recursive,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.519
hybrid,recursive,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_097', 'recursive_chunk_029', 'recursive_chunk_024']",C,"['recursive_chunk_029', 'recursive_chunk_097']",False,True,True,0.388
hybrid,recursive,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_050', 'recursive_chunk_048']",C,['recursive_chunk_048'],False,True,True,0.325
hybrid,recursive,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.518
hybrid,recursive,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.579
hybrid,recursive,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.641
hybrid,recursive,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_049']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.863
hybrid,recursive,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_032', 'recursive_chunk_049']",C,['recursive_chunk_008'],False,True,True,0.579
hybrid,recursive,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.518
hybrid,recursive,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_075']",False,True,True,0.581
hybrid,recursive,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",False,True,True,0.434
hybrid,recursive,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.632
hybrid,recursive,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_023'],False,True,True,0.564
hybrid,recursive,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_072', 'recursive_chunk_029']",C,['recursive_chunk_032'],False,True,True,0.515
hybrid,recursive,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_041', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.502
hybrid,recursive,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_050', 'recursive_chunk_033']",C,"['recursive_chunk_007', 'recursive_chunk_050']",False,True,True,0.648
hybrid,recursive,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_038', 'recursive_chunk_037']",C,['recursive_chunk_013'],False,True,True,0.731
hybrid,recursive,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_024', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.489
hybrid,recursive,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.331
hybrid,recursive,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.623
hybrid,recursive,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_076']",C,"['recursive_chunk_076', 'recursive_chunk_032']",False,True,True,0.818
hybrid,recursive,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_030', 'recursive_chunk_023']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.572
hybrid,recursive,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_014']",X,[],True,False,,0.63
hybrid,recursive,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.527
hybrid,recursive,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.534
hybrid,recursive,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_031', 'recursive_chunk_007']",C,"['recursive_chunk_007', 'recursive_chunk_001']",False,True,True,0.502
hybrid,recursive,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.743
hybrid,recursive,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",C,"['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",False,True,True,0.634
hybrid,recursive,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_096']",B,['recursive_chunk_048'],False,False,True,0.574
hybrid,recursive,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_031', 'recursive_chunk_029']",C,"['recursive_chunk_029', 'recursive_chunk_031']",False,True,True,0.55
hybrid,recursive,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_000', 'recursive_chunk_012']",C,['recursive_chunk_001'],False,True,True,0.624
hybrid,recursive,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_073', 'recursive_chunk_003']",B,['recursive_chunk_004'],False,True,True,0.393
hybrid,recursive,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_088', 'recursive_chunk_126']",B,"['recursive_chunk_029', 'recursive_chunk_088']",False,True,True,0.56
hybrid,recursive,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,2.266
hybrid,recursive,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_032', 'recursive_chunk_125']",C,['recursive_chunk_032'],False,True,False,0.691
hybrid,recursive,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,2.886
hybrid,recursive,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_008']",False,True,True,0.599
hybrid,recursive,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",X,[],True,False,,0.411
hybrid,recursive,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,1.212
hybrid,recursive,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_011']",C,['recursive_chunk_016'],False,True,True,0.543
hybrid,recursive,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.575
hybrid,recursive,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_076', 'recursive_chunk_024']",B,['recursive_chunk_024'],False,False,False,0.397
hybrid,recursive,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_023', 'recursive_chunk_024', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,True,0.422
hybrid,recursive,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.802
hybrid,recursive,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_015']",C,['recursive_chunk_048'],False,True,True,0.511
hybrid,recursive,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.627
hybrid,recursive,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.534
hybrid,recursive,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_029', 'recursive_chunk_076']",C,['recursive_chunk_076'],False,True,False,0.523
hybrid,recursive,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.509
hybrid,recursive,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.674
hybrid,recursive,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_053']",C,['recursive_chunk_048'],False,True,True,0.524
hybrid,recursive,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_031', 'recursive_chunk_039']",C,['recursive_chunk_040'],False,True,True,2.69
hybrid,recursive,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_083', 'recursive_chunk_002']",C,['recursive_chunk_023'],False,True,True,0.865
hybrid,recursive,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_002', 'recursive_chunk_029']",X,[],True,False,,0.496
hybrid,recursive,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_029', 'recursive_chunk_097']",C,['recursive_chunk_023'],False,True,True,0.449
hybrid,recursive,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_100', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,False,0.572
hybrid,recursive,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.633
hybrid,recursive,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_049', 'recursive_chunk_048']",B,"['recursive_chunk_048', 'recursive_chunk_023']",False,True,True,0.359
hybrid,recursive,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_038', 'recursive_chunk_041']",D,['recursive_chunk_041'],False,True,True,2.176
hybrid,recursive,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_000', 'recursive_chunk_008']",C,"['recursive_chunk_007', 'recursive_chunk_000']",False,True,True,0.509
hybrid,recursive,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.574
hybrid,recursive,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.685
hybrid,recursive,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_074']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.549
hybrid,recursive,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_031']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,2.843
hybrid,recursive,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.356
hybrid,recursive,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_006', 'recursive_chunk_001']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.571
hybrid,recursive,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_007', 'recursive_chunk_048']",C,['recursive_chunk_008'],False,True,True,0.578
hybrid,recursive,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.607
hybrid,recursive,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_019', 'recursive_chunk_006']",C,['recursive_chunk_019'],False,True,True,0.693
hybrid,recursive,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,2.688
hybrid,recursive,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,2.197
hybrid,recursive,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_097', 'recursive_chunk_029', 'recursive_chunk_024']",C,"['recursive_chunk_029', 'recursive_chunk_097']",False,True,True,0.443
hybrid,recursive,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_050', 'recursive_chunk_048']",C,['recursive_chunk_048'],False,True,True,0.701
hybrid,recursive,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.516
hybrid,recursive,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.507
hybrid,recursive,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.353
hybrid,recursive,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_049']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.518
hybrid,recursive,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_032', 'recursive_chunk_049']",C,['recursive_chunk_008'],False,True,True,0.965
hybrid,recursive,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.421
hybrid,recursive,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_075']",False,True,True,0.455
hybrid,recursive,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",False,True,True,2.483
hybrid,recursive,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,2.582
hybrid,recursive,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_023'],False,True,True,0.646
hybrid,recursive,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_072', 'recursive_chunk_029']",C,['recursive_chunk_032'],False,True,True,0.535
hybrid,recursive,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_041', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.753
hybrid,recursive,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_050', 'recursive_chunk_033']",C,"['recursive_chunk_007', 'recursive_chunk_050']",False,True,True,0.571
hybrid,recursive,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_038', 'recursive_chunk_037']",C,['recursive_chunk_013'],False,True,True,0.328
hybrid,recursive,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_024', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.751
hybrid,recursive,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.617
hybrid,recursive,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.771
hybrid,recursive,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_076']",C,"['recursive_chunk_076', 'recursive_chunk_032']",False,True,True,0.584
hybrid,recursive,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_030', 'recursive_chunk_023']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.718
hybrid,recursive,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_014']",X,[],True,False,,0.572
hybrid,recursive,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.573
hybrid,recursive,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.528
hybrid,recursive,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_031', 'recursive_chunk_007']",C,"['recursive_chunk_007', 'recursive_chunk_001']",False,True,True,0.584
hybrid,recursive,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.505
hybrid,recursive,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",C,"['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",False,True,True,0.594
hybrid,recursive,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_096']",B,['recursive_chunk_048'],False,False,True,0.522
hybrid,recursive,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_031', 'recursive_chunk_029']",C,"['recursive_chunk_029', 'recursive_chunk_031']",False,True,True,0.699
hybrid,recursive,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_000', 'recursive_chunk_012']",C,['recursive_chunk_001'],False,True,True,0.518
hybrid,recursive,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_073', 'recursive_chunk_003']",B,['recursive_chunk_004'],False,True,True,0.489
hybrid,recursive,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_088', 'recursive_chunk_126']",B,"['recursive_chunk_029', 'recursive_chunk_088']",False,True,True,0.573
hybrid,recursive,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.628
hybrid,recursive,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_032', 'recursive_chunk_125']",C,['recursive_chunk_032'],False,True,False,0.52
hybrid,recursive,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.458
hybrid,recursive,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_008']",False,True,True,0.505
hybrid,recursive,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",X,[],True,False,,0.649
hybrid,recursive,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.539
hybrid,recursive,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_011']",C,['recursive_chunk_016'],False,True,True,0.527
hybrid,recursive,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.688
hybrid,recursive,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_076', 'recursive_chunk_024']",B,['recursive_chunk_024'],False,False,False,0.512
hybrid,recursive,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_023', 'recursive_chunk_024', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,True,0.385
hybrid,recursive,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.462
hybrid,recursive,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_015']",C,['recursive_chunk_048'],False,True,True,0.642
hybrid,recursive,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.618
hybrid,recursive,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.534
hybrid,recursive,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_029', 'recursive_chunk_076']",C,['recursive_chunk_076'],False,True,False,0.512
hybrid,recursive,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.586
hybrid,recursive,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.624
hybrid,recursive,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_053']",C,['recursive_chunk_048'],False,True,True,0.511
hybrid,recursive,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_031', 'recursive_chunk_039']",C,['recursive_chunk_040'],False,True,True,0.324
hybrid,recursive,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_083', 'recursive_chunk_002']",C,['recursive_chunk_023'],False,True,True,0.475
hybrid,recursive,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_002', 'recursive_chunk_029']",X,[],True,False,,0.455
hybrid,recursive,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_029', 'recursive_chunk_097']",C,['recursive_chunk_023'],False,True,True,0.599
hybrid,recursive,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_033', 'recursive_chunk_100', 'recursive_chunk_032']",C,['recursive_chunk_032'],False,True,False,0.507
hybrid,recursive,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_085']",C,"['recursive_chunk_029', 'recursive_chunk_015']",False,True,False,0.341
hybrid,recursive,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_023', 'recursive_chunk_049', 'recursive_chunk_048']",B,"['recursive_chunk_048', 'recursive_chunk_023']",False,True,True,0.363
hybrid,recursive,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['recursive_chunk_040', 'recursive_chunk_038', 'recursive_chunk_041']",D,['recursive_chunk_041'],False,True,True,0.636
hybrid,recursive,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_000', 'recursive_chunk_008']",C,"['recursive_chunk_007', 'recursive_chunk_000']",False,True,True,0.526
hybrid,recursive,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['recursive_chunk_029', 'recursive_chunk_028', 'recursive_chunk_084']",C,"['recursive_chunk_029', 'recursive_chunk_028']",False,True,True,0.583
hybrid,recursive,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['recursive_chunk_008', 'recursive_chunk_029', 'recursive_chunk_095']",C,['recursive_chunk_008'],False,True,True,0.694
hybrid,recursive,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_074']",B,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.461
hybrid,recursive,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['recursive_chunk_038', 'recursive_chunk_039', 'recursive_chunk_031']",C,"['recursive_chunk_038', 'recursive_chunk_039']",False,True,True,0.326
hybrid,recursive,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_084']",C,['recursive_chunk_032'],False,True,True,0.52
hybrid,recursive,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['recursive_chunk_007', 'recursive_chunk_006', 'recursive_chunk_001']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.657
hybrid,recursive,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['recursive_chunk_008', 'recursive_chunk_007', 'recursive_chunk_048']",C,['recursive_chunk_008'],False,True,True,0.522
hybrid,recursive,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.475
hybrid,recursive,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['recursive_chunk_007', 'recursive_chunk_019', 'recursive_chunk_006']",C,['recursive_chunk_019'],False,True,True,0.632
hybrid,recursive,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_047']",C,['recursive_chunk_016'],False,True,True,1.756
hybrid,recursive,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_019', 'recursive_chunk_126']",C,"['recursive_chunk_018', 'recursive_chunk_019']",False,True,True,0.695
hybrid,recursive,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['recursive_chunk_097', 'recursive_chunk_029', 'recursive_chunk_024']",C,"['recursive_chunk_029', 'recursive_chunk_097']",False,True,True,0.601
hybrid,recursive,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_074', 'recursive_chunk_050', 'recursive_chunk_048']",C,['recursive_chunk_048'],False,True,True,0.579
hybrid,recursive,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.341
hybrid,recursive,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['recursive_chunk_041', 'recursive_chunk_007', 'recursive_chunk_032']",C,['recursive_chunk_007'],False,True,True,0.509
hybrid,recursive,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_012']",False,True,True,0.378
hybrid,recursive,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_049']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.522
hybrid,recursive,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['recursive_chunk_008', 'recursive_chunk_032', 'recursive_chunk_049']",C,['recursive_chunk_008'],False,True,True,0.551
hybrid,recursive,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_011', 'recursive_chunk_016', 'recursive_chunk_018']",C,['recursive_chunk_016'],False,True,True,0.565
hybrid,recursive,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_025', 'recursive_chunk_075', 'recursive_chunk_019']",C,"['recursive_chunk_019', 'recursive_chunk_075']",False,True,True,0.593
hybrid,recursive,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",C,"['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_014']",False,True,True,0.763
hybrid,recursive,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_037']",C,['recursive_chunk_040'],False,True,True,0.465
hybrid,recursive,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_012', 'recursive_chunk_050']",C,['recursive_chunk_023'],False,True,True,0.633
hybrid,recursive,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['recursive_chunk_032', 'recursive_chunk_072', 'recursive_chunk_029']",C,['recursive_chunk_032'],False,True,True,0.758
hybrid,recursive,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_007', 'recursive_chunk_041', 'recursive_chunk_023']",C,['recursive_chunk_023'],False,True,True,0.486
hybrid,recursive,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_050', 'recursive_chunk_033']",C,"['recursive_chunk_007', 'recursive_chunk_050']",False,True,True,0.396
hybrid,recursive,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_038', 'recursive_chunk_037']",C,['recursive_chunk_013'],False,True,True,0.552
hybrid,recursive,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_024', 'recursive_chunk_073']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.871
hybrid,recursive,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,False,0.551
hybrid,recursive,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['recursive_chunk_018', 'recursive_chunk_029', 'recursive_chunk_017']",C,"['recursive_chunk_018', 'recursive_chunk_017']",False,True,True,0.57
hybrid,recursive,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['recursive_chunk_032', 'recursive_chunk_029', 'recursive_chunk_076']",C,"['recursive_chunk_076', 'recursive_chunk_032']",False,True,True,0.329
hybrid,recursive,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['recursive_chunk_024', 'recursive_chunk_030', 'recursive_chunk_023']",C,"['recursive_chunk_024', 'recursive_chunk_023']",False,True,True,0.502
hybrid,recursive,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['recursive_chunk_015', 'recursive_chunk_016', 'recursive_chunk_014']",X,[],True,False,,0.525
hybrid,recursive,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['recursive_chunk_037', 'recursive_chunk_040', 'recursive_chunk_036']",C,['recursive_chunk_037'],False,True,True,0.652
hybrid,recursive,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_031']",C,['recursive_chunk_040'],False,True,True,0.526
hybrid,recursive,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_001', 'recursive_chunk_031', 'recursive_chunk_007']",C,"['recursive_chunk_007', 'recursive_chunk_001']",False,True,True,0.394
hybrid,recursive,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_000']",B,['recursive_chunk_015'],False,True,False,0.638
hybrid,recursive,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",C,"['recursive_chunk_012', 'recursive_chunk_032', 'recursive_chunk_029']",False,True,True,0.406
hybrid,recursive,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_096']",B,['recursive_chunk_048'],False,False,True,0.559
hybrid,recursive,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['recursive_chunk_032', 'recursive_chunk_031', 'recursive_chunk_029']",C,"['recursive_chunk_029', 'recursive_chunk_031']",False,True,True,0.71
hybrid,recursive,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_000', 'recursive_chunk_012']",C,['recursive_chunk_001'],False,True,True,0.642
hybrid,recursive,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['recursive_chunk_004', 'recursive_chunk_073', 'recursive_chunk_003']",B,['recursive_chunk_004'],False,True,True,0.519
hybrid,recursive,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['recursive_chunk_029', 'recursive_chunk_088', 'recursive_chunk_126']",B,"['recursive_chunk_029', 'recursive_chunk_088']",False,True,True,0.536
hybrid,recursive,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['recursive_chunk_031', 'recursive_chunk_039', 'recursive_chunk_040']",C,['recursive_chunk_039'],False,True,True,0.638
hybrid,recursive,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_005', 'recursive_chunk_032', 'recursive_chunk_125']",C,['recursive_chunk_032'],False,True,False,0.35
hybrid,recursive,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['recursive_chunk_001', 'recursive_chunk_006', 'recursive_chunk_048']",C,"['recursive_chunk_001', 'recursive_chunk_006']",False,True,True,0.504
hybrid,recursive,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['recursive_chunk_007', 'recursive_chunk_008', 'recursive_chunk_012']",C,"['recursive_chunk_007', 'recursive_chunk_008']",False,True,True,0.571
hybrid,recursive,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['recursive_chunk_007', 'recursive_chunk_017', 'recursive_chunk_012']",X,[],True,False,,0.513
hybrid,recursive,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['recursive_chunk_013', 'recursive_chunk_073', 'recursive_chunk_023']",C,"['recursive_chunk_013', 'recursive_chunk_073']",False,True,True,0.515
hybrid,recursive,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['recursive_chunk_016', 'recursive_chunk_015', 'recursive_chunk_011']",C,['recursive_chunk_016'],False,True,True,0.373
hybrid,recursive,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['recursive_chunk_029', 'recursive_chunk_018', 'recursive_chunk_085']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.541
hybrid,recursive,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['recursive_chunk_008', 'recursive_chunk_076', 'recursive_chunk_024']",B,['recursive_chunk_024'],False,False,False,0.639
hybrid,recursive,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['recursive_chunk_023', 'recursive_chunk_024', 'recursive_chunk_030']",C,['recursive_chunk_030'],False,True,True,0.585
hybrid,recursive,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_039', 'recursive_chunk_023']",C,['recursive_chunk_040'],False,True,True,0.877
hybrid,recursive,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['recursive_chunk_048', 'recursive_chunk_014', 'recursive_chunk_015']",C,['recursive_chunk_048'],False,True,True,0.476
hybrid,recursive,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['recursive_chunk_007', 'recursive_chunk_048', 'recursive_chunk_011']",C,"['recursive_chunk_007', 'recursive_chunk_011']",False,True,True,0.571
hybrid,recursive,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['recursive_chunk_013', 'recursive_chunk_004', 'recursive_chunk_049']",C,['recursive_chunk_013'],False,True,True,0.574
hybrid,recursive,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['recursive_chunk_024', 'recursive_chunk_029', 'recursive_chunk_076']",C,['recursive_chunk_076'],False,True,False,0.677
hybrid,recursive,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['recursive_chunk_029', 'recursive_chunk_015', 'recursive_chunk_032']",B,['recursive_chunk_015'],False,False,False,0.587
hybrid,recursive,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['recursive_chunk_029', 'recursive_chunk_109', 'recursive_chunk_018']",C,"['recursive_chunk_018', 'recursive_chunk_029']",False,True,True,0.579
hybrid,recursive,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['recursive_chunk_048', 'recursive_chunk_024', 'recursive_chunk_053']",C,['recursive_chunk_048'],False,True,True,0.677
hybrid,recursive,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['recursive_chunk_040', 'recursive_chunk_031', 'recursive_chunk_039']",C,['recursive_chunk_040'],False,True,True,0.403
hybrid,recursive,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['recursive_chunk_023', 'recursive_chunk_083', 'recursive_chunk_002']",C,['recursive_chunk_023'],False,True,True,0.756
hybrid,recursive,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['recursive_chunk_015', 'recursive_chunk_002', 'recursive_chunk_029']",X,[],True,False,,0.765
hybrid,recursive,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['recursive_chunk_023', 'recursive_chunk_029', 'recursive_chunk_097']",C,['recursive_chunk_023'],False,True,True,0.584
baseline,paragraph,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.467
baseline,paragraph,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.349
baseline,paragraph,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.76
baseline,paragraph,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.414
baseline,paragraph,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.26
baseline,paragraph,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.448
baseline,paragraph,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.962
baseline,paragraph,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.447
baseline,paragraph,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.457
baseline,paragraph,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.591
baseline,paragraph,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.442
baseline,paragraph,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.465
baseline,paragraph,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.334
baseline,paragraph,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.568
baseline,paragraph,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.369
baseline,paragraph,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.534
baseline,paragraph,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.499
baseline,paragraph,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.457
baseline,paragraph,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.348
baseline,paragraph,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.469
baseline,paragraph,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.44
baseline,paragraph,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.588
baseline,paragraph,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.375
baseline,paragraph,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.511
baseline,paragraph,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.376
baseline,paragraph,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.457
baseline,paragraph,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.63
baseline,paragraph,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.429
baseline,paragraph,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.438
baseline,paragraph,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.305
baseline,paragraph,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.399
baseline,paragraph,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.387
baseline,paragraph,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.459
baseline,paragraph,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.508
baseline,paragraph,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.401
baseline,paragraph,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.507
baseline,paragraph,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.407
baseline,paragraph,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.276
baseline,paragraph,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.505
baseline,paragraph,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.446
baseline,paragraph,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.476
baseline,paragraph,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.382
baseline,paragraph,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.364
baseline,paragraph,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.463
baseline,paragraph,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.26
baseline,paragraph,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.411
baseline,paragraph,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.425
baseline,paragraph,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.385
baseline,paragraph,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.594
baseline,paragraph,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.519
baseline,paragraph,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.454
baseline,paragraph,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.451
baseline,paragraph,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.464
baseline,paragraph,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.382
baseline,paragraph,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.465
baseline,paragraph,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.28
baseline,paragraph,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.407
baseline,paragraph,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.365
baseline,paragraph,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.587
baseline,paragraph,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.271
baseline,paragraph,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.461
baseline,paragraph,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.454
baseline,paragraph,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.394
baseline,paragraph,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.401
baseline,paragraph,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.389
baseline,paragraph,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.404
baseline,paragraph,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.453
baseline,paragraph,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.521
baseline,paragraph,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.283
baseline,paragraph,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.474
baseline,paragraph,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.869
baseline,paragraph,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.521
baseline,paragraph,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.534
baseline,paragraph,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.408
baseline,paragraph,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.505
baseline,paragraph,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.52
baseline,paragraph,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.55
baseline,paragraph,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.475
baseline,paragraph,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.517
baseline,paragraph,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.43
baseline,paragraph,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.478
baseline,paragraph,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.579
baseline,paragraph,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.271
baseline,paragraph,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.447
baseline,paragraph,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.512
baseline,paragraph,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.401
baseline,paragraph,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.455
baseline,paragraph,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.509
baseline,paragraph,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.354
baseline,paragraph,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.253
baseline,paragraph,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.559
baseline,paragraph,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.277
baseline,paragraph,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.404
baseline,paragraph,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.271
baseline,paragraph,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.397
baseline,paragraph,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.273
baseline,paragraph,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.29
baseline,paragraph,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.569
baseline,paragraph,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.44
baseline,paragraph,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.342
baseline,paragraph,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.525
baseline,paragraph,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.505
baseline,paragraph,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.437
baseline,paragraph,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.446
baseline,paragraph,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.498
baseline,paragraph,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.83
baseline,paragraph,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.269
baseline,paragraph,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.449
baseline,paragraph,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.475
baseline,paragraph,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.338
baseline,paragraph,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.484
baseline,paragraph,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.885
baseline,paragraph,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.57
baseline,paragraph,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.482
baseline,paragraph,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.463
baseline,paragraph,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.469
baseline,paragraph,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.51
baseline,paragraph,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.522
baseline,paragraph,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.449
baseline,paragraph,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.409
baseline,paragraph,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.543
baseline,paragraph,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.429
baseline,paragraph,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.754
baseline,paragraph,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.392
baseline,paragraph,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.398
baseline,paragraph,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.422
baseline,paragraph,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.438
baseline,paragraph,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.281
baseline,paragraph,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.466
baseline,paragraph,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.394
baseline,paragraph,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.393
baseline,paragraph,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.41
baseline,paragraph,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.519
baseline,paragraph,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.281
baseline,paragraph,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.516
baseline,paragraph,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.263
baseline,paragraph,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.46
baseline,paragraph,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.243
baseline,paragraph,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.465
baseline,paragraph,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.726
baseline,paragraph,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.381
baseline,paragraph,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.509
baseline,paragraph,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.495
baseline,paragraph,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.503
baseline,paragraph,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.327
baseline,paragraph,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.291
baseline,paragraph,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.499
baseline,paragraph,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.33
baseline,paragraph,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.461
baseline,paragraph,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.512
baseline,paragraph,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.459
baseline,paragraph,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.434
baseline,paragraph,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.47
baseline,paragraph,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.624
baseline,paragraph,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.474
baseline,paragraph,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.443
baseline,paragraph,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.44
baseline,paragraph,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.397
baseline,paragraph,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.408
baseline,paragraph,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.456
baseline,paragraph,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.665
baseline,paragraph,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.242
baseline,paragraph,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.454
baseline,paragraph,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.524
baseline,paragraph,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.455
baseline,paragraph,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.528
baseline,paragraph,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.507
baseline,paragraph,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.456
baseline,paragraph,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.462
baseline,paragraph,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.284
baseline,paragraph,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.402
baseline,paragraph,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.379
baseline,paragraph,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.473
baseline,paragraph,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.566
baseline,paragraph,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.377
baseline,paragraph,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.493
baseline,paragraph,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.431
baseline,paragraph,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.476
baseline,paragraph,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.808
baseline,paragraph,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.704
baseline,paragraph,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.577
baseline,paragraph,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.525
baseline,paragraph,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.382
baseline,paragraph,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.52
baseline,paragraph,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.46
baseline,paragraph,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.377
baseline,paragraph,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.508
baseline,paragraph,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.481
baseline,paragraph,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.394
baseline,paragraph,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.302
baseline,paragraph,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.386
baseline,paragraph,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.468
baseline,paragraph,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.509
baseline,paragraph,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.524
baseline,paragraph,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.466
baseline,paragraph,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.476
baseline,paragraph,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.624
baseline,paragraph,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.661
baseline,paragraph,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.306
baseline,paragraph,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.765
baseline,paragraph,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.467
baseline,paragraph,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.513
baseline,paragraph,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.522
baseline,paragraph,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.493
baseline,paragraph,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.76
baseline,paragraph,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.412
baseline,paragraph,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.394
baseline,paragraph,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.444
baseline,paragraph,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.465
baseline,paragraph,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.846
baseline,paragraph,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.269
baseline,paragraph,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.868
baseline,paragraph,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.501
baseline,paragraph,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.291
baseline,paragraph,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.38
baseline,paragraph,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.652
baseline,paragraph,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.448
baseline,paragraph,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.525
baseline,paragraph,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.437
baseline,paragraph,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.581
baseline,paragraph,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.276
baseline,paragraph,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.496
baseline,paragraph,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.276
baseline,paragraph,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.481
baseline,paragraph,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.585
baseline,paragraph,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.435
baseline,paragraph,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.424
baseline,paragraph,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.452
baseline,paragraph,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.47
baseline,paragraph,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.464
baseline,paragraph,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.296
baseline,paragraph,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.494
baseline,paragraph,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.412
baseline,paragraph,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.502
baseline,paragraph,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.399
baseline,paragraph,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.459
baseline,paragraph,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.436
baseline,paragraph,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.505
baseline,paragraph,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.676
baseline,paragraph,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.686
baseline,paragraph,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.373
baseline,paragraph,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.283
baseline,paragraph,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.295
baseline,paragraph,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.502
baseline,paragraph,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.45
baseline,paragraph,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.363
baseline,paragraph,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.38
baseline,paragraph,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.269
baseline,paragraph,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.413
baseline,paragraph,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.439
baseline,paragraph,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.522
baseline,paragraph,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.263
baseline,paragraph,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.319
baseline,paragraph,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.522
baseline,paragraph,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.526
baseline,paragraph,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.341
baseline,paragraph,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.499
baseline,paragraph,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.506
baseline,paragraph,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.597
baseline,paragraph,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.491
baseline,paragraph,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.515
baseline,paragraph,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.397
baseline,paragraph,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.785
baseline,paragraph,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.644
baseline,paragraph,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.439
baseline,paragraph,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.513
baseline,paragraph,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.456
baseline,paragraph,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.585
baseline,paragraph,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.552
baseline,paragraph,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.444
baseline,paragraph,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.339
baseline,paragraph,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.515
baseline,paragraph,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.527
baseline,paragraph,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.513
baseline,paragraph,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.517
baseline,paragraph,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.479
baseline,paragraph,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.36
baseline,paragraph,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.633
baseline,paragraph,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.453
baseline,paragraph,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.53
baseline,paragraph,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.514
baseline,paragraph,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.453
baseline,paragraph,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],B,[],False,True,,0.454
baseline,paragraph,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",[],D,[],False,True,,0.739
baseline,paragraph,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",[],C,[],False,True,,0.68
baseline,paragraph,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",[],D,[],False,False,,0.47
baseline,paragraph,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",[],C,[],False,True,,0.254
baseline,paragraph,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",[],A,[],False,False,,0.457
baseline,paragraph,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",[],C,[],False,True,,0.649
baseline,paragraph,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",[],C,[],False,True,,0.405
baseline,paragraph,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",[],A,[],False,False,,0.493
baseline,paragraph,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",[],A,[],False,False,,0.446
baseline,paragraph,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",[],C,[],False,True,,0.402
baseline,paragraph,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",[],C,[],False,True,,0.465
baseline,paragraph,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.573
baseline,paragraph,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",[],C,[],False,True,,0.357
baseline,paragraph,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",[],D,[],False,False,,0.495
baseline,paragraph,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",[],D,[],False,False,,0.447
baseline,paragraph,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",[],C,[],False,True,,0.453
baseline,paragraph,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",[],C,[],False,True,,0.286
baseline,paragraph,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.277
baseline,paragraph,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",[],A,[],False,False,,0.45
baseline,paragraph,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",[],C,[],False,True,,0.644
baseline,paragraph,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],C,[],False,True,,0.47
baseline,paragraph,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],X,[],True,False,,0.534
baseline,paragraph,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],B,[],False,False,,0.59
baseline,paragraph,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",[],C,[],False,True,,0.391
baseline,paragraph,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.465
baseline,paragraph,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",[],C,[],False,True,,0.472
baseline,paragraph,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],A,[],False,False,,0.447
baseline,paragraph,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],X,[],True,False,,0.267
baseline,paragraph,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.384
baseline,paragraph,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",[],C,[],False,True,,0.473
baseline,paragraph,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",[],C,[],False,True,,0.44
baseline,paragraph,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",[],C,[],False,True,,0.467
baseline,paragraph,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",[],C,[],False,True,,0.292
baseline,paragraph,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",[],X,[],True,False,,0.449
baseline,paragraph,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,[],X,[],True,False,,0.53
baseline,paragraph,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,[],X,[],True,False,,0.436
baseline,paragraph,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.472
baseline,paragraph,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.518
baseline,paragraph,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,True,,0.662
baseline,paragraph,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],C,[],False,True,,0.475
baseline,paragraph,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",[],B,['chunk_002'],False,False,False,0.463
baseline,paragraph,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,[],C,[],False,True,,0.48
baseline,paragraph,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.322
baseline,paragraph,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",[],X,[],True,False,,0.31
baseline,paragraph,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",[],B,[],False,True,,0.441
baseline,paragraph,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",[],X,[],True,False,,0.398
baseline,paragraph,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.466
baseline,paragraph,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",[],C,[],False,True,,0.694
baseline,paragraph,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",[],C,[],False,True,,0.516
baseline,paragraph,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",[],C,[],False,True,,0.463
baseline,paragraph,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",[],X,[],True,False,,0.456
baseline,paragraph,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",[],D,[],False,False,,0.562
baseline,paragraph,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",[],C,[],False,True,,0.44
baseline,paragraph,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",[],B,[],False,False,,0.331
baseline,paragraph,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",[],C,[],False,True,,0.465
baseline,paragraph,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",[],C,[],False,True,,0.39
baseline,paragraph,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",[],C,[],False,True,,0.396
baseline,paragraph,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",[],C,[],False,True,,0.531
baseline,paragraph,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",[],X,[],True,False,,0.57
baseline,paragraph,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",[],C,[],False,True,,0.397
baseline,paragraph,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",[],B,[],False,False,,0.489
baseline,paragraph,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",[],C,[],False,True,,0.404
baseline,paragraph,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",[],C,[],False,True,,0.289
baseline,paragraph,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",[],C,[],False,True,,0.437
baseline,paragraph,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",[],C,[],False,True,,0.513
baseline,paragraph,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",[],C,[],False,True,,0.555
baseline,paragraph,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",[],C,[],False,True,,0.728
bm25,paragraph,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_036', 'paragraph_chunk_001']",X,[],True,False,,0.272
bm25,paragraph,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",A,"['014', '021']",False,False,False,0.768
bm25,paragraph,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_009']",B,['paragraph_chunk_034'],False,True,True,0.575
bm25,paragraph,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.527
bm25,paragraph,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_006', 'paragraph_chunk_001']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.549
bm25,paragraph,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_015', 'paragraph_chunk_102']",C,['015'],False,True,False,0.307
bm25,paragraph,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_006', 'paragraph_chunk_024', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.636
bm25,paragraph,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_006']",B,['paragraph_chunk_086'],False,True,True,0.323
bm25,paragraph,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['029', '028']",False,True,False,0.492
bm25,paragraph,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.494
bm25,paragraph,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.502
bm25,paragraph,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_033']",C,['paragraph_chunk_006'],False,True,True,0.566
bm25,paragraph,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_026']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.551
bm25,paragraph,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_005', 'paragraph_chunk_001']",C,['paragraph_chunk_016'],False,True,True,0.518
bm25,paragraph,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_014', 'paragraph_chunk_033', 'paragraph_chunk_015']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.34
bm25,paragraph,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.517
bm25,paragraph,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_018']",C,['021'],False,True,False,0.481
bm25,paragraph,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.447
bm25,paragraph,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.471
bm25,paragraph,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_006', 'paragraph_chunk_003', 'paragraph_chunk_030']",C,['006'],False,True,False,0.448
bm25,paragraph,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['006', '005']",False,True,False,0.36
bm25,paragraph,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_086', 'paragraph_chunk_002', 'paragraph_chunk_034']",C,['paragraph_chunk_086'],False,True,True,0.521
bm25,paragraph,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_035', 'paragraph_chunk_034']",C,['paragraph_chunk_006'],False,True,True,0.461
bm25,paragraph,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_113']",C,['paragraph_chunk_014'],False,True,True,0.398
bm25,paragraph,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_087', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,['paragraph_chunk_016'],False,True,True,0.443
bm25,paragraph,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.426
bm25,paragraph,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.538
bm25,paragraph,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,['paragraph_chunk_018'],False,True,True,0.66
bm25,paragraph,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_099']",C,['paragraph_chunk_025'],False,True,True,0.497
bm25,paragraph,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_030', 'paragraph_chunk_006']",C,['paragraph_chunk_018'],False,True,True,0.316
bm25,paragraph,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_005', 'paragraph_chunk_006']",C,['paragraph_chunk_006'],False,True,True,0.442
bm25,paragraph,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_028', 'paragraph_chunk_087']",C,"['paragraph_chunk_086', 'paragraph_chunk_087']",False,True,True,0.425
bm25,paragraph,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_018']",C,['paragraph_chunk_086'],False,True,True,0.351
bm25,paragraph,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.774
bm25,paragraph,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_021']",C,['paragraph_chunk_015'],False,True,True,0.506
bm25,paragraph,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_034']",C,['paragraph_chunk_016'],False,True,True,0.538
bm25,paragraph,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_029', 'paragraph_chunk_002']",C,['paragraph_chunk_018'],False,True,True,0.502
bm25,paragraph,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_021']",X,[],True,False,,0.507
bm25,paragraph,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_027']",C,"['paragraph_chunk_028', 'paragraph_chunk_027']",False,True,True,0.507
bm25,paragraph,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.507
bm25,paragraph,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_001', 'paragraph_chunk_034', 'paragraph_chunk_033']",C,['paragraph_chunk_001'],False,True,False,0.333
bm25,paragraph,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",B,['paragraph_chunk_014'],False,True,True,0.666
bm25,paragraph,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_021']",C,"['paragraph_chunk_008', 'paragraph_chunk_021']",False,True,False,0.543
bm25,paragraph,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_001', 'paragraph_chunk_036']",B,"['paragraph_chunk_034', 'paragraph_chunk_001']",False,False,False,0.557
bm25,paragraph,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_021', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.498
bm25,paragraph,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_009', 'paragraph_chunk_034']",C,"['008', '009']",False,True,False,0.469
bm25,paragraph,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_001']",B,['paragraph_chunk_004'],False,True,True,0.488
bm25,paragraph,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_103', 'paragraph_chunk_139']",B,['paragraph_chunk_015'],False,True,True,0.923
bm25,paragraph,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.443
bm25,paragraph,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_139', 'paragraph_chunk_001']",C,"['001', '005']",False,True,False,0.522
bm25,paragraph,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_001', 'paragraph_chunk_005', 'paragraph_chunk_034']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.527
bm25,paragraph,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_005', 'paragraph_chunk_015']",C,['006'],False,True,False,0.528
bm25,paragraph,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_015', 'paragraph_chunk_008', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,9.871
bm25,paragraph,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_018', 'paragraph_chunk_010']",C,"['086', '010']",False,True,False,0.499
bm25,paragraph,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_086']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.478
bm25,paragraph,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.45
bm25,paragraph,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_006', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,"['paragraph_chunk_016', 'paragraph_chunk_008']",False,True,True,0.468
bm25,paragraph,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.558
bm25,paragraph,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_002']",C,['paragraph_chunk_029'],False,True,True,0.671
bm25,paragraph,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_033', 'paragraph_chunk_005']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.624
bm25,paragraph,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['paragraph_chunk_006', 'paragraph_chunk_005']",False,True,True,0.51
bm25,paragraph,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_034']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.362
bm25,paragraph,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_100', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.405
bm25,paragraph,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,1.106
bm25,paragraph,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_124', 'paragraph_chunk_015']",C,['015'],False,True,False,0.561
bm25,paragraph,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.498
bm25,paragraph,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.558
bm25,paragraph,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_097', 'paragraph_chunk_002', 'paragraph_chunk_003']",C,"['002', '003']",False,True,False,0.473
bm25,paragraph,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_014', 'paragraph_chunk_017']",C,['paragraph_chunk_017'],False,True,True,0.333
bm25,paragraph,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.444
bm25,paragraph,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_036', 'paragraph_chunk_001']",X,[],True,False,,0.451
bm25,paragraph,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",A,"['014', '021']",False,False,False,0.618
bm25,paragraph,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_009']",B,['paragraph_chunk_034'],False,True,True,0.452
bm25,paragraph,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.544
bm25,paragraph,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_006', 'paragraph_chunk_001']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.505
bm25,paragraph,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_015', 'paragraph_chunk_102']",C,['015'],False,True,False,0.526
bm25,paragraph,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_006', 'paragraph_chunk_024', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.487
bm25,paragraph,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_006']",B,['paragraph_chunk_086'],False,True,True,0.56
bm25,paragraph,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['029', '028']",False,True,False,0.314
bm25,paragraph,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.622
bm25,paragraph,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.568
bm25,paragraph,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_033']",C,['paragraph_chunk_006'],False,True,True,0.471
bm25,paragraph,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_026']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.454
bm25,paragraph,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_005', 'paragraph_chunk_001']",C,['paragraph_chunk_016'],False,True,True,0.332
bm25,paragraph,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_014', 'paragraph_chunk_033', 'paragraph_chunk_015']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.509
bm25,paragraph,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.422
bm25,paragraph,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_018']",C,['021'],False,True,False,0.395
bm25,paragraph,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.5
bm25,paragraph,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.461
bm25,paragraph,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_006', 'paragraph_chunk_003', 'paragraph_chunk_030']",C,['006'],False,True,False,0.455
bm25,paragraph,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['006', '005']",False,True,False,0.53
bm25,paragraph,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_086', 'paragraph_chunk_002', 'paragraph_chunk_034']",C,['paragraph_chunk_086'],False,True,True,0.499
bm25,paragraph,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_035', 'paragraph_chunk_034']",C,['paragraph_chunk_006'],False,True,True,0.474
bm25,paragraph,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_113']",C,['paragraph_chunk_014'],False,True,True,0.445
bm25,paragraph,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_087', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,['paragraph_chunk_016'],False,True,True,0.506
bm25,paragraph,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.492
bm25,paragraph,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.384
bm25,paragraph,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,['paragraph_chunk_018'],False,True,True,0.47
bm25,paragraph,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_099']",C,['paragraph_chunk_025'],False,True,True,0.603
bm25,paragraph,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_030', 'paragraph_chunk_006']",C,['paragraph_chunk_018'],False,True,True,0.294
bm25,paragraph,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_005', 'paragraph_chunk_006']",C,['paragraph_chunk_006'],False,True,True,0.563
bm25,paragraph,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_028', 'paragraph_chunk_087']",C,"['paragraph_chunk_086', 'paragraph_chunk_087']",False,True,True,0.471
bm25,paragraph,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_018']",C,['paragraph_chunk_086'],False,True,True,0.476
bm25,paragraph,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.726
bm25,paragraph,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_021']",C,['paragraph_chunk_015'],False,True,True,0.56
bm25,paragraph,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_034']",C,['paragraph_chunk_016'],False,True,True,0.528
bm25,paragraph,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_029', 'paragraph_chunk_002']",C,['paragraph_chunk_018'],False,True,True,0.325
bm25,paragraph,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_021']",X,[],True,False,,0.72
bm25,paragraph,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_027']",C,"['paragraph_chunk_028', 'paragraph_chunk_027']",False,True,True,0.697
bm25,paragraph,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.79
bm25,paragraph,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_001', 'paragraph_chunk_034', 'paragraph_chunk_033']",C,['paragraph_chunk_001'],False,True,False,0.427
bm25,paragraph,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",B,['paragraph_chunk_014'],False,True,True,0.597
bm25,paragraph,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_021']",C,"['paragraph_chunk_008', 'paragraph_chunk_021']",False,True,False,0.509
bm25,paragraph,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_001', 'paragraph_chunk_036']",B,"['paragraph_chunk_034', 'paragraph_chunk_001']",False,False,False,0.523
bm25,paragraph,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_021', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.493
bm25,paragraph,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_009', 'paragraph_chunk_034']",C,"['008', '009']",False,True,False,0.549
bm25,paragraph,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_001']",B,['paragraph_chunk_004'],False,True,True,0.545
bm25,paragraph,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_103', 'paragraph_chunk_139']",B,['paragraph_chunk_015'],False,True,True,0.448
bm25,paragraph,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.556
bm25,paragraph,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_139', 'paragraph_chunk_001']",C,"['001', '005']",False,True,False,0.563
bm25,paragraph,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_001', 'paragraph_chunk_005', 'paragraph_chunk_034']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.451
bm25,paragraph,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_005', 'paragraph_chunk_015']",C,['006'],False,True,False,0.394
bm25,paragraph,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_015', 'paragraph_chunk_008', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.511
bm25,paragraph,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_018', 'paragraph_chunk_010']",C,"['086', '010']",False,True,False,0.469
bm25,paragraph,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_086']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.433
bm25,paragraph,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.517
bm25,paragraph,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_006', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,"['paragraph_chunk_016', 'paragraph_chunk_008']",False,True,True,0.349
bm25,paragraph,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.556
bm25,paragraph,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_002']",C,['paragraph_chunk_029'],False,True,True,0.438
bm25,paragraph,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_033', 'paragraph_chunk_005']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.339
bm25,paragraph,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['paragraph_chunk_006', 'paragraph_chunk_005']",False,True,True,0.739
bm25,paragraph,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_034']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.497
bm25,paragraph,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_100', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.667
bm25,paragraph,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.507
bm25,paragraph,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_124', 'paragraph_chunk_015']",C,['015'],False,True,False,2.563
bm25,paragraph,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.52
bm25,paragraph,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.45
bm25,paragraph,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_097', 'paragraph_chunk_002', 'paragraph_chunk_003']",C,"['002', '003']",False,True,False,0.627
bm25,paragraph,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_014', 'paragraph_chunk_017']",C,['paragraph_chunk_017'],False,True,True,5.992
bm25,paragraph,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.484
bm25,paragraph,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_036', 'paragraph_chunk_001']",X,[],True,False,,0.383
bm25,paragraph,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",A,"['014', '021']",False,False,False,2.668
bm25,paragraph,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_009']",B,['paragraph_chunk_034'],False,True,True,0.631
bm25,paragraph,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.593
bm25,paragraph,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_006', 'paragraph_chunk_001']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.698
bm25,paragraph,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_015', 'paragraph_chunk_102']",C,['015'],False,True,False,0.339
bm25,paragraph,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_006', 'paragraph_chunk_024', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.486
bm25,paragraph,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_006']",B,['paragraph_chunk_086'],False,True,True,2.111
bm25,paragraph,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['029', '028']",False,True,False,0.529
bm25,paragraph,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.403
bm25,paragraph,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.312
bm25,paragraph,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_033']",C,['paragraph_chunk_006'],False,True,True,0.688
bm25,paragraph,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_026']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.35
bm25,paragraph,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_005', 'paragraph_chunk_001']",C,['paragraph_chunk_016'],False,True,True,0.461
bm25,paragraph,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_014', 'paragraph_chunk_033', 'paragraph_chunk_015']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.479
bm25,paragraph,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.537
bm25,paragraph,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_018']",C,['021'],False,True,False,0.293
bm25,paragraph,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.286
bm25,paragraph,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,2.282
bm25,paragraph,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_006', 'paragraph_chunk_003', 'paragraph_chunk_030']",C,['006'],False,True,False,0.411
bm25,paragraph,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['006', '005']",False,True,False,0.453
bm25,paragraph,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_086', 'paragraph_chunk_002', 'paragraph_chunk_034']",C,['paragraph_chunk_086'],False,True,True,0.542
bm25,paragraph,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_035', 'paragraph_chunk_034']",C,['paragraph_chunk_006'],False,True,True,0.469
bm25,paragraph,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_113']",C,['paragraph_chunk_014'],False,True,True,0.457
bm25,paragraph,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_087', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,['paragraph_chunk_016'],False,True,True,0.452
bm25,paragraph,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.474
bm25,paragraph,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.528
bm25,paragraph,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,['paragraph_chunk_018'],False,True,True,0.472
bm25,paragraph,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_099']",C,['paragraph_chunk_025'],False,True,True,0.55
bm25,paragraph,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_030', 'paragraph_chunk_006']",C,['paragraph_chunk_018'],False,True,True,0.316
bm25,paragraph,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_005', 'paragraph_chunk_006']",C,['paragraph_chunk_006'],False,True,True,0.622
bm25,paragraph,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_028', 'paragraph_chunk_087']",C,"['paragraph_chunk_086', 'paragraph_chunk_087']",False,True,True,0.626
bm25,paragraph,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_018']",C,['paragraph_chunk_086'],False,True,True,0.608
bm25,paragraph,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.5
bm25,paragraph,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_021']",C,['paragraph_chunk_015'],False,True,True,0.392
bm25,paragraph,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_034']",C,['paragraph_chunk_016'],False,True,True,0.509
bm25,paragraph,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_029', 'paragraph_chunk_002']",C,['paragraph_chunk_018'],False,True,True,0.584
bm25,paragraph,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_021']",X,[],True,False,,0.51
bm25,paragraph,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_027']",C,"['paragraph_chunk_028', 'paragraph_chunk_027']",False,True,True,0.662
bm25,paragraph,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.554
bm25,paragraph,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_001', 'paragraph_chunk_034', 'paragraph_chunk_033']",C,['paragraph_chunk_001'],False,True,False,0.392
bm25,paragraph,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",B,['paragraph_chunk_014'],False,True,True,0.87
bm25,paragraph,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_021']",C,"['paragraph_chunk_008', 'paragraph_chunk_021']",False,True,False,0.53
bm25,paragraph,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_001', 'paragraph_chunk_036']",B,"['paragraph_chunk_034', 'paragraph_chunk_001']",False,False,False,0.645
bm25,paragraph,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_021', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.513
bm25,paragraph,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_009', 'paragraph_chunk_034']",C,"['008', '009']",False,True,False,0.463
bm25,paragraph,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_001']",B,['paragraph_chunk_004'],False,True,True,0.284
bm25,paragraph,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_103', 'paragraph_chunk_139']",B,['paragraph_chunk_015'],False,True,True,0.565
bm25,paragraph,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.826
bm25,paragraph,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_139', 'paragraph_chunk_001']",C,"['001', '005']",False,True,False,0.766
bm25,paragraph,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_001', 'paragraph_chunk_005', 'paragraph_chunk_034']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,1.441
bm25,paragraph,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_005', 'paragraph_chunk_015']",C,['006'],False,True,False,0.936
bm25,paragraph,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_015', 'paragraph_chunk_008', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.287
bm25,paragraph,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_018', 'paragraph_chunk_010']",C,"['086', '010']",False,True,False,0.624
bm25,paragraph,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_086']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.334
bm25,paragraph,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.518
bm25,paragraph,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_006', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,"['paragraph_chunk_016', 'paragraph_chunk_008']",False,True,True,3.413
bm25,paragraph,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.441
bm25,paragraph,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_002']",C,['paragraph_chunk_029'],False,True,True,0.499
bm25,paragraph,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_033', 'paragraph_chunk_005']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.601
bm25,paragraph,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['paragraph_chunk_006', 'paragraph_chunk_005']",False,True,True,0.58
bm25,paragraph,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_034']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.329
bm25,paragraph,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_100', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.518
bm25,paragraph,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.505
bm25,paragraph,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_124', 'paragraph_chunk_015']",C,['015'],False,True,False,0.562
bm25,paragraph,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.496
bm25,paragraph,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.527
bm25,paragraph,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_097', 'paragraph_chunk_002', 'paragraph_chunk_003']",C,"['002', '003']",False,True,False,0.489
bm25,paragraph,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_014', 'paragraph_chunk_017']",C,['paragraph_chunk_017'],False,True,True,0.507
bm25,paragraph,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.496
bm25,paragraph,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_036', 'paragraph_chunk_001']",X,[],True,False,,0.465
bm25,paragraph,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",A,"['014', '021']",False,False,False,0.566
bm25,paragraph,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_009']",B,['paragraph_chunk_034'],False,True,True,0.466
bm25,paragraph,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.439
bm25,paragraph,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_006', 'paragraph_chunk_001']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.462
bm25,paragraph,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_015', 'paragraph_chunk_102']",C,['015'],False,True,False,0.449
bm25,paragraph,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_006', 'paragraph_chunk_024', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.448
bm25,paragraph,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_006']",B,['paragraph_chunk_086'],False,True,True,0.677
bm25,paragraph,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['029', '028']",False,True,False,0.441
bm25,paragraph,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.413
bm25,paragraph,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.677
bm25,paragraph,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_033']",C,['paragraph_chunk_006'],False,True,True,0.571
bm25,paragraph,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_026']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.413
bm25,paragraph,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_005', 'paragraph_chunk_001']",C,['paragraph_chunk_016'],False,True,True,0.482
bm25,paragraph,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_014', 'paragraph_chunk_033', 'paragraph_chunk_015']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,1.366
bm25,paragraph,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.452
bm25,paragraph,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_018']",C,['021'],False,True,False,0.559
bm25,paragraph,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.999
bm25,paragraph,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.468
bm25,paragraph,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_006', 'paragraph_chunk_003', 'paragraph_chunk_030']",C,['006'],False,True,False,0.497
bm25,paragraph,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['006', '005']",False,True,False,0.554
bm25,paragraph,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_086', 'paragraph_chunk_002', 'paragraph_chunk_034']",C,['paragraph_chunk_086'],False,True,True,0.505
bm25,paragraph,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_035', 'paragraph_chunk_034']",C,['paragraph_chunk_006'],False,True,True,0.548
bm25,paragraph,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_113']",C,['paragraph_chunk_014'],False,True,True,0.559
bm25,paragraph,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_087', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,['paragraph_chunk_016'],False,True,True,0.446
bm25,paragraph,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.503
bm25,paragraph,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.949
bm25,paragraph,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,['paragraph_chunk_018'],False,True,True,0.673
bm25,paragraph,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_099']",C,['paragraph_chunk_025'],False,True,True,0.77
bm25,paragraph,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_030', 'paragraph_chunk_006']",C,['paragraph_chunk_018'],False,True,True,0.307
bm25,paragraph,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_005', 'paragraph_chunk_006']",C,['paragraph_chunk_006'],False,True,True,0.523
bm25,paragraph,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_028', 'paragraph_chunk_087']",C,"['paragraph_chunk_086', 'paragraph_chunk_087']",False,True,True,0.563
bm25,paragraph,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_018']",C,['paragraph_chunk_086'],False,True,True,0.454
bm25,paragraph,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.449
bm25,paragraph,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_021']",C,['paragraph_chunk_015'],False,True,True,0.678
bm25,paragraph,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_034']",C,['paragraph_chunk_016'],False,True,True,0.339
bm25,paragraph,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_029', 'paragraph_chunk_002']",C,['paragraph_chunk_018'],False,True,True,0.505
bm25,paragraph,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_021']",X,[],True,False,,0.47
bm25,paragraph,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_027']",C,"['paragraph_chunk_028', 'paragraph_chunk_027']",False,True,True,0.449
bm25,paragraph,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.442
bm25,paragraph,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_001', 'paragraph_chunk_034', 'paragraph_chunk_033']",C,['paragraph_chunk_001'],False,True,False,0.51
bm25,paragraph,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",B,['paragraph_chunk_014'],False,True,True,0.596
bm25,paragraph,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_021']",C,"['paragraph_chunk_008', 'paragraph_chunk_021']",False,True,False,0.49
bm25,paragraph,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_001', 'paragraph_chunk_036']",B,"['paragraph_chunk_034', 'paragraph_chunk_001']",False,False,False,0.501
bm25,paragraph,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_021', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.567
bm25,paragraph,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_009', 'paragraph_chunk_034']",C,"['008', '009']",False,True,False,0.458
bm25,paragraph,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_001']",B,['paragraph_chunk_004'],False,True,True,0.499
bm25,paragraph,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_103', 'paragraph_chunk_139']",B,['paragraph_chunk_015'],False,True,True,0.508
bm25,paragraph,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.507
bm25,paragraph,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_139', 'paragraph_chunk_001']",C,"['001', '005']",False,True,False,0.424
bm25,paragraph,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_001', 'paragraph_chunk_005', 'paragraph_chunk_034']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.542
bm25,paragraph,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_005', 'paragraph_chunk_015']",C,['006'],False,True,False,0.541
bm25,paragraph,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_015', 'paragraph_chunk_008', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.526
bm25,paragraph,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_018', 'paragraph_chunk_010']",C,"['086', '010']",False,True,False,0.33
bm25,paragraph,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_086']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.445
bm25,paragraph,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.347
bm25,paragraph,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_006', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,"['paragraph_chunk_016', 'paragraph_chunk_008']",False,True,True,0.51
bm25,paragraph,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.457
bm25,paragraph,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_002']",C,['paragraph_chunk_029'],False,True,True,0.503
bm25,paragraph,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_033', 'paragraph_chunk_005']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.453
bm25,paragraph,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['paragraph_chunk_006', 'paragraph_chunk_005']",False,True,True,0.299
bm25,paragraph,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_034']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.449
bm25,paragraph,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_100', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.624
bm25,paragraph,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.348
bm25,paragraph,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_124', 'paragraph_chunk_015']",C,['015'],False,True,False,0.616
bm25,paragraph,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.517
bm25,paragraph,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.514
bm25,paragraph,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_097', 'paragraph_chunk_002', 'paragraph_chunk_003']",C,"['002', '003']",False,True,False,0.525
bm25,paragraph,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_014', 'paragraph_chunk_017']",C,['paragraph_chunk_017'],False,True,True,0.601
bm25,paragraph,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.366
bm25,paragraph,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_036', 'paragraph_chunk_001']",X,[],True,False,,0.499
bm25,paragraph,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",A,"['014', '021']",False,False,False,0.708
bm25,paragraph,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_009']",B,['paragraph_chunk_034'],False,True,True,0.556
bm25,paragraph,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.738
bm25,paragraph,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_006', 'paragraph_chunk_001']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.337
bm25,paragraph,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_015', 'paragraph_chunk_102']",C,['015'],False,True,False,0.468
bm25,paragraph,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_006', 'paragraph_chunk_024', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.456
bm25,paragraph,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_006']",B,['paragraph_chunk_086'],False,True,True,0.327
bm25,paragraph,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['029', '028']",False,True,False,0.348
bm25,paragraph,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.454
bm25,paragraph,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.461
bm25,paragraph,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_033']",C,['paragraph_chunk_006'],False,True,True,0.436
bm25,paragraph,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_026']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.596
bm25,paragraph,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_005', 'paragraph_chunk_001']",C,['paragraph_chunk_016'],False,True,True,0.496
bm25,paragraph,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_014', 'paragraph_chunk_033', 'paragraph_chunk_015']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.589
bm25,paragraph,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.32
bm25,paragraph,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_018']",C,['021'],False,True,False,0.449
bm25,paragraph,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_035', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.512
bm25,paragraph,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.488
bm25,paragraph,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_006', 'paragraph_chunk_003', 'paragraph_chunk_030']",C,['006'],False,True,False,0.326
bm25,paragraph,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['006', '005']",False,True,False,0.557
bm25,paragraph,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_086', 'paragraph_chunk_002', 'paragraph_chunk_034']",C,['paragraph_chunk_086'],False,True,True,0.504
bm25,paragraph,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_035', 'paragraph_chunk_034']",C,['paragraph_chunk_006'],False,True,True,0.378
bm25,paragraph,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_113']",C,['paragraph_chunk_014'],False,True,True,0.524
bm25,paragraph,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_087', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,['paragraph_chunk_016'],False,True,True,0.324
bm25,paragraph,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.686
bm25,paragraph,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.455
bm25,paragraph,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,['paragraph_chunk_018'],False,True,True,0.657
bm25,paragraph,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_099']",C,['paragraph_chunk_025'],False,True,True,0.463
bm25,paragraph,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_030', 'paragraph_chunk_006']",C,['paragraph_chunk_018'],False,True,True,0.396
bm25,paragraph,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_005', 'paragraph_chunk_006']",C,['paragraph_chunk_006'],False,True,True,0.328
bm25,paragraph,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_028', 'paragraph_chunk_087']",C,"['paragraph_chunk_086', 'paragraph_chunk_087']",False,True,True,0.862
bm25,paragraph,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_087', 'paragraph_chunk_018']",C,['paragraph_chunk_086'],False,True,True,0.585
bm25,paragraph,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,1.066
bm25,paragraph,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_021']",C,['paragraph_chunk_015'],False,True,True,0.356
bm25,paragraph,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_034']",C,['paragraph_chunk_016'],False,True,True,0.295
bm25,paragraph,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_029', 'paragraph_chunk_002']",C,['paragraph_chunk_018'],False,True,True,0.47
bm25,paragraph,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_021']",X,[],True,False,,0.401
bm25,paragraph,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_027']",C,"['paragraph_chunk_028', 'paragraph_chunk_027']",False,True,True,0.733
bm25,paragraph,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.582
bm25,paragraph,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_001', 'paragraph_chunk_034', 'paragraph_chunk_033']",C,['paragraph_chunk_001'],False,True,False,0.449
bm25,paragraph,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",B,['paragraph_chunk_014'],False,True,True,0.743
bm25,paragraph,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_021']",C,"['paragraph_chunk_008', 'paragraph_chunk_021']",False,True,False,0.808
bm25,paragraph,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_001', 'paragraph_chunk_036']",B,"['paragraph_chunk_034', 'paragraph_chunk_001']",False,False,False,0.541
bm25,paragraph,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_021', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.444
bm25,paragraph,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_009', 'paragraph_chunk_034']",C,"['008', '009']",False,True,False,0.698
bm25,paragraph,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_001']",B,['paragraph_chunk_004'],False,True,True,0.442
bm25,paragraph,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_103', 'paragraph_chunk_139']",B,['paragraph_chunk_015'],False,True,True,0.459
bm25,paragraph,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.46
bm25,paragraph,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_139', 'paragraph_chunk_001']",C,"['001', '005']",False,True,False,0.59
bm25,paragraph,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_001', 'paragraph_chunk_005', 'paragraph_chunk_034']",C,"['paragraph_chunk_001', 'paragraph_chunk_005']",False,True,True,0.463
bm25,paragraph,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_005', 'paragraph_chunk_015']",C,['006'],False,True,False,0.531
bm25,paragraph,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_015', 'paragraph_chunk_008', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.515
bm25,paragraph,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_018', 'paragraph_chunk_010']",C,"['086', '010']",False,True,False,0.504
bm25,paragraph,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_015', 'paragraph_chunk_086']",C,"['paragraph_chunk_014', 'paragraph_chunk_015']",False,True,True,0.51
bm25,paragraph,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.321
bm25,paragraph,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_006', 'paragraph_chunk_016', 'paragraph_chunk_008']",C,"['paragraph_chunk_016', 'paragraph_chunk_008']",False,True,True,0.507
bm25,paragraph,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.461
bm25,paragraph,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_002']",C,['paragraph_chunk_029'],False,True,True,0.323
bm25,paragraph,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_033', 'paragraph_chunk_005']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.292
bm25,paragraph,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_005']",C,"['paragraph_chunk_006', 'paragraph_chunk_005']",False,True,True,0.582
bm25,paragraph,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_034']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.455
bm25,paragraph,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_100', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.597
bm25,paragraph,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_021', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.284
bm25,paragraph,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_124', 'paragraph_chunk_015']",C,['015'],False,True,False,0.479
bm25,paragraph,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_006']",C,['paragraph_chunk_034'],False,True,True,0.5
bm25,paragraph,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,1.177
bm25,paragraph,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_097', 'paragraph_chunk_002', 'paragraph_chunk_003']",C,"['002', '003']",False,True,False,0.56
bm25,paragraph,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_014', 'paragraph_chunk_017']",C,['paragraph_chunk_017'],False,True,True,0.583
bm25,paragraph,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.647
dense,paragraph,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",D,['paragraph_chunk_008'],False,False,False,0.989
dense,paragraph,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_008']",C,['paragraph_chunk_021'],False,True,False,0.646
dense,paragraph,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_013']",X,[],True,False,,0.584
dense,paragraph,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_003']",D,['003'],False,True,False,0.626
dense,paragraph,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_008', 'paragraph_chunk_003', 'paragraph_chunk_114']",C,['paragraph_chunk_008'],False,True,False,0.396
dense,paragraph,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",C,['123'],False,True,False,0.459
dense,paragraph,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,True,0.622
dense,paragraph,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_025']",B,['paragraph_chunk_010'],False,True,True,0.532
dense,paragraph,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.592
dense,paragraph,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.635
dense,paragraph,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_005', 'paragraph_chunk_036']",C,"['paragraph_chunk_005', 'paragraph_chunk_036']",False,True,True,1.024
dense,paragraph,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_015', 'paragraph_chunk_061']",C,['paragraph_chunk_015'],False,True,False,0.696
dense,paragraph,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.633
dense,paragraph,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_091', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,"['091', '007']",False,True,False,0.53
dense,paragraph,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.526
dense,paragraph,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_102']",C,"['paragraph_chunk_015', 'paragraph_chunk_102']",False,True,True,0.626
dense,paragraph,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_024', 'paragraph_chunk_022']",C,['paragraph_chunk_024'],False,True,True,0.51
dense,paragraph,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_012', 'paragraph_chunk_012']",X,[],True,False,,0.578
dense,paragraph,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.61
dense,paragraph,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_007', 'paragraph_chunk_083', 'paragraph_chunk_003']",C,['007'],False,True,False,0.359
dense,paragraph,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_009', 'paragraph_chunk_014']",C,"['009', '014']",False,True,False,0.397
dense,paragraph,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.513
dense,paragraph,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_022', 'paragraph_chunk_024', 'paragraph_chunk_109']",C,"['paragraph_chunk_022', 'paragraph_chunk_024']",False,True,False,0.752
dense,paragraph,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_062']",C,['paragraph_chunk_026'],False,True,False,0.351
dense,paragraph,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_091']",C,['paragraph_chunk_016'],False,True,True,0.609
dense,paragraph,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,['paragraph_chunk_010'],False,True,True,0.618
dense,paragraph,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_129']",C,['paragraph_chunk_029'],False,True,True,0.65
dense,paragraph,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_091']",C,['paragraph_chunk_018'],False,True,True,0.787
dense,paragraph,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_109', 'paragraph_chunk_018']",C,['paragraph_chunk_025'],False,True,True,0.331
dense,paragraph,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_003']",C,"['007', '003']",False,True,False,0.606
dense,paragraph,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_026']",B,['008'],False,False,False,0.595
dense,paragraph,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_010'],False,True,True,0.619
dense,paragraph,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_024', 'paragraph_chunk_013']",C,"['paragraph_chunk_010', 'paragraph_chunk_013']",False,True,True,2.494
dense,paragraph,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.515
dense,paragraph,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_123', 'paragraph_chunk_102']",C,['paragraph_chunk_015'],False,True,True,0.497
dense,paragraph,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",X,[],True,False,,0.337
dense,paragraph,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_022', 'paragraph_chunk_012', 'paragraph_chunk_011']",B,['paragraph_chunk_011'],False,False,True,0.389
dense,paragraph,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_126', 'paragraph_chunk_014', 'paragraph_chunk_026']",X,[],True,False,,0.618
dense,paragraph,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_101', 'paragraph_chunk_005']",C,['paragraph_chunk_028'],False,True,True,0.503
dense,paragraph,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.664
dense,paragraph,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,False,0.508
dense,paragraph,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_025']",B,['021'],False,True,False,0.454
dense,paragraph,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,True,0.737
dense,paragraph,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_023', 'paragraph_chunk_008', 'paragraph_chunk_013']",C,['008'],False,True,False,0.643
dense,paragraph,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,['paragraph_chunk_024'],False,True,True,0.722
dense,paragraph,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.345
dense,paragraph,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_105', 'paragraph_chunk_059']",B,['paragraph_chunk_004'],False,True,True,0.664
dense,paragraph,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",B,['123'],False,True,False,0.345
dense,paragraph,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.534
dense,paragraph,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.465
dense,paragraph,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_107', 'paragraph_chunk_001']",C,"['001', '107']",False,True,True,0.569
dense,paragraph,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['paragraph_chunk_006'],False,True,True,0.629
dense,paragraph,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_007']",C,['paragraph_chunk_007'],False,True,True,0.641
dense,paragraph,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.626
dense,paragraph,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_092', 'paragraph_chunk_113', 'paragraph_chunk_026']",D,"['092', '113', '026']",False,False,False,0.668
dense,paragraph,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['paragraph_chunk_123'],False,True,False,0.653
dense,paragraph,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_088', 'paragraph_chunk_088']",B,['paragraph_chunk_088'],False,False,False,0.754
dense,paragraph,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_117']",C,"['paragraph_chunk_021', 'paragraph_chunk_018']",False,True,True,0.573
dense,paragraph,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_129']",C,['029'],False,True,False,0.599
dense,paragraph,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.391
dense,paragraph,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_016']",C,['007'],False,True,False,0.467
dense,paragraph,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_009', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.442
dense,paragraph,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_019', 'paragraph_chunk_091', 'paragraph_chunk_099']",C,['099'],False,True,False,0.589
dense,paragraph,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.457
dense,paragraph,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['123'],False,True,False,0.568
dense,paragraph,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.632
dense,paragraph,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.504
dense,paragraph,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_003', 'paragraph_chunk_003']",X,[],True,False,,0.671
dense,paragraph,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_051', 'paragraph_chunk_004', 'paragraph_chunk_060']",X,[],True,False,,0.576
dense,paragraph,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_003', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.66
dense,paragraph,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",D,['paragraph_chunk_008'],False,False,False,0.552
dense,paragraph,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_008']",C,['paragraph_chunk_021'],False,True,False,0.63
dense,paragraph,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_013']",X,[],True,False,,0.416
dense,paragraph,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_003']",D,['003'],False,True,False,0.568
dense,paragraph,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_008', 'paragraph_chunk_003', 'paragraph_chunk_114']",C,['paragraph_chunk_008'],False,True,False,2.843
dense,paragraph,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",C,['123'],False,True,False,0.493
dense,paragraph,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,True,0.656
dense,paragraph,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_025']",B,['paragraph_chunk_010'],False,True,True,0.401
dense,paragraph,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.356
dense,paragraph,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.633
dense,paragraph,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_005', 'paragraph_chunk_036']",C,"['paragraph_chunk_005', 'paragraph_chunk_036']",False,True,True,0.563
dense,paragraph,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_015', 'paragraph_chunk_061']",C,['paragraph_chunk_015'],False,True,False,0.619
dense,paragraph,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.391
dense,paragraph,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_091', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,"['091', '007']",False,True,False,0.662
dense,paragraph,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.602
dense,paragraph,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_102']",C,"['paragraph_chunk_015', 'paragraph_chunk_102']",False,True,True,0.559
dense,paragraph,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_024', 'paragraph_chunk_022']",C,['paragraph_chunk_024'],False,True,True,0.646
dense,paragraph,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_012', 'paragraph_chunk_012']",X,[],True,False,,0.513
dense,paragraph,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.504
dense,paragraph,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_007', 'paragraph_chunk_083', 'paragraph_chunk_003']",C,['007'],False,True,False,0.627
dense,paragraph,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_009', 'paragraph_chunk_014']",C,"['009', '014']",False,True,False,0.703
dense,paragraph,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.579
dense,paragraph,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_022', 'paragraph_chunk_024', 'paragraph_chunk_109']",C,"['paragraph_chunk_022', 'paragraph_chunk_024']",False,True,False,0.599
dense,paragraph,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_062']",C,['paragraph_chunk_026'],False,True,False,0.62
dense,paragraph,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_091']",C,['paragraph_chunk_016'],False,True,True,0.608
dense,paragraph,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,['paragraph_chunk_010'],False,True,True,0.704
dense,paragraph,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_129']",C,['paragraph_chunk_029'],False,True,True,0.674
dense,paragraph,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_091']",C,['paragraph_chunk_018'],False,True,True,0.556
dense,paragraph,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_109', 'paragraph_chunk_018']",C,['paragraph_chunk_025'],False,True,True,0.341
dense,paragraph,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_003']",C,"['007', '003']",False,True,False,0.872
dense,paragraph,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_026']",B,['008'],False,False,False,0.393
dense,paragraph,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_010'],False,True,True,0.563
dense,paragraph,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_024', 'paragraph_chunk_013']",C,"['paragraph_chunk_010', 'paragraph_chunk_013']",False,True,True,0.767
dense,paragraph,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.702
dense,paragraph,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_123', 'paragraph_chunk_102']",C,['paragraph_chunk_015'],False,True,True,0.596
dense,paragraph,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",X,[],True,False,,0.515
dense,paragraph,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_022', 'paragraph_chunk_012', 'paragraph_chunk_011']",B,['paragraph_chunk_011'],False,False,True,0.593
dense,paragraph,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_126', 'paragraph_chunk_014', 'paragraph_chunk_026']",X,[],True,False,,0.391
dense,paragraph,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_101', 'paragraph_chunk_005']",C,['paragraph_chunk_028'],False,True,True,0.621
dense,paragraph,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.559
dense,paragraph,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,False,0.402
dense,paragraph,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_025']",B,['021'],False,True,False,0.342
dense,paragraph,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,True,0.561
dense,paragraph,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_023', 'paragraph_chunk_008', 'paragraph_chunk_013']",C,['008'],False,True,False,0.745
dense,paragraph,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,['paragraph_chunk_024'],False,True,True,0.608
dense,paragraph,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.63
dense,paragraph,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_105', 'paragraph_chunk_059']",B,['paragraph_chunk_004'],False,True,True,0.339
dense,paragraph,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",B,['123'],False,True,False,0.493
dense,paragraph,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.551
dense,paragraph,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.512
dense,paragraph,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_107', 'paragraph_chunk_001']",C,"['001', '107']",False,True,True,0.562
dense,paragraph,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['paragraph_chunk_006'],False,True,True,0.577
dense,paragraph,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_007']",C,['paragraph_chunk_007'],False,True,True,0.369
dense,paragraph,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.665
dense,paragraph,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_092', 'paragraph_chunk_113', 'paragraph_chunk_026']",D,"['092', '113', '026']",False,False,False,0.748
dense,paragraph,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['paragraph_chunk_123'],False,True,False,0.611
dense,paragraph,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_088', 'paragraph_chunk_088']",B,['paragraph_chunk_088'],False,False,False,0.57
dense,paragraph,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_117']",C,"['paragraph_chunk_021', 'paragraph_chunk_018']",False,True,True,0.519
dense,paragraph,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_129']",C,['029'],False,True,False,0.577
dense,paragraph,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.569
dense,paragraph,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_016']",C,['007'],False,True,False,0.718
dense,paragraph,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_009', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.564
dense,paragraph,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_019', 'paragraph_chunk_091', 'paragraph_chunk_099']",C,['099'],False,True,False,0.503
dense,paragraph,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.532
dense,paragraph,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['123'],False,True,False,1.225
dense,paragraph,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.502
dense,paragraph,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.675
dense,paragraph,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_003', 'paragraph_chunk_003']",X,[],True,False,,0.413
dense,paragraph,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_051', 'paragraph_chunk_004', 'paragraph_chunk_060']",X,[],True,False,,0.557
dense,paragraph,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_003', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.761
dense,paragraph,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",D,['paragraph_chunk_008'],False,False,False,0.587
dense,paragraph,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_008']",C,['paragraph_chunk_021'],False,True,False,0.336
dense,paragraph,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_013']",X,[],True,False,,0.555
dense,paragraph,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_003']",D,['003'],False,True,False,0.655
dense,paragraph,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_008', 'paragraph_chunk_003', 'paragraph_chunk_114']",C,['paragraph_chunk_008'],False,True,False,0.666
dense,paragraph,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",C,['123'],False,True,False,0.664
dense,paragraph,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,True,0.504
dense,paragraph,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_025']",B,['paragraph_chunk_010'],False,True,True,0.408
dense,paragraph,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.547
dense,paragraph,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.457
dense,paragraph,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_005', 'paragraph_chunk_036']",C,"['paragraph_chunk_005', 'paragraph_chunk_036']",False,True,True,0.72
dense,paragraph,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_015', 'paragraph_chunk_061']",C,['paragraph_chunk_015'],False,True,False,0.655
dense,paragraph,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.671
dense,paragraph,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_091', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,"['091', '007']",False,True,False,0.616
dense,paragraph,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.565
dense,paragraph,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_102']",C,"['paragraph_chunk_015', 'paragraph_chunk_102']",False,True,True,0.598
dense,paragraph,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_024', 'paragraph_chunk_022']",C,['paragraph_chunk_024'],False,True,True,0.339
dense,paragraph,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_012', 'paragraph_chunk_012']",X,[],True,False,,0.622
dense,paragraph,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.6
dense,paragraph,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_007', 'paragraph_chunk_083', 'paragraph_chunk_003']",C,['007'],False,True,False,0.51
dense,paragraph,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_009', 'paragraph_chunk_014']",C,"['009', '014']",False,True,False,0.641
dense,paragraph,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.655
dense,paragraph,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_022', 'paragraph_chunk_024', 'paragraph_chunk_109']",C,"['paragraph_chunk_022', 'paragraph_chunk_024']",False,True,False,0.602
dense,paragraph,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_062']",C,['paragraph_chunk_026'],False,True,False,0.745
dense,paragraph,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_091']",C,['paragraph_chunk_016'],False,True,True,0.62
dense,paragraph,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,['paragraph_chunk_010'],False,True,True,0.625
dense,paragraph,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_129']",C,['paragraph_chunk_029'],False,True,True,0.63
dense,paragraph,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_091']",C,['paragraph_chunk_018'],False,True,True,0.351
dense,paragraph,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_109', 'paragraph_chunk_018']",C,['paragraph_chunk_025'],False,True,True,0.592
dense,paragraph,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_003']",C,"['007', '003']",False,True,False,0.606
dense,paragraph,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_026']",B,['008'],False,False,False,0.344
dense,paragraph,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_010'],False,True,True,0.393
dense,paragraph,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_024', 'paragraph_chunk_013']",C,"['paragraph_chunk_010', 'paragraph_chunk_013']",False,True,True,0.526
dense,paragraph,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.598
dense,paragraph,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_123', 'paragraph_chunk_102']",C,['paragraph_chunk_015'],False,True,True,0.557
dense,paragraph,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",X,[],True,False,,0.337
dense,paragraph,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_022', 'paragraph_chunk_012', 'paragraph_chunk_011']",B,['paragraph_chunk_011'],False,False,True,0.557
dense,paragraph,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_126', 'paragraph_chunk_014', 'paragraph_chunk_026']",X,[],True,False,,0.481
dense,paragraph,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_101', 'paragraph_chunk_005']",C,['paragraph_chunk_028'],False,True,True,0.419
dense,paragraph,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.62
dense,paragraph,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,False,0.57
dense,paragraph,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_025']",B,['021'],False,True,False,0.481
dense,paragraph,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,True,1.096
dense,paragraph,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_023', 'paragraph_chunk_008', 'paragraph_chunk_013']",C,['008'],False,True,False,0.514
dense,paragraph,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,['paragraph_chunk_024'],False,True,True,0.384
dense,paragraph,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.569
dense,paragraph,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_105', 'paragraph_chunk_059']",B,['paragraph_chunk_004'],False,True,True,0.598
dense,paragraph,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",B,['123'],False,True,False,0.663
dense,paragraph,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.513
dense,paragraph,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.62
dense,paragraph,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_107', 'paragraph_chunk_001']",C,"['001', '107']",False,True,True,0.793
dense,paragraph,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['paragraph_chunk_006'],False,True,True,0.634
dense,paragraph,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_007']",C,['paragraph_chunk_007'],False,True,True,0.596
dense,paragraph,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.528
dense,paragraph,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_092', 'paragraph_chunk_113', 'paragraph_chunk_026']",D,"['092', '113', '026']",False,False,False,0.554
dense,paragraph,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['paragraph_chunk_123'],False,True,False,0.619
dense,paragraph,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_088', 'paragraph_chunk_088']",B,['paragraph_chunk_088'],False,False,False,0.587
dense,paragraph,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_117']",C,"['paragraph_chunk_021', 'paragraph_chunk_018']",False,True,True,0.548
dense,paragraph,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_129']",C,['029'],False,True,False,0.517
dense,paragraph,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.534
dense,paragraph,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_016']",C,['007'],False,True,False,0.523
dense,paragraph,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_009', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.51
dense,paragraph,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_019', 'paragraph_chunk_091', 'paragraph_chunk_099']",C,['099'],False,True,False,0.754
dense,paragraph,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.354
dense,paragraph,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['123'],False,True,False,0.566
dense,paragraph,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.439
dense,paragraph,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.659
dense,paragraph,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_003', 'paragraph_chunk_003']",X,[],True,False,,0.339
dense,paragraph,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_051', 'paragraph_chunk_004', 'paragraph_chunk_060']",X,[],True,False,,0.659
dense,paragraph,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_003', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.624
dense,paragraph,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",D,['paragraph_chunk_008'],False,False,False,0.449
dense,paragraph,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_008']",C,['paragraph_chunk_021'],False,True,False,0.528
dense,paragraph,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_013']",X,[],True,False,,0.632
dense,paragraph,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_003']",D,['003'],False,True,False,0.532
dense,paragraph,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_008', 'paragraph_chunk_003', 'paragraph_chunk_114']",C,['paragraph_chunk_008'],False,True,False,0.493
dense,paragraph,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",C,['123'],False,True,False,0.357
dense,paragraph,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,True,0.467
dense,paragraph,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_025']",B,['paragraph_chunk_010'],False,True,True,0.572
dense,paragraph,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.499
dense,paragraph,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.698
dense,paragraph,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_005', 'paragraph_chunk_036']",C,"['paragraph_chunk_005', 'paragraph_chunk_036']",False,True,True,0.692
dense,paragraph,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_015', 'paragraph_chunk_061']",C,['paragraph_chunk_015'],False,True,False,0.592
dense,paragraph,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.617
dense,paragraph,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_091', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,"['091', '007']",False,True,False,0.575
dense,paragraph,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.512
dense,paragraph,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_102']",C,"['paragraph_chunk_015', 'paragraph_chunk_102']",False,True,True,0.588
dense,paragraph,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_024', 'paragraph_chunk_022']",C,['paragraph_chunk_024'],False,True,True,0.522
dense,paragraph,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_012', 'paragraph_chunk_012']",X,[],True,False,,0.362
dense,paragraph,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.622
dense,paragraph,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_007', 'paragraph_chunk_083', 'paragraph_chunk_003']",C,['007'],False,True,False,0.452
dense,paragraph,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_009', 'paragraph_chunk_014']",C,"['009', '014']",False,True,False,0.698
dense,paragraph,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.445
dense,paragraph,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_022', 'paragraph_chunk_024', 'paragraph_chunk_109']",C,"['paragraph_chunk_022', 'paragraph_chunk_024']",False,True,False,0.344
dense,paragraph,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_062']",C,['paragraph_chunk_026'],False,True,False,0.614
dense,paragraph,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_091']",C,['paragraph_chunk_016'],False,True,True,0.84
dense,paragraph,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,['paragraph_chunk_010'],False,True,True,0.609
dense,paragraph,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_129']",C,['paragraph_chunk_029'],False,True,True,0.574
dense,paragraph,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_091']",C,['paragraph_chunk_018'],False,True,True,0.565
dense,paragraph,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_109', 'paragraph_chunk_018']",C,['paragraph_chunk_025'],False,True,True,0.561
dense,paragraph,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_003']",C,"['007', '003']",False,True,False,0.692
dense,paragraph,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_026']",B,['008'],False,False,False,0.533
dense,paragraph,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_010'],False,True,True,0.609
dense,paragraph,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_024', 'paragraph_chunk_013']",C,"['paragraph_chunk_010', 'paragraph_chunk_013']",False,True,True,0.512
dense,paragraph,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.683
dense,paragraph,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_123', 'paragraph_chunk_102']",C,['paragraph_chunk_015'],False,True,True,0.505
dense,paragraph,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",X,[],True,False,,0.501
dense,paragraph,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_022', 'paragraph_chunk_012', 'paragraph_chunk_011']",B,['paragraph_chunk_011'],False,False,True,0.57
dense,paragraph,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_126', 'paragraph_chunk_014', 'paragraph_chunk_026']",X,[],True,False,,0.507
dense,paragraph,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_101', 'paragraph_chunk_005']",C,['paragraph_chunk_028'],False,True,True,0.542
dense,paragraph,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.733
dense,paragraph,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,False,0.696
dense,paragraph,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_025']",B,['021'],False,True,False,0.613
dense,paragraph,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,True,0.562
dense,paragraph,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_023', 'paragraph_chunk_008', 'paragraph_chunk_013']",C,['008'],False,True,False,0.407
dense,paragraph,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,['paragraph_chunk_024'],False,True,True,0.603
dense,paragraph,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.543
dense,paragraph,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_105', 'paragraph_chunk_059']",B,['paragraph_chunk_004'],False,True,True,0.629
dense,paragraph,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",B,['123'],False,True,False,0.46
dense,paragraph,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.492
dense,paragraph,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.364
dense,paragraph,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_107', 'paragraph_chunk_001']",C,"['001', '107']",False,True,True,0.7
dense,paragraph,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['paragraph_chunk_006'],False,True,True,0.512
dense,paragraph,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_007']",C,['paragraph_chunk_007'],False,True,True,0.632
dense,paragraph,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.575
dense,paragraph,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_092', 'paragraph_chunk_113', 'paragraph_chunk_026']",D,"['092', '113', '026']",False,False,False,0.581
dense,paragraph,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['paragraph_chunk_123'],False,True,False,0.613
dense,paragraph,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_088', 'paragraph_chunk_088']",B,['paragraph_chunk_088'],False,False,False,0.365
dense,paragraph,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_117']",C,"['paragraph_chunk_021', 'paragraph_chunk_018']",False,True,True,0.567
dense,paragraph,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_129']",C,['029'],False,True,False,0.68
dense,paragraph,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.362
dense,paragraph,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_016']",C,['007'],False,True,False,0.467
dense,paragraph,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_009', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.628
dense,paragraph,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_019', 'paragraph_chunk_091', 'paragraph_chunk_099']",C,['099'],False,True,False,0.567
dense,paragraph,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.613
dense,paragraph,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['123'],False,True,False,0.391
dense,paragraph,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.605
dense,paragraph,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.517
dense,paragraph,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_003', 'paragraph_chunk_003']",X,[],True,False,,0.518
dense,paragraph,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_051', 'paragraph_chunk_004', 'paragraph_chunk_060']",X,[],True,False,,0.588
dense,paragraph,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_003', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.611
dense,paragraph,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",D,['paragraph_chunk_008'],False,False,False,0.503
dense,paragraph,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_008']",C,['paragraph_chunk_021'],False,True,False,0.356
dense,paragraph,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_013']",X,[],True,False,,0.517
dense,paragraph,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_003']",D,['003'],False,True,False,0.657
dense,paragraph,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_008', 'paragraph_chunk_003', 'paragraph_chunk_114']",C,['paragraph_chunk_008'],False,True,False,0.517
dense,paragraph,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",C,['123'],False,True,False,0.932
dense,paragraph,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,True,0.502
dense,paragraph,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_025']",B,['paragraph_chunk_010'],False,True,True,0.349
dense,paragraph,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.85
dense,paragraph,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_025'],False,True,True,0.497
dense,paragraph,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_005', 'paragraph_chunk_036']",C,"['paragraph_chunk_005', 'paragraph_chunk_036']",False,True,True,0.555
dense,paragraph,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_015', 'paragraph_chunk_061']",C,['paragraph_chunk_015'],False,True,False,0.724
dense,paragraph,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.676
dense,paragraph,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_091', 'paragraph_chunk_007', 'paragraph_chunk_034']",C,"['091', '007']",False,True,False,0.615
dense,paragraph,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.609
dense,paragraph,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_102']",C,"['paragraph_chunk_015', 'paragraph_chunk_102']",False,True,True,0.596
dense,paragraph,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_024', 'paragraph_chunk_022']",C,['paragraph_chunk_024'],False,True,True,0.718
dense,paragraph,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_012', 'paragraph_chunk_012']",X,[],True,False,,0.672
dense,paragraph,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.615
dense,paragraph,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_007', 'paragraph_chunk_083', 'paragraph_chunk_003']",C,['007'],False,True,False,0.634
dense,paragraph,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_009', 'paragraph_chunk_014']",C,"['009', '014']",False,True,False,0.5
dense,paragraph,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.363
dense,paragraph,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_022', 'paragraph_chunk_024', 'paragraph_chunk_109']",C,"['paragraph_chunk_022', 'paragraph_chunk_024']",False,True,False,0.563
dense,paragraph,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_092', 'paragraph_chunk_026', 'paragraph_chunk_062']",C,['paragraph_chunk_026'],False,True,False,0.52
dense,paragraph,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_021', 'paragraph_chunk_091']",C,['paragraph_chunk_016'],False,True,True,0.609
dense,paragraph,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,['paragraph_chunk_010'],False,True,True,0.515
dense,paragraph,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_129']",C,['paragraph_chunk_029'],False,True,True,0.52
dense,paragraph,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_091']",C,['paragraph_chunk_018'],False,True,True,0.795
dense,paragraph,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_109', 'paragraph_chunk_018']",C,['paragraph_chunk_025'],False,True,True,0.564
dense,paragraph,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_003']",C,"['007', '003']",False,True,False,0.512
dense,paragraph,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_003', 'paragraph_chunk_008', 'paragraph_chunk_026']",B,['008'],False,False,False,0.492
dense,paragraph,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_025', 'paragraph_chunk_024']",C,['paragraph_chunk_010'],False,True,True,0.507
dense,paragraph,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_024', 'paragraph_chunk_013']",C,"['paragraph_chunk_010', 'paragraph_chunk_013']",False,True,True,0.61
dense,paragraph,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.612
dense,paragraph,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_123', 'paragraph_chunk_102']",C,['paragraph_chunk_015'],False,True,True,0.773
dense,paragraph,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_008']",X,[],True,False,,0.467
dense,paragraph,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_022', 'paragraph_chunk_012', 'paragraph_chunk_011']",B,['paragraph_chunk_011'],False,False,True,0.391
dense,paragraph,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_126', 'paragraph_chunk_014', 'paragraph_chunk_026']",X,[],True,False,,0.538
dense,paragraph,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_101', 'paragraph_chunk_005']",C,['paragraph_chunk_028'],False,True,True,0.57
dense,paragraph,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.562
dense,paragraph,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_008']",C,['paragraph_chunk_008'],False,True,False,0.474
dense,paragraph,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_025']",B,['021'],False,True,False,0.623
dense,paragraph,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,True,0.504
dense,paragraph,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_023', 'paragraph_chunk_008', 'paragraph_chunk_013']",C,['008'],False,True,False,0.51
dense,paragraph,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_024', 'paragraph_chunk_025']",C,['paragraph_chunk_024'],False,True,True,0.68
dense,paragraph,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.613
dense,paragraph,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_105', 'paragraph_chunk_059']",B,['paragraph_chunk_004'],False,True,True,0.578
dense,paragraph,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_025', 'paragraph_chunk_008', 'paragraph_chunk_123']",B,['123'],False,True,False,0.829
dense,paragraph,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_023', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.507
dense,paragraph,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_013', 'paragraph_chunk_008', 'paragraph_chunk_003']",C,['paragraph_chunk_008'],False,True,False,0.514
dense,paragraph,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_107', 'paragraph_chunk_001']",C,"['001', '107']",False,True,True,0.506
dense,paragraph,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['paragraph_chunk_006'],False,True,True,0.552
dense,paragraph,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_008', 'paragraph_chunk_007']",C,['paragraph_chunk_007'],False,True,True,0.567
dense,paragraph,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_087', 'paragraph_chunk_103']",C,['paragraph_chunk_010'],False,True,True,0.412
dense,paragraph,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_092', 'paragraph_chunk_113', 'paragraph_chunk_026']",D,"['092', '113', '026']",False,False,False,0.51
dense,paragraph,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['paragraph_chunk_123'],False,True,False,0.547
dense,paragraph,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_088', 'paragraph_chunk_088']",B,['paragraph_chunk_088'],False,False,False,0.503
dense,paragraph,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_021', 'paragraph_chunk_018', 'paragraph_chunk_117']",C,"['paragraph_chunk_021', 'paragraph_chunk_018']",False,True,True,0.542
dense,paragraph,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_129']",C,['029'],False,True,False,0.666
dense,paragraph,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.495
dense,paragraph,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_007', 'paragraph_chunk_016']",C,['007'],False,True,False,0.58
dense,paragraph,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_009', 'paragraph_chunk_010']",C,['paragraph_chunk_010'],False,True,True,0.613
dense,paragraph,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_019', 'paragraph_chunk_091', 'paragraph_chunk_099']",C,['099'],False,True,False,0.537
dense,paragraph,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_021', 'paragraph_chunk_124']",X,[],True,False,,0.607
dense,paragraph,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_123']",C,['123'],False,True,False,0.72
dense,paragraph,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_002']",C,['paragraph_chunk_034'],False,True,True,0.346
dense,paragraph,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_029', 'paragraph_chunk_029']",C,['paragraph_chunk_029'],False,True,True,0.508
dense,paragraph,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_003', 'paragraph_chunk_003']",X,[],True,False,,0.514
dense,paragraph,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_051', 'paragraph_chunk_004', 'paragraph_chunk_060']",X,[],True,False,,0.561
dense,paragraph,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_003', 'paragraph_chunk_018', 'paragraph_chunk_024']",C,['paragraph_chunk_018'],False,True,True,0.994
hybrid,paragraph,1,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_013', 'paragraph_chunk_036']",X,[],True,False,,0.609
hybrid,paragraph,1,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_025', 'paragraph_chunk_014']",A,"['014', '021']",False,False,False,0.602
hybrid,paragraph,1,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_035']",B,['paragraph_chunk_034'],False,True,True,0.562
hybrid,paragraph,1,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.586
hybrid,paragraph,1,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_008', 'paragraph_chunk_006']",C,['paragraph_chunk_005'],False,True,True,0.635
hybrid,paragraph,1,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_025', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.567
hybrid,paragraph,1,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['006'],False,True,False,0.524
hybrid,paragraph,1,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",B,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.744
hybrid,paragraph,1,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,1.054
hybrid,paragraph,1,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_025'],False,True,True,0.827
hybrid,paragraph,1,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.636
hybrid,paragraph,1,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.51
hybrid,paragraph,1,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.563
hybrid,paragraph,1,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_091', 'paragraph_chunk_005']",C,['paragraph_chunk_016'],False,True,True,0.509
hybrid,paragraph,1,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_092']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.418
hybrid,paragraph,1,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.341
hybrid,paragraph,1,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.583
hybrid,paragraph,1,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_035']",C,['034'],False,True,False,0.57
hybrid,paragraph,1,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.397
hybrid,paragraph,1,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_003', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,"['006', '007']",False,True,False,0.845
hybrid,paragraph,1,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_034', 'paragraph_chunk_006']",C,['chunk_006'],False,True,False,0.554
hybrid,paragraph,1,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_002']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.571
hybrid,paragraph,1,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_022', 'paragraph_chunk_035']",C,['paragraph_chunk_006'],False,True,True,0.609
hybrid,paragraph,1,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.388
hybrid,paragraph,1,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_087', 'paragraph_chunk_021']",C,"['paragraph_chunk_016', 'paragraph_chunk_087']",False,True,True,0.545
hybrid,paragraph,1,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.625
hybrid,paragraph,1,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_028']",C,['paragraph_chunk_029'],False,True,True,0.572
hybrid,paragraph,1,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_007']",C,['paragraph_chunk_018'],False,True,True,0.714
hybrid,paragraph,1,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_109']",C,['paragraph_chunk_025'],False,True,True,0.391
hybrid,paragraph,1,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_030']",C,['paragraph_chunk_018'],False,True,True,0.532
hybrid,paragraph,1,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_005']",C,['paragraph_chunk_005'],False,True,False,0.561
hybrid,paragraph,1,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_028']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.685
hybrid,paragraph,1,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.622
hybrid,paragraph,1,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.57
hybrid,paragraph,1,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_123']",C,['paragraph_chunk_015'],False,True,True,0.508
hybrid,paragraph,1,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_016', 'paragraph_chunk_021']",C,['paragraph_chunk_016'],False,True,True,0.578
hybrid,paragraph,1,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_022', 'paragraph_chunk_029']",C,['paragraph_chunk_018'],False,True,True,0.606
hybrid,paragraph,1,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_126']",X,[],True,False,,0.556
hybrid,paragraph,1,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_101']",C,['028'],False,True,False,0.569
hybrid,paragraph,1,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.564
hybrid,paragraph,1,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,"['008', '001']",False,True,False,0.564
hybrid,paragraph,1,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",B,['014'],False,True,False,0.564
hybrid,paragraph,1,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_013']",C,['paragraph_chunk_008'],False,True,True,1.002
hybrid,paragraph,1,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_023', 'paragraph_chunk_001']",B,['034'],False,False,False,0.513
hybrid,paragraph,1,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_025', 'paragraph_chunk_021']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.798
hybrid,paragraph,1,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_009']",D,"['paragraph_chunk_008', 'paragraph_chunk_009']",False,False,False,0.393
hybrid,paragraph,1,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_105']",B,['paragraph_chunk_004'],False,True,True,0.626
hybrid,paragraph,1,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_103']",B,['paragraph_chunk_015'],False,True,True,0.546
hybrid,paragraph,1,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.57
hybrid,paragraph,1,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_013', 'paragraph_chunk_139']",C,['paragraph_chunk_005'],False,True,False,0.759
hybrid,paragraph,1,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_001', 'paragraph_chunk_005']",C,"['001', '005', '107']",False,True,True,0.708
hybrid,paragraph,1,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_034', 'paragraph_chunk_005']",C,['006'],False,True,False,0.514
hybrid,paragraph,1,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_015', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.525
hybrid,paragraph,1,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_018']",C,"['010', '086']",False,True,False,0.671
hybrid,paragraph,1,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.555
hybrid,paragraph,1,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.354
hybrid,paragraph,1,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['016'],False,True,False,0.482
hybrid,paragraph,1,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.703
hybrid,paragraph,1,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.515
hybrid,paragraph,1,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_033']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.561
hybrid,paragraph,1,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,['paragraph_chunk_006'],False,True,True,0.477
hybrid,paragraph,1,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_010', 'paragraph_chunk_086']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.515
hybrid,paragraph,1,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_019', 'paragraph_chunk_100']",C,['paragraph_chunk_018'],False,True,True,0.778
hybrid,paragraph,1,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.942
hybrid,paragraph,1,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_025', 'paragraph_chunk_124']",C,['paragraph_chunk_020'],False,True,False,0.527
hybrid,paragraph,1,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_012']",C,['paragraph_chunk_034'],False,True,True,0.783
hybrid,paragraph,1,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,1.002
hybrid,paragraph,1,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_097', 'paragraph_chunk_002']",C,"['002', '003']",False,True,False,0.465
hybrid,paragraph,1,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_051', 'paragraph_chunk_014']",X,[],True,False,,0.556
hybrid,paragraph,1,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.514
hybrid,paragraph,2,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_013', 'paragraph_chunk_036']",X,[],True,False,,0.611
hybrid,paragraph,2,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_025', 'paragraph_chunk_014']",A,"['014', '021']",False,False,False,1.065
hybrid,paragraph,2,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_035']",B,['paragraph_chunk_034'],False,True,True,0.347
hybrid,paragraph,2,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,1.058
hybrid,paragraph,2,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_008', 'paragraph_chunk_006']",C,['paragraph_chunk_005'],False,True,True,0.5
hybrid,paragraph,2,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_025', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.388
hybrid,paragraph,2,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['006'],False,True,False,0.967
hybrid,paragraph,2,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",B,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.564
hybrid,paragraph,2,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.623
hybrid,paragraph,2,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_025'],False,True,True,0.44
hybrid,paragraph,2,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.6
hybrid,paragraph,2,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.624
hybrid,paragraph,2,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.678
hybrid,paragraph,2,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_091', 'paragraph_chunk_005']",C,['paragraph_chunk_016'],False,True,True,0.624
hybrid,paragraph,2,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_092']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.652
hybrid,paragraph,2,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.348
hybrid,paragraph,2,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.596
hybrid,paragraph,2,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_035']",C,['034'],False,True,False,0.453
hybrid,paragraph,2,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.405
hybrid,paragraph,2,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_003', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,"['006', '007']",False,True,False,0.342
hybrid,paragraph,2,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_034', 'paragraph_chunk_006']",C,['chunk_006'],False,True,False,0.542
hybrid,paragraph,2,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_002']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.599
hybrid,paragraph,2,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_022', 'paragraph_chunk_035']",C,['paragraph_chunk_006'],False,True,True,0.734
hybrid,paragraph,2,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.565
hybrid,paragraph,2,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_087', 'paragraph_chunk_021']",C,"['paragraph_chunk_016', 'paragraph_chunk_087']",False,True,True,0.328
hybrid,paragraph,2,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.621
hybrid,paragraph,2,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_028']",C,['paragraph_chunk_029'],False,True,True,0.556
hybrid,paragraph,2,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_007']",C,['paragraph_chunk_018'],False,True,True,0.388
hybrid,paragraph,2,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_109']",C,['paragraph_chunk_025'],False,True,True,0.362
hybrid,paragraph,2,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_030']",C,['paragraph_chunk_018'],False,True,True,0.601
hybrid,paragraph,2,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_005']",C,['paragraph_chunk_005'],False,True,False,0.608
hybrid,paragraph,2,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_028']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.616
hybrid,paragraph,2,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.576
hybrid,paragraph,2,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.494
hybrid,paragraph,2,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_123']",C,['paragraph_chunk_015'],False,True,True,0.572
hybrid,paragraph,2,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_016', 'paragraph_chunk_021']",C,['paragraph_chunk_016'],False,True,True,0.598
hybrid,paragraph,2,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_022', 'paragraph_chunk_029']",C,['paragraph_chunk_018'],False,True,True,0.571
hybrid,paragraph,2,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_126']",X,[],True,False,,0.608
hybrid,paragraph,2,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_101']",C,['028'],False,True,False,0.614
hybrid,paragraph,2,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.654
hybrid,paragraph,2,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,"['008', '001']",False,True,False,0.519
hybrid,paragraph,2,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",B,['014'],False,True,False,0.559
hybrid,paragraph,2,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_013']",C,['paragraph_chunk_008'],False,True,True,0.603
hybrid,paragraph,2,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_023', 'paragraph_chunk_001']",B,['034'],False,False,False,0.496
hybrid,paragraph,2,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_025', 'paragraph_chunk_021']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.704
hybrid,paragraph,2,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_009']",D,"['paragraph_chunk_008', 'paragraph_chunk_009']",False,False,False,0.563
hybrid,paragraph,2,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_105']",B,['paragraph_chunk_004'],False,True,True,0.572
hybrid,paragraph,2,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_103']",B,['paragraph_chunk_015'],False,True,True,0.568
hybrid,paragraph,2,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.546
hybrid,paragraph,2,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_013', 'paragraph_chunk_139']",C,['paragraph_chunk_005'],False,True,False,0.297
hybrid,paragraph,2,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_001', 'paragraph_chunk_005']",C,"['001', '005', '107']",False,True,True,0.554
hybrid,paragraph,2,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_034', 'paragraph_chunk_005']",C,['006'],False,True,False,0.548
hybrid,paragraph,2,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_015', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.545
hybrid,paragraph,2,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_018']",C,"['010', '086']",False,True,False,0.869
hybrid,paragraph,2,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.352
hybrid,paragraph,2,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.568
hybrid,paragraph,2,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['016'],False,True,False,0.721
hybrid,paragraph,2,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.789
hybrid,paragraph,2,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.683
hybrid,paragraph,2,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_033']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.614
hybrid,paragraph,2,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,['paragraph_chunk_006'],False,True,True,0.628
hybrid,paragraph,2,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_010', 'paragraph_chunk_086']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.343
hybrid,paragraph,2,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_019', 'paragraph_chunk_100']",C,['paragraph_chunk_018'],False,True,True,0.992
hybrid,paragraph,2,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,2.186
hybrid,paragraph,2,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_025', 'paragraph_chunk_124']",C,['paragraph_chunk_020'],False,True,False,0.391
hybrid,paragraph,2,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_012']",C,['paragraph_chunk_034'],False,True,True,0.55
hybrid,paragraph,2,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.537
hybrid,paragraph,2,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_097', 'paragraph_chunk_002']",C,"['002', '003']",False,True,False,0.594
hybrid,paragraph,2,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_051', 'paragraph_chunk_014']",X,[],True,False,,0.583
hybrid,paragraph,2,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.395
hybrid,paragraph,3,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_013', 'paragraph_chunk_036']",X,[],True,False,,0.979
hybrid,paragraph,3,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_025', 'paragraph_chunk_014']",A,"['014', '021']",False,False,False,0.481
hybrid,paragraph,3,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_035']",B,['paragraph_chunk_034'],False,True,True,0.57
hybrid,paragraph,3,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.66
hybrid,paragraph,3,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_008', 'paragraph_chunk_006']",C,['paragraph_chunk_005'],False,True,True,0.61
hybrid,paragraph,3,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_025', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.524
hybrid,paragraph,3,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['006'],False,True,False,0.502
hybrid,paragraph,3,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",B,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.442
hybrid,paragraph,3,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.87
hybrid,paragraph,3,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_025'],False,True,True,0.681
hybrid,paragraph,3,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.51
hybrid,paragraph,3,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.542
hybrid,paragraph,3,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.631
hybrid,paragraph,3,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_091', 'paragraph_chunk_005']",C,['paragraph_chunk_016'],False,True,True,0.541
hybrid,paragraph,3,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_092']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.592
hybrid,paragraph,3,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.575
hybrid,paragraph,3,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.541
hybrid,paragraph,3,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_035']",C,['034'],False,True,False,0.725
hybrid,paragraph,3,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.388
hybrid,paragraph,3,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_003', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,"['006', '007']",False,True,False,0.625
hybrid,paragraph,3,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_034', 'paragraph_chunk_006']",C,['chunk_006'],False,True,False,0.773
hybrid,paragraph,3,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_002']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.7
hybrid,paragraph,3,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_022', 'paragraph_chunk_035']",C,['paragraph_chunk_006'],False,True,True,0.508
hybrid,paragraph,3,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.491
hybrid,paragraph,3,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_087', 'paragraph_chunk_021']",C,"['paragraph_chunk_016', 'paragraph_chunk_087']",False,True,True,0.623
hybrid,paragraph,3,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.621
hybrid,paragraph,3,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_028']",C,['paragraph_chunk_029'],False,True,True,0.914
hybrid,paragraph,3,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_007']",C,['paragraph_chunk_018'],False,True,True,0.562
hybrid,paragraph,3,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_109']",C,['paragraph_chunk_025'],False,True,True,0.386
hybrid,paragraph,3,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_030']",C,['paragraph_chunk_018'],False,True,True,0.567
hybrid,paragraph,3,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_005']",C,['paragraph_chunk_005'],False,True,False,0.638
hybrid,paragraph,3,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_028']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.584
hybrid,paragraph,3,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.651
hybrid,paragraph,3,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.654
hybrid,paragraph,3,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_123']",C,['paragraph_chunk_015'],False,True,True,0.544
hybrid,paragraph,3,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_016', 'paragraph_chunk_021']",C,['paragraph_chunk_016'],False,True,True,0.395
hybrid,paragraph,3,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_022', 'paragraph_chunk_029']",C,['paragraph_chunk_018'],False,True,True,0.376
hybrid,paragraph,3,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_126']",X,[],True,False,,0.658
hybrid,paragraph,3,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_101']",C,['028'],False,True,False,0.603
hybrid,paragraph,3,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.569
hybrid,paragraph,3,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,"['008', '001']",False,True,False,0.569
hybrid,paragraph,3,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",B,['014'],False,True,False,0.691
hybrid,paragraph,3,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_013']",C,['paragraph_chunk_008'],False,True,True,0.452
hybrid,paragraph,3,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_023', 'paragraph_chunk_001']",B,['034'],False,False,False,0.387
hybrid,paragraph,3,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_025', 'paragraph_chunk_021']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.606
hybrid,paragraph,3,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_009']",D,"['paragraph_chunk_008', 'paragraph_chunk_009']",False,False,False,0.749
hybrid,paragraph,3,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_105']",B,['paragraph_chunk_004'],False,True,True,0.621
hybrid,paragraph,3,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_103']",B,['paragraph_chunk_015'],False,True,True,0.557
hybrid,paragraph,3,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.611
hybrid,paragraph,3,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_013', 'paragraph_chunk_139']",C,['paragraph_chunk_005'],False,True,False,0.652
hybrid,paragraph,3,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_001', 'paragraph_chunk_005']",C,"['001', '005', '107']",False,True,True,0.699
hybrid,paragraph,3,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_034', 'paragraph_chunk_005']",C,['006'],False,True,False,0.345
hybrid,paragraph,3,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_015', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.503
hybrid,paragraph,3,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_018']",C,"['010', '086']",False,True,False,0.534
hybrid,paragraph,3,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.575
hybrid,paragraph,3,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,1.267
hybrid,paragraph,3,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['016'],False,True,False,0.346
hybrid,paragraph,3,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.672
hybrid,paragraph,3,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.514
hybrid,paragraph,3,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_033']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.661
hybrid,paragraph,3,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,['paragraph_chunk_006'],False,True,True,0.691
hybrid,paragraph,3,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_010', 'paragraph_chunk_086']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.511
hybrid,paragraph,3,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_019', 'paragraph_chunk_100']",C,['paragraph_chunk_018'],False,True,True,0.625
hybrid,paragraph,3,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.751
hybrid,paragraph,3,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_025', 'paragraph_chunk_124']",C,['paragraph_chunk_020'],False,True,False,0.628
hybrid,paragraph,3,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_012']",C,['paragraph_chunk_034'],False,True,True,0.588
hybrid,paragraph,3,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.627
hybrid,paragraph,3,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_097', 'paragraph_chunk_002']",C,"['002', '003']",False,True,False,0.493
hybrid,paragraph,3,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_051', 'paragraph_chunk_014']",X,[],True,False,,0.73
hybrid,paragraph,3,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.551
hybrid,paragraph,4,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_013', 'paragraph_chunk_036']",X,[],True,False,,0.684
hybrid,paragraph,4,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_025', 'paragraph_chunk_014']",A,"['014', '021']",False,False,False,0.56
hybrid,paragraph,4,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_035']",B,['paragraph_chunk_034'],False,True,True,0.546
hybrid,paragraph,4,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.509
hybrid,paragraph,4,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_008', 'paragraph_chunk_006']",C,['paragraph_chunk_005'],False,True,True,0.4
hybrid,paragraph,4,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_025', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.63
hybrid,paragraph,4,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['006'],False,True,False,0.498
hybrid,paragraph,4,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",B,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.89
hybrid,paragraph,4,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.512
hybrid,paragraph,4,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_025'],False,True,True,0.575
hybrid,paragraph,4,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.829
hybrid,paragraph,4,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.562
hybrid,paragraph,4,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.78
hybrid,paragraph,4,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_091', 'paragraph_chunk_005']",C,['paragraph_chunk_016'],False,True,True,0.717
hybrid,paragraph,4,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_092']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.425
hybrid,paragraph,4,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.341
hybrid,paragraph,4,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.465
hybrid,paragraph,4,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_035']",C,['034'],False,True,False,0.576
hybrid,paragraph,4,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.629
hybrid,paragraph,4,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_003', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,"['006', '007']",False,True,False,0.519
hybrid,paragraph,4,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_034', 'paragraph_chunk_006']",C,['chunk_006'],False,True,False,0.31
hybrid,paragraph,4,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_002']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.357
hybrid,paragraph,4,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_022', 'paragraph_chunk_035']",C,['paragraph_chunk_006'],False,True,True,0.562
hybrid,paragraph,4,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.343
hybrid,paragraph,4,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_087', 'paragraph_chunk_021']",C,"['paragraph_chunk_016', 'paragraph_chunk_087']",False,True,True,0.337
hybrid,paragraph,4,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.354
hybrid,paragraph,4,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_028']",C,['paragraph_chunk_029'],False,True,True,0.456
hybrid,paragraph,4,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_007']",C,['paragraph_chunk_018'],False,True,True,0.491
hybrid,paragraph,4,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_109']",C,['paragraph_chunk_025'],False,True,True,0.511
hybrid,paragraph,4,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_030']",C,['paragraph_chunk_018'],False,True,True,0.505
hybrid,paragraph,4,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_005']",C,['paragraph_chunk_005'],False,True,False,0.563
hybrid,paragraph,4,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_028']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.566
hybrid,paragraph,4,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,1.358
hybrid,paragraph,4,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.505
hybrid,paragraph,4,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_123']",C,['paragraph_chunk_015'],False,True,True,0.336
hybrid,paragraph,4,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_016', 'paragraph_chunk_021']",C,['paragraph_chunk_016'],False,True,True,0.41
hybrid,paragraph,4,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_022', 'paragraph_chunk_029']",C,['paragraph_chunk_018'],False,True,True,0.55
hybrid,paragraph,4,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_126']",X,[],True,False,,0.5
hybrid,paragraph,4,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_101']",C,['028'],False,True,False,0.554
hybrid,paragraph,4,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.511
hybrid,paragraph,4,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,"['008', '001']",False,True,False,0.462
hybrid,paragraph,4,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",B,['014'],False,True,False,0.531
hybrid,paragraph,4,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_013']",C,['paragraph_chunk_008'],False,True,True,0.334
hybrid,paragraph,4,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_023', 'paragraph_chunk_001']",B,['034'],False,False,False,0.606
hybrid,paragraph,4,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_025', 'paragraph_chunk_021']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.573
hybrid,paragraph,4,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_009']",D,"['paragraph_chunk_008', 'paragraph_chunk_009']",False,False,False,0.499
hybrid,paragraph,4,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_105']",B,['paragraph_chunk_004'],False,True,True,0.385
hybrid,paragraph,4,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_103']",B,['paragraph_chunk_015'],False,True,True,0.572
hybrid,paragraph,4,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.61
hybrid,paragraph,4,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_013', 'paragraph_chunk_139']",C,['paragraph_chunk_005'],False,True,False,0.563
hybrid,paragraph,4,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_001', 'paragraph_chunk_005']",C,"['001', '005', '107']",False,True,True,0.45
hybrid,paragraph,4,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_034', 'paragraph_chunk_005']",C,['006'],False,True,False,0.552
hybrid,paragraph,4,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_015', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.539
hybrid,paragraph,4,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_018']",C,"['010', '086']",False,True,False,0.546
hybrid,paragraph,4,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.666
hybrid,paragraph,4,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.642
hybrid,paragraph,4,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['016'],False,True,False,0.323
hybrid,paragraph,4,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.619
hybrid,paragraph,4,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.582
hybrid,paragraph,4,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_033']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.942
hybrid,paragraph,4,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,['paragraph_chunk_006'],False,True,True,0.348
hybrid,paragraph,4,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_010', 'paragraph_chunk_086']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.566
hybrid,paragraph,4,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_019', 'paragraph_chunk_100']",C,['paragraph_chunk_018'],False,True,True,0.62
hybrid,paragraph,4,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.989
hybrid,paragraph,4,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_025', 'paragraph_chunk_124']",C,['paragraph_chunk_020'],False,True,False,0.621
hybrid,paragraph,4,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_012']",C,['paragraph_chunk_034'],False,True,True,0.613
hybrid,paragraph,4,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.722
hybrid,paragraph,4,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_097', 'paragraph_chunk_002']",C,"['002', '003']",False,True,False,0.598
hybrid,paragraph,4,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_051', 'paragraph_chunk_014']",X,[],True,False,,0.514
hybrid,paragraph,4,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.343
hybrid,paragraph,5,1,What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?,"{'A': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'B': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'C': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'D': 'It offloads all attention computations to a specialized lightweight encoder model.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_117', 'paragraph_chunk_013', 'paragraph_chunk_036']",X,[],True,False,,0.412
hybrid,paragraph,5,2,"During the continual pre-training phase of REFRAG, what is the specific purpose of the reconstruction task?","{'A': 'To train the decoder to generate more fluent text from compressed inputs.', 'B': 'To fine-tune the entire model for downstream RAG applications.', 'C': 'To encourage the model to rely on context memory for reconstruction rather than its internal parametric memory.', 'D': 'To pre-train the lightweight encoder model from scratch using next-paragraph prediction.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_025', 'paragraph_chunk_014']",A,"['014', '021']",False,False,False,0.34
hybrid,paragraph,5,3,What is a key limitation of the CEPE model for long-context applications that REFRAG is designed to overcome?,"{'A': ""CEPE requires significant modifications to the LLM's core architecture, making it difficult to adapt."", 'B': 'CEPE is unsuitable for tasks like multi-turn RAG because it disrupts the causal structure of the context.', 'C': 'CEPE does not offer any improvement in Time-to-First-Token (TTFT) latency over standard models.', 'D': 'CEPE is unable to process context lengths beyond 4096 tokens, unlike REFRAG.'}",B,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_035']",B,['paragraph_chunk_034'],False,True,True,0.623
hybrid,paragraph,5,4,How does REFRAG's performance in multi-turn conversation tasks compare to a fine-tuned LLaMA model (LLAMAFT) in scenarios with long conversational histories?,"{'A': 'REFRAG performs similarly to LLAMAFT because both models use the same underlying decoder.', 'B': 'LLAMAFT consistently outperforms REFRAG due to its ability to process uncompressed conversational history.', 'C': ""REFRAG's performance degrades significantly as it must compress the entire conversational history into a single vector."", 'D': 'REFRAG shows improved performance because it avoids truncating crucial conversational history, a problem LLAMAFT faces due to its limited context window.'}",D,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_034']",D,['paragraph_chunk_029'],False,True,False,0.535
hybrid,paragraph,5,5,What fundamental observation about the nature of RAG contexts motivates the REFRAG framework's design?,"{'A': 'Retrieved passages in RAG contexts are always highly relevant and semantically similar to the user query.', 'B': 'RAG contexts are computationally inexpensive to process, but they introduce high memory overhead.', 'C': 'The attention patterns in RAG contexts are often sparse and block-diagonal due to the diverse and unrelated nature of retrieved chunks.', 'D': 'Most computations over the RAG context are dedicated to encoding the user query rather than the retrieved documents.'}",C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_008', 'paragraph_chunk_006']",C,['paragraph_chunk_005'],False,True,True,0.634
hybrid,paragraph,5,6,Why is curriculum learning considered an essential component of the training methodology for REFRAG?,"{'A': 'It is required to fine-tune the model for specific downstream tasks like long document summarization.', 'B': 'It prevents the model from overfitting on the Slimpajama dataset during the initial pre-training phase.', 'C': 'It helps the model manage the exponential complexity of possible token combinations when learning to reconstruct from chunk embeddings.', 'D': 'It allows the model to learn selective compression by gradually introducing the reinforcement learning policy.'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.","['paragraph_chunk_100', 'paragraph_chunk_025', 'paragraph_chunk_015']",C,['paragraph_chunk_015'],False,True,True,0.522
hybrid,paragraph,5,7,What is the role of the lightweight reinforcement learning (RL) policy within the REFRAG framework?,"{'A': 'To decide the optimal compression rate (k) for the entire context based on the query.', 'B': 'To replace the lightweight encoder with a more dynamic, policy-based compression mechanism.', 'C': 'To determine on a chunk-by-chunk basis whether to use a compressed embedding or the full token input.', 'D': 'To re-rank the retrieved passages before they are processed by the encoder.'}",C,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.","['paragraph_chunk_024', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['006'],False,True,False,0.677
hybrid,paragraph,5,8,Under what conditions does the REFRAG model achieve its highest acceleration in Time-to-First-Token (TTFT) and throughput?,"{'A': 'For short context lengths, where it achieves up to kx acceleration.', 'B': 'For longer context lengths, where acceleration can reach up to k²x.', 'C': 'When the cache is disabled, leading to a consistent kx acceleration regardless of context length.', 'D': 'In multi-turn conversation, where acceleration is independent of context length.'}",B,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",B,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.584
hybrid,paragraph,5,9,How does REFRAG achieve performance gains in RAG tasks under equivalent latency constraints when compared to a fine-tuned LLaMA model?,"{'A': 'By using a more powerful decoder model while keeping the latency the same.', 'B': 'By processing fewer passages than LLaMA but extracting more relevant information from each one.', 'C': 'By using the latency savings from compression to include more passages and thus more context.', 'D': 'By caching the results of previous queries to reduce latency for repeated questions.'}",C,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.396
hybrid,paragraph,5,10,"What happens to REFRAG's performance as the compression rate becomes increasingly aggressive, for instance, a rate of 64?","{'A': 'Performance continues to improve linearly with the compression rate due to increased efficiency.', 'B': ""The model's performance remains competitive and stable, showing no significant degradation."", 'C': 'Performance begins to diminish, suggesting a practical upper limit to how much compression can be applied effectively.', 'D': 'The model fails to produce any output due to memory errors caused by over-compression.'}",C,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_025'],False,True,True,0.356
hybrid,paragraph,5,11,What property of RAG contexts makes it possible to eliminate much of the decoding computation with little impact on performance?,"{'A': 'High semantic similarity among retrieved passages leading to uniform attention', 'B': 'Global attention that densely connects all tokens across passages', 'C': 'Block-diagonal attention patterns caused by mostly unrelated chunks in the context', 'D': 'Dynamic sharing of key–value caches across unrelated prompts'}",C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.","['paragraph_chunk_005', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,['paragraph_chunk_001'],False,True,True,0.441
hybrid,paragraph,5,12,"When replacing token sequences with precomputed chunk embeddings in decoding, how does the attention computation scale and why is this helpful?","{'A': 'It becomes linear in the number of tokens, reducing quadratic costs tied to layers', 'B': 'It remains quadratic in the number of tokens but with a smaller constant factor', 'C': 'It becomes quadratic in the number of chunks instead of the number of tokens, shortening the effective sequence', 'D': 'It becomes sublinear due to caching, independent of chunks or tokens'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_015']",C,['paragraph_chunk_006'],False,True,True,0.682
hybrid,paragraph,5,13,Which statement correctly characterizes the theoretical acceleration in TTFT/throughput as context length varies?,"{'A': 'Up to k²× for short contexts and up to k× for long contexts', 'B': 'Constant for short contexts and logarithmic in k for long contexts', 'C': 'Up to k× for short contexts and up to k²× for long contexts', 'D': 'Linear in k for all context lengths'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.505
hybrid,paragraph,5,14,"How are important context chunks selected for expansion, and what decoding property is preserved during this process?","{'A': 'By random sampling; decoder’s causal masking is removed to allow cross-chunk fusion', 'B': 'By TF-IDF scores; decoder’s cross-attention is disabled to reduce interference', 'C': 'By an RL policy using next-paragraph perplexity as a negative reward; the decoder’s autoregressive property is preserved', 'D': 'By maximum similarity to the query; the decoder operates bidirectionally over the context'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","['paragraph_chunk_016', 'paragraph_chunk_091', 'paragraph_chunk_005']",C,['paragraph_chunk_016'],False,True,True,0.503
hybrid,paragraph,5,15,"During the reconstruction phase used to align encoder and decoder, which components are trained and what behavior does this encourage?","{'A': 'Only the decoder is trained; it encourages reliance on parametric memory over context', 'B': 'Both encoder and decoder are trained; it encourages aggressive token dropout', 'C': 'Only the encoder and projection layer are trained; it encourages reliance on context memory rather than parametric memory', 'D': 'Only the retriever is trained; it encourages higher lexical overlap among chunks'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_092']",C,"['paragraph_chunk_015', 'paragraph_chunk_014']",False,True,True,0.631
hybrid,paragraph,5,16,"Why is curriculum learning applied in training with chunk embeddings, and how is task difficulty increased?","{'A': 'To stabilize gradients; difficulty is increased by reducing batch size each epoch', 'B': 'To avoid overfitting; difficulty is increased by adding label noise', 'C': 'To handle the exponential V^k combination space; difficulty is increased from single-chunk to multi-chunk reconstruction via data mixture scheduling', 'D': 'To speed up data loading; difficulty is increased by shortening sequences progressively'}",C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_016', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.614
hybrid,paragraph,5,17,What empirical result is reported when using RL-based selective compression to achieve an effective compression rate of 8?,"{'A': 'REFRAG8 consistently outperforms REFRAG16 because it is natively trained at rate 8', 'B': 'REFRAG8 and REFRAG16 perform identically when matched for compression rate', 'C': 'REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 without recomputing chunk embeddings', 'D': 'REFRAG16 underperforms due to over-compression even when expanded to rate 8'}",C,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.","['paragraph_chunk_109', 'paragraph_chunk_021', 'paragraph_chunk_024']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.61
hybrid,paragraph,5,18,Which limitation prevents CEPE from supporting multi-turn RAG and similar applications efficiently?,"{'A': 'It requires encoder–decoder pretraining from scratch for every task', 'B': 'It relies on dense global attention over all tokens, increasing KV memory', 'C': 'It disrupts the causal structure and is limited to prefix contexts, while not using token compression', 'D': 'It cannot cache any intermediate states across turns'}",C,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_012', 'paragraph_chunk_034', 'paragraph_chunk_035']",C,['034'],False,True,False,0.549
hybrid,paragraph,5,19,"Under equal latency in a weak retriever setting, what passage configuration is compared and what average performance gain is reported?","{'A': '4 passages vs. 2; 0.71% average gain', 'B': '10 passages vs. 10; 5.26% average gain', 'C': '8 passages for the compressed approach vs. 1 for the baseline; 1.93% average gain', 'D': '1 passage vs. 8; 1.22% average gain'}",C,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,"['paragraph_chunk_029', 'paragraph_chunk_028']",False,True,True,0.832
hybrid,paragraph,5,20,How is the effective context window extended beyond the decoder’s native limit?,"{'A': 'By switching to a bi-directional decoder with rotary scaling', 'B': 'By stacking additional attention layers at inference time', 'C': 'By leveraging chunk embeddings and compression to extend the context size by up to 16×', 'D': 'By replacing attention with recurrent state updates over tokens'}",C,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.","['paragraph_chunk_003', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,"['006', '007']",False,True,False,0.616
hybrid,paragraph,5,21,Which benefit specifically results from replacing raw context tokens with pre-computed chunk embeddings during decoding?,"{'A': 'Eliminating the key–value (KV) cache entirely to achieve constant memory use across context lengths.', 'B': 'Reducing attention complexity to linear in the total number of original tokens through token pruning.', 'C': 'Reducing attention computation so complexity scales with the number of chunks (quadratic in chunks) rather than with the number of tokens.', 'D': 'Avoiding the need for an encoder by learning chunk embeddings directly inside the decoder layers.'}",C,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_015', 'paragraph_chunk_034', 'paragraph_chunk_006']",C,['chunk_006'],False,True,False,0.608
hybrid,paragraph,5,22,"Under long-context settings, what acceleration does the method theoretically achieve for time-to-first-token and throughput?","{'A': 'Up to log(k)× for TTFT and constant for throughput.', 'B': 'Up to k× for TTFT but no change in throughput.', 'C': 'Up to k^2× for both TTFT and throughput.', 'D': 'Up to sqrt(k)× for TTFT and k× for throughput.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_002']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.739
hybrid,paragraph,5,23,What capability allows the approach to support multi-turn and agentic applications while still benefiting from compression?,"{'A': 'Cross-attention removal that forces all context to be prefix-only.', 'B': 'A decoder architecture with non-autoregressive generation for context tokens.', 'C': 'Compression of token chunks at arbitrary positions with an RL policy that decides when to expand selected chunks.', 'D': 'A retrieval stage that always deduplicates to a single document before decoding.'}",C,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .","['paragraph_chunk_006', 'paragraph_chunk_022', 'paragraph_chunk_035']",C,['paragraph_chunk_006'],False,True,True,0.616
hybrid,paragraph,5,24,"When aligning the encoder and decoder via a reconstruction task, what are the two primary goals for the encoder and projection layer?","{'A': 'To maximize reliance on the decoder’s parametric memory and to minimize use of context memory.', 'B': 'To map token embeddings into the encoder space and discard positional information entirely.', 'C': 'To compress k tokens with minimal information loss and project chunk embeddings into the decoder’s token space so the decoder can reconstruct accurately.', 'D': 'To train only the decoder end-to-end while freezing both the encoder and projection layers.'}",C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.515
hybrid,paragraph,5,25,What learning signal guides the policy that decides which context chunks should remain uncompressed?,"{'A': 'BLEU score measured on generated answers during instruction tuning.', 'B': 'Wall-clock latency improvements observed during prefill.', 'C': 'Next-paragraph prediction perplexity used as a negative reward in reinforcement learning.', 'D': 'A fixed TF-IDF score threshold computed over the retrieval corpus.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_016', 'paragraph_chunk_087', 'paragraph_chunk_021']",C,"['paragraph_chunk_016', 'paragraph_chunk_087']",False,True,True,0.648
hybrid,paragraph,5,26,"For mid-to-long contexts (length 16,384) and compression rate k=16, what empirical TTFT acceleration with caching was observed?","{'A': '8.59×', 'B': '9.30×', 'C': '16.53×', 'D': '32.99×'}",C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_100']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.638
hybrid,paragraph,5,27,"At equal latency in a strong-retriever setting (8 passages vs. 1), what average change in performance across RAG tasks is achieved?","{'A': 'A 5.26% improvement due to higher passage recall.', 'B': 'A 0.71% improvement driven by better weak-retriever robustness.', 'C': 'A 1.22% improvement averaged over 16 tasks.', 'D': 'No measurable improvement; only latency decreases.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.","['paragraph_chunk_029', 'paragraph_chunk_023', 'paragraph_chunk_028']",C,['paragraph_chunk_029'],False,True,True,2.24
hybrid,paragraph,5,28,"When the number of chunk embeddings matches 256 (s/k = 256), how does performance compare to using only the last 256 tokens of the context?","{'A': 'They perform approximately the same due to equal token counts.', 'B': 'Using the last 256 tokens performs better because no projection is needed.', 'C': 'Compressed chunk embeddings consistently surpass using the last 256 tokens.', 'D': 'Both settings fail due to out-of-memory errors on long contexts.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_007']",C,['paragraph_chunk_018'],False,True,True,0.589
hybrid,paragraph,5,29,What is the observed effect of increasing the compression rate to 64 on model capability?,"{'A': 'Performance continues to improve due to stronger regularization.', 'B': 'Latency improves but perplexity remains unchanged.', 'C': 'Performance diminishes, suggesting a practical limit beyond which capability degrades.', 'D': 'Training diverges unless the encoder is made significantly larger.'}",C,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.","['paragraph_chunk_025', 'paragraph_chunk_021', 'paragraph_chunk_109']",C,['paragraph_chunk_025'],False,True,True,0.575
hybrid,paragraph,5,30,What mechanism enables extending effective context beyond the original 4k window without modifying the decoder’s positional encoding?,"{'A': 'Rotary positional scaling applied during fine-tuning.', 'B': 'Speculative decoding that predicts future tokens to compress the prompt.', 'C': 'Using chunk embeddings to extrapolate context length and support broader applications.', 'D': 'LoRA adapters that re-parameterize the attention projections.'}",C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_018', 'paragraph_chunk_034', 'paragraph_chunk_030']",C,['paragraph_chunk_018'],False,True,True,2.62
hybrid,paragraph,5,31,How does REFRAG change the scaling of attention computation during decoding when using compressed context?,"{'A': 'It makes attention computation linear in the number of tokens regardless of context size.', 'B': 'It keeps quadratic scaling in the number of tokens but reduces constant factors with caching.', 'C': 'It makes attention computation scale quadratically with the number of chunks rather than the number of tokens.', 'D': 'It changes attention complexity to cubic in the number of chunks to better model cross-chunk interactions.'}",C,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_034', 'paragraph_chunk_003', 'paragraph_chunk_005']",C,['paragraph_chunk_005'],False,True,False,0.566
hybrid,paragraph,5,32,What is the theoretical acceleration in TTFT/throughput that REFRAG can achieve for short versus long contexts as a function of the compression rate k?,"{'A': 'Up to k× for long contexts and no improvement for short contexts.', 'B': 'Up to k× for both short and long contexts.', 'C': 'Up to k× for short contexts and up to k²× for long contexts.', 'D': 'Up to √k× for short contexts and linear in k for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_028']",C,"['paragraph_chunk_086', 'paragraph_chunk_010']",False,True,True,0.578
hybrid,paragraph,5,33,"At a context length of 16,384 with compression rate k = 16, what TTFT acceleration does REFRAG achieve with and without cached chunk embeddings?","{'A': '2.01× with cache and 1.04× without cache.', 'B': '8.59× with cache and 16.53× without cache.', 'C': '16.53× with cache and 8.59× without cache.', 'D': '32.99× with cache and 3.75× without cache.'}",C,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).","['paragraph_chunk_086', 'paragraph_chunk_010', 'paragraph_chunk_087']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.599
hybrid,paragraph,5,34,"During the reconstruction task used in continual pre-training, which components are updated and what behavior does the task encourage?","{'A': 'The decoder and projection layer are trained, encouraging reliance on parametric memory.', 'B': 'Only the decoder is trained, encouraging the model to ignore context memory.', 'C': 'The decoder is frozen while the encoder and projection layer are trained, encouraging reliance on context memory.', 'D': 'Only the encoder is frozen, encouraging stronger cross-attention to question tokens.'}",C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,1.183
hybrid,paragraph,5,35,How is curriculum learning applied to the reconstruction task to address optimization difficulty as k and the number of chunks increase?,"{'A': 'By starting with all chunks reconstructed at once and progressively pruning tokens using scheduled sampling.', 'B': 'By freezing the encoder and gradually unfreezing layers of the decoder based on validation loss.', 'C': 'By beginning with reconstructing a single chunk and gradually increasing to multiple chunks while adjusting the data mixture toward harder examples over time.', 'D': 'By replacing ground-truth tokens with model predictions earlier in training to induce robustness to errors.'}",C,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.","['paragraph_chunk_015', 'paragraph_chunk_014', 'paragraph_chunk_123']",C,['paragraph_chunk_015'],False,True,True,0.717
hybrid,paragraph,5,36,"What signal drives REFRAG’s RL-based selective compression policy, and which property of the decoder does the policy explicitly preserve?","{'A': 'Policy gradient on token-level cross-entropy, preserving bidirectional attention.', 'B': 'Entropy regularization on chunk selection, preserving causal masking.', 'C': 'Next-paragraph prediction perplexity as a negative reward, preserving the decoder’s autoregressive property.', 'D': 'Contrastive loss between chunks, preserving cross-document attention.'}",C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","['paragraph_chunk_024', 'paragraph_chunk_016', 'paragraph_chunk_021']",C,['paragraph_chunk_016'],False,True,True,0.6
hybrid,paragraph,5,37,Which compression rate delivers a 9.3% average perplexity improvement over CEPE across four datasets according to the reported results?,"{'A': '4', 'B': '8', 'C': '16', 'D': '32'}",C,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.","['paragraph_chunk_018', 'paragraph_chunk_022', 'paragraph_chunk_029']",C,['paragraph_chunk_018'],False,True,True,0.67
hybrid,paragraph,5,38,What is the scale and composition of the continual pre-training corpus used to align the encoder and decoder?,"{'A': '627B tokens from CommonCrawl only with heavy deduplication.', 'B': '10B tokens from Wikipedia and StackExchange in equal proportions.', 'C': '20B tokens consisting of 50% ArXiv and 50% Book domains from SlimPajama.', 'D': '1.1M instruction-tuning examples spanning five domains.'}",C,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,"['paragraph_chunk_014', 'paragraph_chunk_026', 'paragraph_chunk_126']",X,[],True,False,,0.789
hybrid,paragraph,5,39,"Which retriever and corpus specification are used to obtain passages for RAG, including corpus size and passage length constraint?","{'A': 'BM25 over a 100M-passage Wikipedia-only corpus, max 512 words per passage.', 'B': 'Contriever over a 500M CommonCrawl corpus, no length limit per passage.', 'C': 'DRAGON+ over a 400M-passage corpus built from Wikipedia and CommonCrawl, with each passage under 200 words.', 'D': 'ColBERTv2 over a 1B mixed-domain corpus, up to 300 words per passage.'}",C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,"['paragraph_chunk_028', 'paragraph_chunk_029', 'paragraph_chunk_101']",C,['028'],False,True,False,0.415
hybrid,paragraph,5,40,"In the strong-retriever setting at equal latency, how are passages allocated between REFRAG and LLaMA, and what average performance change is observed?","{'A': 'Both use 10 passages; REFRAG lags LLaMA by 0.71% on average.', 'B': 'REPRAG uses 1 passage and LLaMA 8; REFRAG leads by 1.22% on average.', 'C': 'REFRAG uses 8 passages and LLaMA 1; REFRAG leads by 1.22% on average.', 'D': 'REFRAG uses 4 passages and LLaMA 4; performance is identical on average.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_005', 'paragraph_chunk_027']",C,['paragraph_chunk_029'],False,True,True,0.631
hybrid,paragraph,5,41,How does the REFRAG framework handle input from retrieved passages to improve decoding efficiency?,"{'A': 'It exclusively uses the full token sequences from all retrieved passages as direct input for the decoder.', 'B': 'It fine-tunes the LLM architecture to better process sparse attention patterns from RAG inputs.', 'C': 'It utilizes pre-computed, compressed chunk embeddings as approximate representations of the passages, feeding these directly into the decoder.', 'D': 'It relies on a reinforcement learning policy to re-rank passages and only feeds the top-ranked passage to the decoder.'}",C,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_008', 'paragraph_chunk_001', 'paragraph_chunk_034']",C,"['008', '001']",False,True,False,0.334
hybrid,paragraph,5,42,What is the primary objective of the reconstruction task in the continual pre-training recipe for REFRAG?,"{'A': 'To train the decoder to generate more fluent and coherent text.', 'B': 'To align the encoder and projection layer so the decoder can accurately interpret compressed chunk embeddings and reconstruct the original information.', 'C': 'To increase the parametric memory of the model for better performance on downstream tasks.', 'D': 'To reduce the overall perplexity of the model on the pre-training dataset.'}",B,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",B,['014'],False,True,False,0.947
hybrid,paragraph,5,43,How does REFRAG's selective compression mechanism decide which context chunks to leave uncompressed?,"{'A': 'It randomly selects a fixed percentage of chunks to expand.', 'B': 'It uses a heuristic based on the semantic similarity of chunks to the initial query.', 'C': 'It employs a reinforcement learning policy that uses next-paragraph prediction perplexity as a negative reward to determine which chunks to expand.', 'D': 'It expands the chunks that have the highest token count to preserve the most information.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_008', 'paragraph_chunk_034', 'paragraph_chunk_013']",C,['paragraph_chunk_008'],False,True,True,0.621
hybrid,paragraph,5,44,What advantage does REFRAG have over models like CEPE in multi-turn RAG or summarization tasks?,"{'A': 'REFRAG requires significantly less memory for its key-value cache.', 'B': 'REFRAG can be applied to prefix context applications without disrupting the causal structure.', 'C': 'REFRAG is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt.', 'D': 'REFRAG achieves a lower time-to-first-token latency by using a more efficient attention mechanism.'}",C,"In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.","['paragraph_chunk_034', 'paragraph_chunk_023', 'paragraph_chunk_001']",B,['034'],False,False,False,0.507
hybrid,paragraph,5,45,What was a key finding when comparing the performance of REFRAG with a compression rate of 16 (REFRAG16) against a natively trained model with a compression rate of 8 (REFRAG8)?,"{'A': 'REFRAG8 consistently outperformed REFRAG16 with RL-based selective compression.', 'B': 'Both approaches performed identically, showing that the compression rate is the only factor.', 'C': 'REFRAG16 using RL-based selective compression to achieve an effective rate of 8 consistently outperformed the natively trained REFRAG8.', 'D': 'The RL-based policy for REFRAG16 was unable to match the performance of the natively trained REFRAG8.'}",C,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"['paragraph_chunk_024', 'paragraph_chunk_025', 'paragraph_chunk_021']",C,"['paragraph_chunk_021', 'paragraph_chunk_024']",False,True,True,0.747
hybrid,paragraph,5,46,"In the context of Retrieval-Augmented Generation (RAG), what specific structural characteristic of the input context does REFRAG exploit to eliminate unnecessary computations?","{'A': 'The high degree of semantic similarity between all retrieved passages.', 'B': 'The sequential and autoregressive nature of standard LLM generation tasks.', 'C': 'The predominantly zero cross-attention between unrelated context chunks from diverse retrieved passages.', 'D': 'The inherent redundancy created by the initial retrieval process that REFRAG deduplicates.'}",C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_008', 'paragraph_chunk_013', 'paragraph_chunk_009']",D,"['paragraph_chunk_008', 'paragraph_chunk_009']",False,False,False,0.774
hybrid,paragraph,5,47,"According to the paper's analysis, how does the time-to-first-token (TTFT) latency scale with increasing prompt length in conventional Large Language Models?","{'A': 'It increases linearly.', 'B': 'It increases quadratically.', 'C': 'It remains constant regardless of prompt length.', 'D': 'It increases logarithmically.'}",B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.","['paragraph_chunk_004', 'paragraph_chunk_026', 'paragraph_chunk_105']",B,['paragraph_chunk_004'],False,True,True,0.861
hybrid,paragraph,5,48,What is the role of curriculum learning in the training process of the REFRAG model?,"{'A': 'It is used to fine-tune the model for specific downstream tasks like RAG and summarization.', 'B': 'It helps the model to gradually acquire complex skills by incrementally increasing the difficulty of the reconstruction and pre-training tasks.', 'C': 'It is a technique to reduce the memory footprint during training by processing smaller chunks first.', 'D': 'It is primarily used to train the reinforcement learning policy for selective compression.'}",B,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_103']",B,['paragraph_chunk_015'],False,True,True,0.702
hybrid,paragraph,5,49,How did REFRAG perform in a weak retriever scenario compared to a standard LLaMA model when given the same number of retrieved passages?,"{'A': 'It underperformed LLaMA because the compressed context lost crucial information from the less relevant passages.', 'B': 'It matched the performance of LLaMA but with significantly higher latency.', 'C': 'It outperformed LLaMA because its ability to handle a larger context allowed it to extract more useful information from the noisy retrieved set.', 'D': 'The performance was identical, as both models struggled equally with the irrelevant passages.'}",C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","['paragraph_chunk_023', 'paragraph_chunk_029', 'paragraph_chunk_028']",C,"['paragraph_chunk_029', 'paragraph_chunk_023']",False,True,True,0.529
hybrid,paragraph,5,50,What happens to the computational complexity of the attention mechanism in REFRAG due to its design?,"{'A': 'It remains quadratic with the number of tokens in the context.', 'B': 'It becomes linear with the number of tokens in the context.', 'C': 'It scales quadratically with the number of compressed chunks rather than the number of tokens.', 'D': 'It is reduced to a constant time complexity through pre-computation.'}",C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_005', 'paragraph_chunk_013', 'paragraph_chunk_139']",C,['paragraph_chunk_005'],False,True,False,0.619
hybrid,paragraph,5,51,What decoding-time attention pattern is induced in RAG contexts due to diversity and deduplication during re-ranking?,"{'A': 'Dense cross-attention uniformly linking every chunk to every other chunk.', 'B': 'Strictly local sliding-window attention across all tokens regardless of chunk boundaries.', 'C': 'Predominantly zero cross-attention between unrelated chunks, yielding block-diagonal structure.', 'D': 'Global cross-attention from every chunk to the query with uniform weights.'}",C,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).","['paragraph_chunk_107', 'paragraph_chunk_001', 'paragraph_chunk_005']",C,"['001', '005', '107']",False,True,True,0.615
hybrid,paragraph,5,52,Which benefit of feeding pre-computed chunk embeddings directly to the decoder most directly reduces attention computation complexity?,"{'A': 'Eliminating positional encodings from the decoder stack.', 'B': 'Increasing the decoder’s vocabulary size to absorb more context.', 'C': 'Making attention scale with the number of chunks rather than the number of tokens in the context.', 'D': 'Replacing self-attention with cross-attention to token embeddings as in CEPE.'}",C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","['paragraph_chunk_006', 'paragraph_chunk_034', 'paragraph_chunk_005']",C,['006'],False,True,False,0.572
hybrid,paragraph,5,53,"In REFRAG’s architecture, how are chunk embeddings aligned to the decoder’s token embedding space before decoding?","{'A': 'By adding rotary positional encodings to each chunk embedding.', 'B': 'By averaging chunk embeddings with the query’s token embeddings.', 'C': 'By projecting each chunk embedding with a learnable layer ϕ to match the decoder’s token embedding size.', 'D': 'By concatenating raw chunk embeddings directly into the KV cache without transformation.'}",C,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).","['paragraph_chunk_008', 'paragraph_chunk_015', 'paragraph_chunk_016']",C,['paragraph_chunk_015'],False,True,True,0.684
hybrid,paragraph,5,54,"According to the theoretical analysis, what is the maximum acceleration achievable for TTFT at longer context lengths with compression rate k?","{'A': 'log k', 'B': '√k', 'C': 'Up to k²', 'D': 'k/2'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.","['paragraph_chunk_010', 'paragraph_chunk_086', 'paragraph_chunk_018']",C,"['010', '086']",False,True,False,0.688
hybrid,paragraph,5,55,"During the reconstruction task used to align encoder and decoder, which components are updated?","{'A': 'Only the decoder parameters are trained while the encoder is frozen.', 'B': 'Only the encoder is trained and the projection layer is discarded.', 'C': 'The encoder and the projection layer are trained while the decoder is frozen.', 'D': 'All modules (encoder, decoder, and projection) are jointly fine-tuned.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…","['paragraph_chunk_014', 'paragraph_chunk_092', 'paragraph_chunk_015']",C,['paragraph_chunk_014'],False,True,True,0.741
hybrid,paragraph,5,56,Why is curriculum learning applied to the reconstruction and continual pre-training tasks in REFRAG?,"{'A': 'To shrink the model by pruning layers without hurting perplexity.', 'B': 'To switch from unsupervised to fully supervised training mid-course.', 'C': 'To address optimization challenges after direct pre-training failed to reduce perplexity by gradually increasing task difficulty.', 'D': 'To change the retriever’s loss from contrastive to generative.'}",C,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.","['paragraph_chunk_015', 'paragraph_chunk_025', 'paragraph_chunk_014']",C,['paragraph_chunk_015'],False,True,True,0.537
hybrid,paragraph,5,57,What signal guides the reinforcement learning policy when deciding which chunks to expand uncompressed?,"{'A': 'Token-level negative log-likelihood on the current output tokens.', 'B': 'Next-token entropy estimated by the decoder’s softmax.', 'C': 'Next-paragraph prediction perplexity used as a negative reward.', 'D': 'Beam search scores averaged over candidate continuations.'}",C,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.","['paragraph_chunk_088', 'paragraph_chunk_006', 'paragraph_chunk_016']",C,['016'],False,True,False,2.913
hybrid,paragraph,5,58,"When targeting an effective compression rate of 8, which approach performed better across datasets and context lengths?","{'A': 'REFRAG8 trained with full compression at rate 8.', 'B': 'CEPE with heuristic chunk pruning.', 'C': 'REFRAG16 with RL-based selective compression achieving rate 8 without recomputing embeddings.', 'D': 'LLaMA-32K using naive truncation to match token count.'}",C,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…","['paragraph_chunk_018', 'paragraph_chunk_021', 'paragraph_chunk_141']",C,['paragraph_chunk_021'],False,True,True,0.559
hybrid,paragraph,5,59,"Under a strong retriever, at equal latency, which configuration yields an average 1.22% improvement across 16 RAG tasks?","{'A': 'LLaMA with 8 passages versus REFRAG with 1 passage.', 'B': 'REFRAG and LLaMA both with 1 passage each.', 'C': 'REFRAG with 8 passages versus LLaMA with 1 passage.', 'D': 'LLaMA-32K with 80 passages versus REFRAG with 8 passages.'}",C,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_018', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,1.027
hybrid,paragraph,5,60,What limitation of CEPE makes it unsuitable for tasks like multi-turn RAG or summarization?,"{'A': 'It requires massive retriever pre-training on billions of documents.', 'B': 'It compresses tokens into discrete codes that cannot be decoded.', 'C': 'It is limited to prefix context and disrupts the causal structure of the context.', 'D': 'It lacks any mechanism to reduce KV-cache memory during decoding.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.","['paragraph_chunk_034', 'paragraph_chunk_012', 'paragraph_chunk_033']",C,"['paragraph_chunk_034', 'paragraph_chunk_033']",False,True,True,0.649
hybrid,paragraph,5,61,Which benefit does representing retrieved context as projected chunk embeddings provide during decoding?,"{'A': 'It increases the size of the key–value cache to preserve more cross-attention states.', 'B': 'It requires adding new decoder layers to handle compressed inputs.', 'C': 'It reduces attention computation by letting complexity scale with the number of chunks rather than the number of tokens.', 'D': 'It prevents the model from handling multi-turn inputs due to loss of causal structure.'}",C,"This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.","['paragraph_chunk_034', 'paragraph_chunk_006', 'paragraph_chunk_007']",C,['paragraph_chunk_006'],False,True,True,0.413
hybrid,paragraph,5,62,How does time-to-first-token (TTFT) acceleration scale with the compression rate k for short versus long contexts?,"{'A': 'It is constant for short contexts and linear for long contexts.', 'B': 'It scales quadratically for short contexts and linearly for long contexts.', 'C': 'It scales up to k× for short contexts and up to k²× for long contexts.', 'D': 'It scales exponentially for short contexts and logarithmically for long contexts.'}",C,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).","['paragraph_chunk_009', 'paragraph_chunk_010', 'paragraph_chunk_086']",C,"['paragraph_chunk_010', 'paragraph_chunk_086']",False,True,True,0.576
hybrid,paragraph,5,63,"When a fraction p of chunks is expanded (left uncompressed), what is the effective compression rate for the remaining decoder input?","{'A': 'k · (1 − p)', 'B': 'k / (1 + p)', 'C': 'k / (1 − p + k p)', 'D': '(k − p) / (1 + k)'}",C,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).","['paragraph_chunk_018', 'paragraph_chunk_019', 'paragraph_chunk_100']",C,['paragraph_chunk_018'],False,True,True,0.658
hybrid,paragraph,5,64,"During the reconstruction task used in continual pre-training, which components are trained and which are frozen?","{'A': 'Only the decoder is trained while the encoder and projection are frozen.', 'B': 'Both the encoder and decoder are trained while the projection is frozen.', 'C': 'The encoder and projection are trained while the decoder is frozen.', 'D': 'All components are frozen to regularize the reconstruction objective.'}",C,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","['paragraph_chunk_021', 'paragraph_chunk_014', 'paragraph_chunk_020']",C,['paragraph_chunk_014'],False,True,True,0.484
hybrid,paragraph,5,65,Why is curriculum learning employed for REFRAG’s reconstruction and continual pre-training tasks?,"{'A': 'To reduce GPU memory by training on progressively smaller vocabularies.', 'B': 'To permit mixed-precision arithmetic without affecting perplexity.', 'C': 'To manage the exponential growth in token combinations with larger k and the difficulty of reconstructing s = k×L tokens from only L chunk embeddings.', 'D': 'To align the encoder with the retriever by distilling dense retrieval scores.'}",C,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.","['paragraph_chunk_020', 'paragraph_chunk_025', 'paragraph_chunk_124']",C,['paragraph_chunk_020'],False,True,False,0.8
hybrid,paragraph,5,66,What limitation prevents CEPE from supporting multi-turn RAG or summarization tasks?,"{'A': 'It requires much larger encoders than the decoder, causing overfitting.', 'B': 'It eliminates retrieval and relies solely on parametric memory.', 'C': 'It disrupts the causal structure of the context and is limited to prefix-only applications.', 'D': 'It compresses tokens so aggressively that gradients vanish during training.'}",C,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.","['paragraph_chunk_034', 'paragraph_chunk_116', 'paragraph_chunk_012']",C,['paragraph_chunk_034'],False,True,True,0.523
hybrid,paragraph,5,67,"Under a strong retriever with equal latency budgets (8 passages for REFRAG vs. 1 for LLaMA), what performance and latency outcomes are observed?","{'A': 'LLaMA outperforms REFRAG by over 5% but is slower.', 'B': 'Both models achieve identical accuracy and identical TTFT.', 'C': 'REFRAG achieves about a 1.22% average accuracy improvement while delivering a 5.26× TTFT speedup.', 'D': 'REFRAG underperforms LLaMA by 1–2% but matches TTFT.'}",C,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.","['paragraph_chunk_029', 'paragraph_chunk_028', 'paragraph_chunk_023']",C,['paragraph_chunk_029'],False,True,True,0.598
hybrid,paragraph,5,68,How does REFRAG extend usable context beyond the original 4K window of LLaMA-2-7B?,"{'A': 'By modifying positional encodings to 32K during pre-training.', 'B': 'By swapping the decoder for an encoder–decoder architecture with cross-attention.', 'C': 'By extrapolating with chunk embeddings, enabling longer contexts without extending the model’s positional encodings.', 'D': 'By caching KV states from previous prompts across sessions.'}",C,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.","['paragraph_chunk_003', 'paragraph_chunk_097', 'paragraph_chunk_002']",C,"['002', '003']",False,True,False,0.613
hybrid,paragraph,5,69,What data mix and scale are used to construct the continual pre-training corpus for long-context learning?,"{'A': 'Only Wikipedia and CommonCrawl totaling 10B tokens.', 'B': 'StackExchange and GitHub totaling 5B tokens.', 'C': 'A 20B-token corpus with 50% ArXiv and 50% Book domains from SlimPajama.', 'D': 'All SlimPajama domains totaling 627B tokens.'}",C,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.","['paragraph_chunk_033', 'paragraph_chunk_051', 'paragraph_chunk_014']",X,[],True,False,,0.849
hybrid,paragraph,5,70,"When matching the number of context signals (256) between methods, how does REFRAG with compression rate 8 compare to a truncated-token baseline?","{'A': 'It performs worse because the chunk embeddings discard token order.', 'B': 'It performs roughly the same since both expose 256 inputs to the decoder.', 'C': 'It consistently surpasses a LLaMA baseline using only the last 256 tokens, demonstrating the effectiveness of compressed chunk embeddings.', 'D': 'It cannot be compared because REFRAG needs at least 512 chunk embeddings.'}",C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.","['paragraph_chunk_018', 'paragraph_chunk_024', 'paragraph_chunk_021']",C,['paragraph_chunk_018'],False,True,True,0.574
